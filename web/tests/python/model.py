import tvm
metadata = tvm.ir.load_json("""{
  \"root\": 1, 
  \"nodes\": [
    {
      \"type_key\": \"\"
    }, 
    {
      \"type_key\": \"Map\", 
      \"keys\": [
        \"relax.expr.Constant\"
      ], 
      \"data\": [2]
    }, 
    {
      \"type_key\": \"Array\", 
      \"data\": [3, 14]
    }, 
    {
      \"type_key\": \"relax.expr.Constant\", 
      \"attrs\": {
        \"_checked_type_\": \"13\", 
        \"data\": \"0\", 
        \"span\": \"0\", 
        \"struct_info_\": \"4\"
      }
    }, 
    {
      \"type_key\": \"relax.TensorStructInfo\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"4\", 
        \"shape\": \"5\", 
        \"span\": \"0\", 
        \"vdevice\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.expr.ShapeExpr\", 
      \"attrs\": {
        \"_checked_type_\": \"12\", 
        \"span\": \"0\", 
        \"struct_info_\": \"11\", 
        \"values\": \"6\"
      }
    }, 
    {
      \"type_key\": \"Array\", 
      \"data\": [7, 8, 9, 10]
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"1\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"1\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"77\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"77\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeStructInfo\", 
      \"attrs\": {
        \"ndim\": \"4\", 
        \"span\": \"0\", 
        \"values\": \"6\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeType\", 
      \"attrs\": {
        \"ndim\": \"4\", 
        \"span\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.DynTensorType\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"4\", 
        \"span\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.expr.Constant\", 
      \"attrs\": {
        \"_checked_type_\": \"22\", 
        \"data\": \"1\", 
        \"span\": \"0\", 
        \"struct_info_\": \"15\"
      }
    }, 
    {
      \"type_key\": \"relax.TensorStructInfo\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"2\", 
        \"shape\": \"16\", 
        \"span\": \"0\", 
        \"vdevice\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.expr.ShapeExpr\", 
      \"attrs\": {
        \"_checked_type_\": \"21\", 
        \"span\": \"0\", 
        \"struct_info_\": \"20\", 
        \"values\": \"17\"
      }
    }, 
    {
      \"type_key\": \"Array\", 
      \"data\": [18, 19]
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"1\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"160\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeStructInfo\", 
      \"attrs\": {
        \"ndim\": \"2\", 
        \"span\": \"0\", 
        \"values\": \"17\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeType\", 
      \"attrs\": {
        \"ndim\": \"2\", 
        \"span\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.DynTensorType\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"2\", 
        \"span\": \"0\"
      }
    }
  ], 
  \"b64ndarrays\": [
    \"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\", 
    \"P6G0lvBAXt0AAAAAAAAAAAEAAAAAAAAAAgAAAAIgAQABAAAAAAAAAKAAAAAAAAAAgAIAAAAAAAAAAIA/+a1xPwUpZD+rZVc/F1lLPxD5Pz/vOzU/lhgrP2uGIT9QfRg/mfUPPwroBz/OTQA/4EDyPrWz5D6a6Nc+s9TLPsFtwD4YqrU+loCrPprooT4B2pg+G02QPqc6iD7Mm4A+IdRyPrk+ZT7Xa1g+mlBMPrniQD6GGDY+1OgrPgVLIj7qNhk+0aQQPnaNCD746QA+v2fzPRLK5T1n79g9y8zMPfhXwT03h7Y9VFGsPaytoj0MlJk9v/yQPXfgiD1WOIE9svtzPcBVZj1Dc1k9R0lNPYDNQT0p9jY9FLosPZEQIz1o8Rk94FQRPaozCT3hhgE9A5D0PMHh5jxw99k8EsbNPFBDwjxhZbc8ESOtPKxzozz7Tpo8Oa2RPBKHiTyd1YE8piR1PB9uZzzwe1o8JkNOPGK5Qjze1Dc8UowtPAjXIzzGrBo8wgUSPKzaCTyKJAI8rLn1O8f65zvBANs7icDOO8EvwzuYRLg7zPWtO6I6pDvMCps7h16SO3Quijumc4I7DU92O82HaDvchVs7Mj5PO2imQzuctDg7jF8uO3KeJDsMaRs7gbcSO3SCCjvvwgI7v+T2OiYV6TpOC9w6M7zPOlgdxDrlJLk6h8muOoUCpTqBx5s6rRCTOqjWijptEoM63Xp3OteiaToSkVw6bjpQOomURDprlTk6yDMvOtRmJTo0Jhw6GGoTOg4rCzobYgM6PxH4Odcw6jkhF905BLnQOQkMxTk9Bro5T56vOWDLpTkghZw5sMOTOaN/izn0sYM5DKh4OS2/ajmOnV056zdROdODRTlSdzo5CwkwOSMwJjlB5Bw5fx0UOWzUCzkIAgQ5PD/5OOhN6zg/JN44\"
  ], 
  \"attrs\": {\"tvm_version\": \"0.14.dev0\"}
}""")
from tvm.script import ir as I
from tvm.script import tir as T
from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func
    def add(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3], B[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_add[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] + B[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def concatenate(A: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), B: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(59136))
                        v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(59136) // T.int64(768))
                        v_ax2 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(768))
                        T.where((ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) * T.int64(256) + ax0_ax1_ax2_fused_2 < T.int64(118272))
                        T.reads(B[v_ax0 - T.int64(1), v_ax1, v_ax2], A[v_ax0, v_ax1, v_ax2])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2])
                        T_concat[v_ax0, v_ax1, v_ax2] = T.if_then_else(T.int64(1) <= v_ax0, B[v_ax0 - T.int64(1), v_ax1, v_ax2], A[v_ax0, v_ax1, v_ax2])

    @T.prim_func
    def concatenate1(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_concat"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16384))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16384) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(B[v_ax0 - T.int64(1), v_ax1, v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1) <= v_ax0, B[v_ax0 - T.int64(1), v_ax1, v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate10(A: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(80)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2621440))
                        v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2621440) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(B[v_ax0, v_ax1 - T.int64(320), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(320) <= v_ax1, B[v_ax0, v_ax1 - T.int64(320), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate3(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), B: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(163840))
                        v_ax1 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(163840) // T.int64(64))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                        v_ax3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                        T.reads(B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate4(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), B: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate5(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), B: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(15)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(491520))
                        v_ax1 = T.axis.spatial(T.int64(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(491520) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate6(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), B: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(60)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1966080))
                        v_ax1 = T.axis.spatial(T.int64(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1966080) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate7(A: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), B: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(640) <= v_ax1, B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate8(A: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), B: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(30)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(983040))
                        v_ax1 = T.axis.spatial(T.int64(960), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(983040) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(640) <= v_ax1, B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate9(A: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(120)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3932160))
                        v_ax1 = T.axis.spatial(T.int64(960), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3932160) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(640) <= v_ax1, B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def divide(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3], B[()])
                    T.writes(T_divide[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_divide[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] / B[()]

    @T.prim_func
    def divide11(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_divide[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_divide[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] * T.float32(0.083333333333333329)

    @T.prim_func
    def divide12(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_divide[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_divide[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] * T.float32(0.5)

    @T.prim_func
    def fused_broadcast_to1_strided_slice_reshape20_cast3_multiply12_multiply13_tir_sin_tir_cos_concatenate2_strided_slice1_reshape21_strided_slice2_reshape21_concatenate2(inp_1: T.Buffer((), "int32"), param_0: T.Buffer((T.int64(1), T.int64(160)), "float32"), var_T_concat_intermediate: T.Buffer((T.int64(2), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_concat_1"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) // T.int64(320))
                    v_ax1 = T.axis.spatial(T.int64(320), (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) % T.int64(320))
                    T.where(ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 < T.int64(640))
                    T.reads(inp_1[()], param_0[T.int64(0), v_ax1 % T.int64(160) - T.int64(160):v_ax1 % T.int64(160) - T.int64(160) + T.int64(321)])
                    T.writes(var_T_concat_intermediate[v_ax0, v_ax1])
                    var_T_concat_intermediate[v_ax0, v_ax1] = T.if_then_else(T.int64(160) <= v_ax1, T.if_then_else(T.int64(160) <= (v_ax1 - T.int64(160)) % T.int64(160), T.cos(T.Cast("float32", inp_1[()]) * param_0[T.int64(0), (v_ax1 - T.int64(160)) % T.int64(160) - T.int64(160)]), T.sin(T.Cast("float32", inp_1[()]) * param_0[T.int64(0), (v_ax1 - T.int64(160)) % T.int64(160)])), T.if_then_else(T.int64(160) <= v_ax1 % T.int64(160) + T.int64(160), T.cos(T.Cast("float32", inp_1[()]) * param_0[T.int64(0), v_ax1 % T.int64(160) + T.int64(160) - T.int64(160)]), T.sin(T.Cast("float32", inp_1[()]) * param_0[T.int64(0), v_ax1 % T.int64(160) + T.int64(160)])))

    @T.prim_func
    def fused_conv2d10_add11(lv207: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_1_conv1_weight: T.Buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), "float32"), lv209: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(514), T.int64(514)), scope="shared")
        vae_decoder_up_blocks_3_resnets_1_conv1_weight_shared = T.alloc_buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32768), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(64) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(128), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(100))
                                        v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(100) // T.int64(10))
                                        v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(200))
                                        T.reads(lv207[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv207[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.3.resnets.1.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(18))
                                        v1 = T.axis.spatial(T.int64(128), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(288))
                                        T.reads(vae_decoder_up_blocks_3_resnets_1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_3_resnets_1_conv1_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_3_resnets_1_conv1_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_3_resnets_1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(64) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(128), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_3_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_3_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(64) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv209[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv209[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d10_add11_add12_divide5(lv197: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_conv2_weight: T.Buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), "float32"), lv199: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), lv203: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(514), T.int64(514)), scope="shared")
        vae_decoder_up_blocks_3_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(128), rc_0)
                                        v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(6))
                                        v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(6))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(396))
                                        T.reads(lv197[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv197[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.3.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(128), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(144))
                                        T.reads(vae_decoder_up_blocks_3_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_3_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_3_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_3_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(128), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_3_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_3_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + ax3)
                            T.reads(lv203[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv199[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv203[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv199[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d11_add11(lv190: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(128), T.int64(256), T.int64(1), T.int64(1)), "float32"), lv202: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), scope="shared")
        vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(128), T.int64(256), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32768), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.reads(lv190[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv190[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("vae.decoder.up_blocks.3.resnets.0.conv_shortcut.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                    T.writes(vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                    vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 * T.int64(16) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv202[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv202[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d12_add13_divide6_add14_tir_clip(lv231: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), vae_decoder_conv_out_weight: T.Buffer((T.int64(3), T.int64(128), T.int64(3), T.int64(3)), "float32"), lv233: T.Buffer((T.int64(1), T.int64(3), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(514), T.int64(514)), scope="shared")
        vae_decoder_conv_out_weight_shared = T.alloc_buffer((T.int64(3), T.int64(128), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(3), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(3), nn_1_ff_1_yy_1_xx_1_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(128), rc_0)
                                        v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(18))
                                        v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(180))
                                        T.reads(lv231[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv231[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("vae.decoder.conv_out.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(128), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(27))
                                    T.reads(vae_decoder_conv_out_weight[v0, v1, v2, v3])
                                    T.writes(vae_decoder_conv_out_weight_shared[v0, v1, v2, v3])
                                    vae_decoder_conv_out_weight_shared[v0, v1, v2, v3] = vae_decoder_conv_out_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(3), nn_1_ff_1_yy_1_xx_1_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(128), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_conv_out_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_conv_out_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(3), nn_1_ff_1_yy_1_xx_1_fused + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv233[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_compute_intermediate[v0, v1, v2, v3])
                            var_compute_intermediate[v0, v1, v2, v3] = T.max(T.min((var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv233[v0, v1, T.int64(0), T.int64(0)]) * T.float32(0.5) + T.float32(0.5), T.float32(1)), T.float32(0))

    @T.prim_func
    def fused_conv2d13_add21(lv: T.Buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), "float32"), unet_conv_in_weight: T.Buffer((T.int64(320), T.int64(4), T.int64(3), T.int64(3)), "float32"), lv23: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(4), T.int64(66), T.int64(66)), scope="shared")
        unet_conv_in_weight_shared = T.alloc_buffer((T.int64(320), T.int64(4), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(128) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(2), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512))
                                        v1 = T.axis.spatial(T.int64(4), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(66), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(128))
                                        T.reads(lv[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.conv_in.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(128) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(6))
                                        v1 = T.axis.spatial(T.int64(4), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(6) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(unet_conv_in_weight[v0, v1, v2, v3])
                                        T.writes(unet_conv_in_weight_shared[v0, v1, v2, v3])
                                        unet_conv_in_weight_shared[v0, v1, v2, v3] = unet_conv_in_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(128) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(4), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_conv_in_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_conv_in_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(128) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv23[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv23[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d14_add21_add23(lv26: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_resnets_0_conv1_weight: T.Buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv28: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv35: T.Buffer((T.int64(2), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(66), T.int64(66)), scope="shared")
        unet_down_blocks_0_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(720))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(720) // T.int64(180))
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(180) // T.int64(10))
                                        v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1440))
                                        T.reads(lv26[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv26[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.down_blocks.0.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(288))
                                        T.reads(unet_down_blocks_0_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_0_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_0_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_down_blocks_0_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(3), T.int64(1), T.int64(4), T.int64(2), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_0_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_0_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv28[T.int64(0), v1, T.int64(0), T.int64(0)], lv35[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv28[T.int64(0), v1, T.int64(0), T.int64(0)] + lv35[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d14_add21_add24_divide7(lv38: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_resnets_0_conv2_weight: T.Buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv40: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv24: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(66), T.int64(66)), scope="shared")
        unet_down_blocks_0_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2560), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(128) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(72))
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(72) // T.int64(18))
                                        v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(288))
                                        T.reads(lv38[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv38[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.down_blocks.0.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(128) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_down_blocks_0_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_0_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_0_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = unet_down_blocks_0_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(128) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_0_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_0_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(128) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(lv24[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv40[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv24[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv40[T.int64(0), v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d15_add21(lv44: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_attentions_0_proj_in_weight: T.Buffer((T.int64(320), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv46: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="shared")
        unet_down_blocks_0_attentions_0_proj_in_weight_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 < T.int64(64))
                                    T.reads(lv44[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv44[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.down_blocks.0.attentions.0.proj_in.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(640) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(640) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(320))
                                        T.reads(unet_down_blocks_0_attentions_0_proj_in_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_0_attentions_0_proj_in_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_0_attentions_0_proj_in_weight_shared[v0, v1, v2, v3] = unet_down_blocks_0_attentions_0_proj_in_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_0_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_0_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv46[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv46[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d15_add21_add24(lv120: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_attentions_0_proj_out_weight: T.Buffer((T.int64(320), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv122: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv43: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="shared")
        unet_down_blocks_0_attentions_0_proj_out_weight_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2560), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv120[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv120[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.down_blocks.0.attentions.0.proj_out.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(32))
                                        T.reads(unet_down_blocks_0_attentions_0_proj_out_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_0_attentions_0_proj_out_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_0_attentions_0_proj_out_weight_shared[v0, v1, v2, v3] = unet_down_blocks_0_attentions_0_proj_out_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_0_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_0_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv122[T.int64(0), v1, T.int64(0), T.int64(0)], lv43[v0, v1, v2, v3])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv122[T.int64(0), v1, T.int64(0), T.int64(0)] + lv43[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d16_add28(lv224: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_downsamplers_0_conv_weight: T.Buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv226: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(66), T.int64(66)), scope="shared")
        unet_down_blocks_0_downsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(5120), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2560) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2560) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2560))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(5) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(81))
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(81) // T.int64(9))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(405))
                                    T.reads(lv224[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv224[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.down_blocks.0.downsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2560) // T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(45))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(5) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(45) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(360))
                                        T.reads(unet_down_blocks_0_downsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_0_downsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_0_downsamplers_0_conv_weight_shared[v0, v1, v2, v3] = unet_down_blocks_0_downsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(5), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2560) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2560) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(5) + rc_1 * T.int64(5) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx], unet_down_blocks_0_downsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx] * unet_down_blocks_0_downsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2560) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2560) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv226[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv226[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d17_add29_add31(lv229: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_conv1_weight: T.Buffer((T.int64(640), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv231: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv238: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(34), T.int64(34)), scope="shared")
        unet_down_blocks_1_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(640), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(20), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1024) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(34), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv229[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv229[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(12)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.down_blocks.1.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(48))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(48) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(unet_down_blocks_1_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_1_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_1_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(16), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(16) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv231[T.int64(0), v1, T.int64(0), T.int64(0)], lv238[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv231[T.int64(0), v1, T.int64(0), T.int64(0)] + lv238[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d18_add29_add31(lv332: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_1_conv1_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv334: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv341: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(34), T.int64(34)), scope="shared")
        unet_down_blocks_1_resnets_1_conv1_weight_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2560), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(3), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280))
                                        v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(48))
                                        v2 = T.axis.spatial(T.int64(34), ry_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(48) // T.int64(6))
                                        v3 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(6))
                                        T.reads(lv332[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv332[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.down_blocks.1.resnets.1.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(24) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), ry_0)
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_down_blocks_1_resnets_1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_1_resnets_1_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_1_resnets_1_conv1_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_resnets_1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(8) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv334[T.int64(0), v1, T.int64(0), T.int64(0)], lv341[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv334[T.int64(0), v1, T.int64(0), T.int64(0)] + lv341[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d18_add29_add32_divide8(lv241: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_conv2_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv243: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv247: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(34), T.int64(34)), scope="shared")
        unet_down_blocks_1_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(100))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(100) // T.int64(10))
                                        v3 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(200))
                                        T.reads(lv241[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv241[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.down_blocks.1.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(144))
                                        T.reads(unet_down_blocks_1_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_1_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_1_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(2), T.int64(2), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(lv247[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv243[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv247[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv243[T.int64(0), v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d19_add29(lv227: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(640), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv246: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), scope="shared")
        unet_down_blocks_1_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(640), T.int64(320), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(20), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(2))
                                    v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2))
                                    T.reads(lv227[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv227[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.down_blocks.1.resnets.0.conv_shortcut.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_down_blocks_1_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_1_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_1_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(16) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv246[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv246[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d1_add2(lv3: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), vae_decoder_conv_in_weight: T.Buffer((T.int64(512), T.int64(4), T.int64(3), T.int64(3)), "float32"), lv5: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(66), T.int64(66)), scope="shared")
        vae_decoder_conv_in_weight_shared = T.alloc_buffer((T.int64(512), T.int64(4), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(204))
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(204) // T.int64(34))
                                        v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(34))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(816))
                                        T.reads(lv3[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv3[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("vae.decoder.conv_in.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(576))
                                        T.reads(vae_decoder_conv_in_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_conv_in_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_conv_in_weight_shared[v0, v1, v2, v3] = vae_decoder_conv_in_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(4), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_conv_in_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_conv_in_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv5[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv5[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d20_add29(lv250: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_attentions_0_proj_in_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv252: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="shared")
        unet_down_blocks_1_attentions_0_proj_in_weight_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(lv250[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv250[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("unet.down_blocks.1.attentions.0.proj_in.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(unet_down_blocks_1_attentions_0_proj_in_weight[v0, v1, v2, v3])
                                    T.writes(unet_down_blocks_1_attentions_0_proj_in_weight_shared[v0, v1, v2, v3])
                                    unet_down_blocks_1_attentions_0_proj_in_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_attentions_0_proj_in_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv252[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv252[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d20_add29_add32(lv326: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_attentions_0_proj_out_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv328: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv249: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="shared")
        unet_down_blocks_1_attentions_0_proj_out_weight_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1024))
                                    T.reads(lv326[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv326[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.down_blocks.1.attentions.0.proj_out.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(640) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(640) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(40))
                                        T.reads(unet_down_blocks_1_attentions_0_proj_out_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_1_attentions_0_proj_out_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_1_attentions_0_proj_out_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_attentions_0_proj_out_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv328[T.int64(0), v1, T.int64(0), T.int64(0)], lv249[v0, v1, v2, v3])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv328[T.int64(0), v1, T.int64(0), T.int64(0)] + lv249[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d21_add36(lv430: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_downsamplers_0_conv_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv432: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(34), T.int64(34)), scope="shared")
        unet_down_blocks_1_downsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + yy_3_init * T.int64(4) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(198))
                                        v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(198) // T.int64(99))
                                        v2 = T.axis.spatial(T.int64(34), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(99) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(396))
                                        T.reads(lv430[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv430[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.down_blocks.1.downsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(18))
                                        v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_down_blocks_1_downsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_1_downsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_1_downsamplers_0_conv_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_downsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + yy_3 * T.int64(4) + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx], unet_down_blocks_1_downsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx] * unet_down_blocks_1_downsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv432[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv432[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d22_add37_add38(lv435: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_conv1_weight: T.Buffer((T.int64(1280), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv437: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv444: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(18), T.int64(18)), scope="shared")
        unet_down_blocks_2_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(240) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(72))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(18), (ax0_ax1_ax2_ax3_fused_0 * T.int64(240) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(72) // T.int64(4))
                                        v3 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(240) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(144))
                                        T.reads(lv435[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv435[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.down_blocks.2.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(720))
                                        T.reads(unet_down_blocks_2_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_2_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_2_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(3), T.int64(2), T.int64(4), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv437[T.int64(0), v1, T.int64(0), T.int64(0)], lv444[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv437[T.int64(0), v1, T.int64(0), T.int64(0)] + lv444[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d23_add37(lv866: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_0_upsamplers_0_conv_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv868: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(18), T.int64(18)), scope="shared")
        unet_up_blocks_0_upsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(15)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(900))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(5) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(900) // T.int64(180))
                                    v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(180) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(18), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1800))
                                    T.reads(lv866[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv866[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.up_blocks.0.upsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(45))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(5) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(45) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(720))
                                        T.reads(unet_up_blocks_0_upsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_0_upsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_0_upsamplers_0_conv_weight_shared[v0, v1, v2, v3] = unet_up_blocks_0_upsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(5), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(5) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_0_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_0_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv868[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv868[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d23_add37_add38(lv538: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_1_conv1_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv540: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv547: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(18), T.int64(18)), scope="shared")
        unet_down_blocks_2_resnets_1_conv1_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40) // T.int64(4))
                                        v3 = T.axis.spatial(T.int64(18), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        T.reads(lv538[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv538[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.down_blocks.2.resnets.1.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(24) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(unet_down_blocks_2_resnets_1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_2_resnets_1_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_2_resnets_1_conv1_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_resnets_1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(8) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv540[T.int64(0), v1, T.int64(0), T.int64(0)], lv547[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv540[T.int64(0), v1, T.int64(0), T.int64(0)] + lv547[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d23_add37_add39_divide9(lv447: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_conv2_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv449: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv453: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(18), T.int64(18)), scope="shared")
        unet_down_blocks_2_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(32) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(500))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(5) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(500) // T.int64(100))
                                        v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(100) // T.int64(10))
                                        v3 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1000))
                                        T.reads(lv447[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv447[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.down_blocks.2.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(45))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(5) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(45) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(360))
                                        T.reads(unet_down_blocks_2_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_2_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_2_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(3), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(32) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(5) + rc_1 * T.int64(5) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(32) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(lv453[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv449[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv453[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv449[T.int64(0), v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d24_add37(lv433: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(1280), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv452: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), scope="shared")
        unet_down_blocks_2_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(640), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(40) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(5) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(40) // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(640))
                                        v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.reads(lv433[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv433[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("unet.down_blocks.2.resnets.0.conv_shortcut.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) // T.int64(40))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) % T.int64(40))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(unet_down_blocks_2_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                    T.writes(unet_down_blocks_2_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                    unet_down_blocks_2_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(40), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(40) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(5) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(40) // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(40) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(40) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(5) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(40) // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv452[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv452[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d25_add37(lv456: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_attentions_0_proj_in_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv458: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="shared")
        unet_down_blocks_2_attentions_0_proj_in_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(80), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(10) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(160) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(320))
                                    T.reads(lv456[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv456[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.down_blocks.2.attentions.0.proj_in.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(256) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(10))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(10) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_down_blocks_2_attentions_0_proj_in_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_2_attentions_0_proj_in_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_2_attentions_0_proj_in_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_attentions_0_proj_in_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(5), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(10) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv458[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv458[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d25_add37_add39(lv532: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_attentions_0_proj_out_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv534: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv455: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="shared")
        unet_down_blocks_2_attentions_0_proj_out_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(10), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(40), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2048))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2048) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(lv532[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv532[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("unet.down_blocks.2.attentions.0.proj_out.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(unet_down_blocks_2_attentions_0_proj_out_weight[v0, v1, v2, v3])
                                    T.writes(unet_down_blocks_2_attentions_0_proj_out_weight_shared[v0, v1, v2, v3])
                                    unet_down_blocks_2_attentions_0_proj_out_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_attentions_0_proj_out_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(32) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv534[T.int64(0), v1, T.int64(0), T.int64(0)], lv455[v0, v1, v2, v3])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv534[T.int64(0), v1, T.int64(0), T.int64(0)] + lv455[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d26_add43(lv636: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_downsamplers_0_conv_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv638: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(18), T.int64(18)), scope="shared")
        unet_down_blocks_2_downsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1500))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1500) // T.int64(75))
                                        v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(75) // T.int64(15))
                                        v3 = T.axis.spatial(T.int64(18), rx_0 + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(15))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(3000))
                                        T.reads(lv636[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv636[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(15)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("unet.down_blocks.2.downsamplers.0.conv.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(60))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(60) // T.int64(3))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), rx_0)
                                    T.reads(unet_down_blocks_2_downsamplers_0_conv_weight[v0, v1, v2, v3])
                                    T.writes(unet_down_blocks_2_downsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                    unet_down_blocks_2_downsamplers_0_conv_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_downsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(20), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(20) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx], unet_down_blocks_2_downsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx] * unet_down_blocks_2_downsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv638[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv638[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d27_add43_add44(lv641: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), unet_down_blocks_3_resnets_0_conv1_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv643: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv650: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(10), T.int64(10)), scope="shared")
        unet_down_blocks_3_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(2) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(10), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(10), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 < T.int64(128))
                                    T.reads(lv641[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(9) and T.int64(1) <= v3 and v3 < T.int64(9), lv641[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.down_blocks.3.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_down_blocks_3_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_3_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_3_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_down_blocks_3_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(2) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_3_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_3_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv643[T.int64(0), v1, T.int64(0), T.int64(0)], lv650[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv643[T.int64(0), v1, T.int64(0), T.int64(0)] + lv650[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d27_add43_add45_divide10(lv653: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), unet_down_blocks_3_resnets_0_conv2_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv655: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv639: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(10), T.int64(10)), scope="shared")
        unet_down_blocks_3_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(60))
                                        v2 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(60) // T.int64(6))
                                        v3 = T.axis.spatial(T.int64(10), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(6))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(240))
                                        T.reads(lv653[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(9) and T.int64(1) <= v3 and v3 < T.int64(9), lv653[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("unet.down_blocks.3.resnets.0.conv2.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(36))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(36) // T.int64(9))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(unet_down_blocks_3_resnets_0_conv2_weight[v0, v1, v2, v3])
                                    T.writes(unet_down_blocks_3_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                    unet_down_blocks_3_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = unet_down_blocks_3_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_3_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_3_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(lv639[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv655[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv639[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv655[T.int64(0), v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d28_add43(lv697: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), unet_mid_block_attentions_0_proj_in_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv699: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="shared")
        unet_mid_block_attentions_0_proj_in_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(160))
                                        T.reads(lv697[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv697[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.mid_block.attentions.0.proj_in.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(160) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(20))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_mid_block_attentions_0_proj_in_weight[v0, v1, v2, v3])
                                        T.writes(unet_mid_block_attentions_0_proj_in_weight_shared[v0, v1, v2, v3])
                                        unet_mid_block_attentions_0_proj_in_weight_shared[v0, v1, v2, v3] = unet_mid_block_attentions_0_proj_in_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(20), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(20) + rc_1 * T.int64(20) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_mid_block_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_mid_block_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv699[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv699[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d28_add43_add45(lv773: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), unet_mid_block_attentions_0_proj_out_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv775: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv696: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="shared")
        unet_mid_block_attentions_0_proj_out_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.reads(lv773[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv773[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.mid_block.attentions.0.proj_out.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_mid_block_attentions_0_proj_out_weight[v0, v1, v2, v3])
                                        T.writes(unet_mid_block_attentions_0_proj_out_weight_shared[v0, v1, v2, v3])
                                        unet_mid_block_attentions_0_proj_out_weight_shared[v0, v1, v2, v3] = unet_mid_block_attentions_0_proj_out_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(16) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_mid_block_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_mid_block_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv775[T.int64(0), v1, T.int64(0), T.int64(0)], lv696[v0, v1, v2, v3])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv775[T.int64(0), v1, T.int64(0), T.int64(0)] + lv696[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d29_add43_add44(lv799: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32"), unet_up_blocks_0_resnets_0_conv1_weight: T.Buffer((T.int64(1280), T.int64(2560), T.int64(3), T.int64(3)), "float32"), lv801: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv808: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(2560), T.int64(10), T.int64(10)), scope="shared")
        unet_up_blocks_0_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(2560), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(4) + yy_3_init * T.int64(4) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(100))
                                    v2 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(100) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.reads(lv799[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(9) and T.int64(1) <= v3 and v3 < T.int64(9), lv799[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.0.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(72))
                                        v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(72) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_up_blocks_0_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_0_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_0_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_0_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(4) + yy_3 * T.int64(4) + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(2560), rc_0 * T.int64(8) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_0_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_0_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv801[T.int64(0), v1, T.int64(0), T.int64(0)], lv808[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv801[T.int64(0), v1, T.int64(0), T.int64(0)] + lv808[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d2_add2(lv8: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), vae_decoder_mid_block_resnets_0_conv1_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv10: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(66), T.int64(66)), scope="shared")
        vae_decoder_mid_block_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(324))
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(324) // T.int64(18))
                                        v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(648))
                                        T.reads(lv8[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv8[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("vae.decoder.mid_block.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(18))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(288))
                                        T.reads(vae_decoder_mid_block_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_mid_block_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_mid_block_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = vae_decoder_mid_block_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_mid_block_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_mid_block_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv10[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv10[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d2_add2_add3_divide1(lv13: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), vae_decoder_mid_block_resnets_0_conv2_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv15: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv6: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(66), T.int64(66)), scope="shared")
        vae_decoder_mid_block_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + yy_3_init * T.int64(4) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(108))
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(108) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(216))
                                    T.reads(lv13[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv13[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.mid_block.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(18))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(288))
                                        T.reads(vae_decoder_mid_block_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_mid_block_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_mid_block_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = vae_decoder_mid_block_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + yy_3 * T.int64(4) + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_mid_block_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_mid_block_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(lv6[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv15[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv6[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv15[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d2_add2_add3_divide1_divide1(lv61: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), vae_decoder_mid_block_resnets_1_conv2_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv63: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv54: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(66), T.int64(66)), scope="shared")
        vae_decoder_mid_block_resnets_1_conv2_weight_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0)
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v3 = T.axis.spatial(T.int64(66), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(72))
                                        T.reads(lv61[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv61[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("vae.decoder.mid_block.resnets.1.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3))
                                        v1 = T.axis.spatial(T.int64(512), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(384))
                                        T.reads(vae_decoder_mid_block_resnets_1_conv2_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_mid_block_resnets_1_conv2_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_mid_block_resnets_1_conv2_weight_shared[v0, v1, v2, v3] = vae_decoder_mid_block_resnets_1_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_mid_block_resnets_1_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_mid_block_resnets_1_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + ax3)
                            T.reads(lv54[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv63[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv54[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv63[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d30_add43(lv797: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32"), unet_up_blocks_0_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(1280), T.int64(2560), T.int64(1), T.int64(1)), "float32"), lv816: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), scope="shared")
        unet_up_blocks_0_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(2560), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) % T.int64(160) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8))
                                    T.reads(lv797[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv797[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("unet.up_blocks.0.resnets.0.conv_shortcut.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) // T.int64(20))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) % T.int64(20))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(unet_up_blocks_0_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                    T.writes(unet_up_blocks_0_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                    unet_up_blocks_0_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_0_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(10), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(2560), rc_0 * T.int64(20) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_0_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_0_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv816[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv816[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d31_add37_add38(lv872: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_1_resnets_0_conv1_weight: T.Buffer((T.int64(1280), T.int64(2560), T.int64(3), T.int64(3)), "float32"), lv874: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv881: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(2560), T.int64(18), T.int64(18)), scope="shared")
        unet_up_blocks_1_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(2560), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(8) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160))
                                        v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(160))
                                        v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(18), rx_0 + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1280))
                                        T.reads(lv872[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv872[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.up_blocks.1.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(24) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(384))
                                        T.reads(unet_up_blocks_1_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_1_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_1_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_1_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(8) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(2560), rc_0 * T.int64(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv874[T.int64(0), v1, T.int64(0), T.int64(0)], lv881[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv874[T.int64(0), v1, T.int64(0), T.int64(0)] + lv881[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d32_add37(lv870: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_1_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(1280), T.int64(2560), T.int64(1), T.int64(1)), "float32"), lv889: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), scope="shared")
        unet_up_blocks_1_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(2560), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(40), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512))
                                        v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1024))
                                        T.reads(lv870[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv870[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.up_blocks.1.resnets.0.conv_shortcut.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_up_blocks_1_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_1_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_1_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_1_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(2560), rc_0 * T.int64(64) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv889[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv889[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d33_add37_add38(lv1080: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_1_resnets_2_conv1_weight: T.Buffer((T.int64(1280), T.int64(1920), T.int64(3), T.int64(3)), "float32"), lv1082: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv1089: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1920), T.int64(18), T.int64(18)), scope="shared")
        unet_up_blocks_1_resnets_2_conv1_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1920), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(480), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(18), rx_0 + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(lv1080[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv1080[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.1.resnets.2.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(12))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(12) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(unet_up_blocks_1_resnets_2_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_1_resnets_2_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_1_resnets_2_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_1_resnets_2_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1920), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_2_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_2_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1082[T.int64(0), v1, T.int64(0), T.int64(0)], lv1089[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1082[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1089[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d34_add37(lv1078: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_1_resnets_2_conv_shortcut_weight: T.Buffer((T.int64(1280), T.int64(1920), T.int64(1), T.int64(1)), "float32"), lv1097: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), scope="shared")
        unet_up_blocks_1_resnets_2_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1920), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(10) + ff_3_init * T.int64(5) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(12)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(24) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(128))
                                    v2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(lv1078[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1078[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.up_blocks.1.resnets.2.conv_shortcut.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(24) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(24))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1920))
                                        T.reads(unet_up_blocks_1_resnets_2_conv_shortcut_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_1_resnets_2_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_1_resnets_2_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_1_resnets_2_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(6), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(10) + ff_3 * T.int64(5) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1920), rc_0 * T.int64(24) + rc_1 * T.int64(6) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_2_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_2_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(10), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(10) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1097[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1097[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d35_add49(lv1182: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_1_upsamplers_0_conv_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv1184: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(34), T.int64(34)), scope="shared")
        unet_up_blocks_1_upsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(34), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv1182[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv1182[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.1.upsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(12))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(12) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(unet_up_blocks_1_upsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_1_upsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_1_upsamplers_0_conv_weight_shared[v0, v1, v2, v3] = unet_up_blocks_1_upsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(3), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(4) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1184[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1184[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d36_add29_add31(lv1188: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_0_conv1_weight: T.Buffer((T.int64(640), T.int64(1920), T.int64(3), T.int64(3)), "float32"), lv1190: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv1197: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1920), T.int64(34), T.int64(34)), scope="shared")
        unet_up_blocks_2_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(640), T.int64(1920), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(720))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(12) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(720) // T.int64(60))
                                        v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(60) // T.int64(10))
                                        v3 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        T.reads(lv1188[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv1188[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.up_blocks.2.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(108))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(12) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(108) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2160))
                                        T.reads(unet_up_blocks_2_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_2_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_2_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(12), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1920), rc_0 * T.int64(12) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1190[T.int64(0), v1, T.int64(0), T.int64(0)], lv1197[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1190[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1197[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d37_add29(lv1186: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(640), T.int64(1920), T.int64(1), T.int64(1)), "float32"), lv1205: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), scope="shared")
        unet_up_blocks_2_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(640), T.int64(1920), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(12) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(96) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(96) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(96) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(lv1186[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv1186[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.up_blocks.2.resnets.0.conv_shortcut.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(12))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(12) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(12))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_up_blocks_2_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1920), rc_0 * T.int64(12) + rc_1 * T.int64(3) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1205[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1205[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d38_add29_add31(lv1292: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_1_conv1_weight: T.Buffer((T.int64(640), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv1294: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv1301: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(34), T.int64(34)), scope="shared")
        unet_up_blocks_2_resnets_1_conv1_weight_shared = T.alloc_buffer((T.int64(640), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1024) // T.int64(128))
                                    v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(34), rx_0 + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(lv1292[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv1292[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.2.resnets.1.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(24) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(unet_up_blocks_2_resnets_1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_2_resnets_1_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_2_resnets_1_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_resnets_1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(8) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1294[T.int64(0), v1, T.int64(0), T.int64(0)], lv1301[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1294[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1301[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d39_add29(lv1290: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_1_conv_shortcut_weight: T.Buffer((T.int64(640), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv1309: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), scope="shared")
        unet_up_blocks_2_resnets_1_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(640), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(lv1290[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv1290[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.up_blocks.2.resnets.1.conv_shortcut.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_up_blocks_2_resnets_1_conv_shortcut_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_2_resnets_1_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_2_resnets_1_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_resnets_1_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(16) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_1_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_1_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1309[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1309[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d3_add5(lv104: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), vae_decoder_up_blocks_0_upsamplers_0_conv_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv106: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(130), T.int64(130)), scope="shared")
        vae_decoder_up_blocks_0_upsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(16384), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(60))
                                    v2 = T.axis.spatial(T.int64(130), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(60) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(130), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(240))
                                    T.reads(lv104[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(129) and T.int64(1) <= v3 and v3 < T.int64(129), lv104[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.0.upsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(vae_decoder_up_blocks_0_upsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_0_upsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_0_upsamplers_0_conv_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_0_upsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_0_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_0_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv106[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv106[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d3_add5_add6_divide3(lv114: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), vae_decoder_up_blocks_1_resnets_0_conv2_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv116: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv107: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(130), T.int64(130)), scope="shared")
        vae_decoder_up_blocks_1_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(8) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(324))
                                        v2 = T.axis.spatial(T.int64(130), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(324) // T.int64(18))
                                        v3 = T.axis.spatial(T.int64(130), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2592))
                                        T.reads(lv114[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(129) and T.int64(1) <= v3 and v3 < T.int64(129), lv114[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.1.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(72))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(72) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(vae_decoder_up_blocks_1_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_1_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_1_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_1_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(3), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(8) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(8) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_1_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_1_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(lv107[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv116[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv107[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv116[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d40_add29_add31(lv1396: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_2_conv1_weight: T.Buffer((T.int64(640), T.int64(960), T.int64(3), T.int64(3)), "float32"), lv1398: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv1405: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(960), T.int64(34), T.int64(34)), scope="shared")
        unet_up_blocks_2_resnets_2_conv1_weight_shared = T.alloc_buffer((T.int64(640), T.int64(960), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(8) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(408))
                                        v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(3) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(408) // T.int64(136))
                                        v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(136) // T.int64(34))
                                        v3 = T.axis.spatial(T.int64(34), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(34))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(816))
                                        T.reads(lv1396[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv1396[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.up_blocks.2.resnets.2.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(27))
                                        v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(3) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(27) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(432))
                                        T.reads(unet_up_blocks_2_resnets_2_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_2_resnets_2_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_2_resnets_2_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_resnets_2_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(3), T.int64(1), T.int64(3), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(8) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(960), rc_0 * T.int64(3) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_2_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_2_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1398[T.int64(0), v1, T.int64(0), T.int64(0)], lv1405[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1398[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1405[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d41_add29(lv1394: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_2_conv_shortcut_weight: T.Buffer((T.int64(640), T.int64(960), T.int64(1), T.int64(1)), "float32"), lv1413: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), scope="shared")
        unet_up_blocks_2_resnets_2_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(640), T.int64(960), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(5), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(24), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128) // T.int64(4))
                                        v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        T.reads(lv1394[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv1394[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("unet.up_blocks.2.resnets.2.conv_shortcut.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(40))
                                    v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(40))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1600))
                                    T.reads(unet_up_blocks_2_resnets_2_conv_shortcut_weight[v0, v1, v2, v3])
                                    T.writes(unet_up_blocks_2_resnets_2_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                    unet_up_blocks_2_resnets_2_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_resnets_2_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(40), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(960), rc_0 * T.int64(40) + rc_1 * T.int64(40) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_2_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_2_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1413[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1413[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d42_add50(lv1498: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_2_upsamplers_0_conv_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv1500: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(66), T.int64(66)), scope="shared")
        unet_up_blocks_2_upsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(320) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(32) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(24))
                                    v1 = T.axis.spatial(T.int64(640), rc_0)
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(32) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(24) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 < T.int64(48))
                                    T.reads(lv1498[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv1498[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.2.upsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(320) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_up_blocks_2_upsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_2_upsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_2_upsamplers_0_conv_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_upsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(320) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(32) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(320) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(32) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1500[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1500[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d43_add21_add23(lv1504: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_3_resnets_0_conv1_weight: T.Buffer((T.int64(320), T.int64(960), T.int64(3), T.int64(3)), "float32"), lv1506: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv1513: T.Buffer((T.int64(2), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(960), T.int64(66), T.int64(66)), scope="shared")
        unet_up_blocks_3_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(320), T.int64(960), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(960), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(960), rc_0)
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(34))
                                        v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(34))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(102))
                                        T.reads(lv1504[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv1504[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.3.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(960), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_up_blocks_3_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_3_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_3_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_3_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(960), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_3_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_3_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1506[T.int64(0), v1, T.int64(0), T.int64(0)], lv1513[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1506[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1513[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d44_add21(lv1502: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_3_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(320), T.int64(960), T.int64(1), T.int64(1)), "float32"), lv1521: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), scope="shared")
        unet_up_blocks_3_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(320), T.int64(960), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(60), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(40) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(lv1502[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv1502[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("unet.up_blocks.3.resnets.0.conv_shortcut.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(40) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(40) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(unet_up_blocks_3_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                    T.writes(unet_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                    unet_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_3_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(960), rc_0 * T.int64(16) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1521[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1521[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d45_add21_add23(lv1608: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_3_resnets_1_conv1_weight: T.Buffer((T.int64(320), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv1610: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv1617: T.Buffer((T.int64(2), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(66), T.int64(66)), scope="shared")
        unet_up_blocks_3_resnets_1_conv1_weight_shared = T.alloc_buffer((T.int64(320), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(16) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(60))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(16) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(60) // T.int64(6))
                                        v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(6))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(120))
                                        T.reads(lv1608[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv1608[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.3.resnets.1.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(360))
                                        T.reads(unet_up_blocks_3_resnets_1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_3_resnets_1_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_3_resnets_1_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_3_resnets_1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(16) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_3_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_3_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(5), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(16) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1610[T.int64(0), v1, T.int64(0), T.int64(0)], lv1617[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1610[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1617[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d46_add21(lv1606: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_3_resnets_1_conv_shortcut_weight: T.Buffer((T.int64(320), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv1625: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), scope="shared")
        unet_up_blocks_3_resnets_1_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(320), T.int64(640), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(8), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(lv1606[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1606[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("unet.up_blocks.3.resnets.1.conv_shortcut.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(unet_up_blocks_3_resnets_1_conv_shortcut_weight[v0, v1, v2, v3])
                                    T.writes(unet_up_blocks_3_resnets_1_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                    unet_up_blocks_3_resnets_1_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_3_resnets_1_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_3_resnets_1_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_3_resnets_1_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(8), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1625[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1625[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d47_add51(lv1815: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_conv_out_weight: T.Buffer((T.int64(4), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv1817: T.Buffer((T.int64(1), T.int64(4), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(66), T.int64(66)), scope="shared")
        unet_conv_out_weight_shared = T.alloc_buffer((T.int64(4), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(4), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(20), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(33)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(396))
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(396) // T.int64(66))
                                        v3 = T.axis.spatial(T.int64(66), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(66))
                                        T.reads(lv1815[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv1815[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.conv_out.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(144))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(144) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_conv_out_weight[v0, v1, v2, v3])
                                        T.writes(unet_conv_out_weight_shared[v0, v1, v2, v3])
                                        unet_conv_out_weight_shared[v0, v1, v2, v3] = unet_conv_out_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(4), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(16) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_conv_out_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_conv_out_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(4), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1817[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1817[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d4_add7(lv144: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_1_upsamplers_0_conv_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv146: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(258), T.int64(258)), scope="shared")
        vae_decoder_up_blocks_1_upsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(102))
                                    v2 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(102) // T.int64(34))
                                    v3 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(34))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(204))
                                    T.reads(lv144[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(257) and T.int64(1) <= v3 and v3 < T.int64(257), lv144[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.1.upsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(18))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(vae_decoder_up_blocks_1_upsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_1_upsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_1_upsamplers_0_conv_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_1_upsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_1_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_1_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv146[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv146[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d5_add8(lv149: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_conv1_weight: T.Buffer((T.int64(256), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv151: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(258), T.int64(258)), scope="shared")
        vae_decoder_up_blocks_2_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(256), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(96))
                                        v2 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(96) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(258), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(768))
                                        T.reads(lv149[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(257) and T.int64(1) <= v3 and v3 < T.int64(257), lv149[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(12)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("vae.decoder.up_blocks.2.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(24) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(vae_decoder_up_blocks_2_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_2_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_2_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_2_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv151[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv151[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d6_add8(lv164: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_1_conv1_weight: T.Buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), "float32"), lv166: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(258), T.int64(258)), scope="shared")
        vae_decoder_up_blocks_2_resnets_1_conv1_weight_shared = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(180))
                                        v2 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(180) // T.int64(18))
                                        v3 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(720))
                                        T.reads(lv164[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(257) and T.int64(1) <= v3 and v3 < T.int64(257), lv164[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.2.resnets.1.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(576))
                                        T.reads(vae_decoder_up_blocks_2_resnets_1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_2_resnets_1_conv1_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_2_resnets_1_conv1_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_2_resnets_1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv166[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv166[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d6_add8_add9_divide4(lv154: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_conv2_weight: T.Buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), "float32"), lv156: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), lv160: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(258), T.int64(258)), scope="shared")
        vae_decoder_up_blocks_2_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(256), rc_0)
                                    v2 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(34))
                                    v3 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(34))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(102))
                                    T.reads(lv154[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(257) and T.int64(1) <= v3 and v3 < T.int64(257), lv154[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.2.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(256), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(vae_decoder_up_blocks_2_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_2_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_2_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_2_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + ax3)
                            T.reads(lv160[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv156[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv160[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv156[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d7_add8(lv147: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(256), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv159: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), scope="shared")
        vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(256), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(8) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(32) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(128))
                                        T.reads(lv147[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv147[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("vae.decoder.up_blocks.2.resnets.0.conv_shortcut.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(128))
                                        T.reads(vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(8) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv159[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv159[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d8_add10(lv187: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_2_upsamplers_0_conv_weight: T.Buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), "float32"), lv189: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(514), T.int64(514)), scope="shared")
        vae_decoder_up_blocks_2_upsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32768), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8192) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8192) // T.int64(64) * T.int64(4) + yy_3_init * T.int64(4) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(256), rc_0)
                                    v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8192) // T.int64(64) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(60))
                                    T.reads(lv187[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv187[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.2.upsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8192) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(256), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(vae_decoder_up_blocks_2_upsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_2_upsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_2_upsamplers_0_conv_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_2_upsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8192) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8192) // T.int64(64) * T.int64(4) + yy_3 * T.int64(4) + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8192) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8192) // T.int64(64) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv189[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv189[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d9_add11(lv192: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_conv1_weight: T.Buffer((T.int64(128), T.int64(256), T.int64(3), T.int64(3)), "float32"), lv194: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(514), T.int64(514)), scope="shared")
        vae_decoder_up_blocks_3_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(128), T.int64(256), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + xx_3_init * T.int64(8) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(204))
                                    v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(204) // T.int64(34))
                                    v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(34))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(816))
                                    T.reads(lv192[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv192[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.3.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(vae_decoder_up_blocks_3_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_3_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_3_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_3_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + xx_3 * T.int64(8) + xx_4)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_3_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_3_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv194[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv194[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d_add1(lv: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), vae_post_quant_conv_weight: T.Buffer((T.int64(4), T.int64(4), T.int64(1), T.int64(1)), "float32"), lv2: T.Buffer((T.int64(1), T.int64(4), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), scope="shared")
        vae_post_quant_conv_weight_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(4), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(256))
                                        v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.reads(lv[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("vae.post_quant_conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(4), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(8))
                                        T.reads(vae_post_quant_conv_weight[v0, v1, v2, v3])
                                        T.writes(vae_post_quant_conv_weight_shared[v0, v1, v2, v3])
                                        vae_post_quant_conv_weight_shared[v0, v1, v2, v3] = vae_post_quant_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(4), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(4), rc_0 * T.int64(4) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_post_quant_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_post_quant_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv2[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_group_norm10_silu9(lv239: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_norm2_weight: T.Buffer((T.int64(640),), "float32"), unet_down_blocks_1_resnets_0_norm2_bias: T.Buffer((T.int64(640),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(20), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv239[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv239[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv239[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv239[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv239[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)], A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)], unet_down_blocks_1_resnets_0_norm2_weight[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640)], unet_down_blocks_1_resnets_0_norm2_bias[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv239[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_1_resnets_0_norm2_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640)] + unet_down_blocks_1_resnets_0_norm2_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640)]) * T.sigmoid((lv239[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_1_resnets_0_norm2_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640)] + unet_down_blocks_1_resnets_0_norm2_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640)])

    @T.prim_func
    def fused_group_norm12_silu10(lv433: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_norm1_weight: T.Buffer((T.int64(640),), "float32"), unet_down_blocks_2_resnets_0_norm1_bias: T.Buffer((T.int64(640),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(80)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(20), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(lv433[((v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(640), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv433[((v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(640), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv433[((v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(640), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * lv433[((v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(640), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(163840))
                        v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(163840) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(lv433[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)], A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)], A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)], unet_down_blocks_2_resnets_0_norm1_weight[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640)], unet_down_blocks_2_resnets_0_norm1_bias[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv433[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_2_resnets_0_norm1_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640)] + unet_down_blocks_2_resnets_0_norm1_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640)]) * T.sigmoid((lv433[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_2_resnets_0_norm1_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640)] + unet_down_blocks_2_resnets_0_norm1_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640)])

    @T.prim_func
    def fused_group_norm13_silu11(lv445: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_norm2_weight: T.Buffer((T.int64(1280),), "float32"), unet_down_blocks_2_resnets_0_norm2_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(320)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(lv445[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv445[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv445[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * lv445[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(lv445[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)], A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)], A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)], unet_down_blocks_2_resnets_0_norm2_weight[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280)], unet_down_blocks_2_resnets_0_norm2_bias[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv445[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_2_resnets_0_norm2_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280)] + unet_down_blocks_2_resnets_0_norm2_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280)]) * T.sigmoid((lv445[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_2_resnets_0_norm2_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280)] + unet_down_blocks_2_resnets_0_norm2_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280)])

    @T.prim_func
    def fused_group_norm15_silu12(lv639: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), unet_down_blocks_3_resnets_0_norm1_weight: T.Buffer((T.int64(1280),), "float32"), unet_down_blocks_3_resnets_0_norm1_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) // T.int64(64))
                        v_k3 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) % T.int64(64) // T.int64(8))
                        v_k4 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) % T.int64(8))
                        T.reads(lv639[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv639[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv639[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)] * lv639[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(64))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                        v_ax3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv639[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280), (v_ax3 // T.int64(8) + v_ax2) % T.int64(8), v_ax3 % T.int64(8)], A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)], A_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)], unet_down_blocks_3_resnets_0_norm1_weight[((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280)], unet_down_blocks_3_resnets_0_norm1_bias[((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv639[((((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) % T.int64(8), v_ax3 % T.int64(8) % T.int64(8)] - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002) - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002) * (A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_3_resnets_0_norm1_weight[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280)] + unet_down_blocks_3_resnets_0_norm1_bias[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280)]) * T.sigmoid((lv639[((((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) % T.int64(8), v_ax3 % T.int64(8) % T.int64(8)] - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002) - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002) * (A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_3_resnets_0_norm1_weight[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280)] + unet_down_blocks_3_resnets_0_norm1_bias[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280)])

    @T.prim_func
    def fused_group_norm17_silu13(lv797: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32"), unet_up_blocks_0_resnets_0_norm1_weight: T.Buffer((T.int64(2560),), "float32"), unet_up_blocks_0_resnets_0_norm1_bias: T.Buffer((T.int64(2560),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(80)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(80), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) // T.int64(64))
                        v_k3 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(64) // T.int64(8))
                        v_k4 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(8))
                        T.reads(lv797[((v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(2560), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv797[((v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(2560), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv797[((v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(2560), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)] * lv797[((v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(2560), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(163840))
                        v_ax1 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(163840) // T.int64(64))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                        v_ax3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                        T.reads(lv797[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560), (v_ax3 // T.int64(8) + v_ax2) % T.int64(8), v_ax3 % T.int64(8)], A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)], A_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)], unet_up_blocks_0_resnets_0_norm1_weight[((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560)], unet_up_blocks_0_resnets_0_norm1_bias[((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv797[((((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) // T.int64(2560) + (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560), (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) % T.int64(8), v_ax3 % T.int64(8) % T.int64(8)] - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001) - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001) * (A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_0_resnets_0_norm1_weight[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560)] + unet_up_blocks_0_resnets_0_norm1_bias[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560)]) * T.sigmoid((lv797[((((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) // T.int64(2560) + (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560), (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) % T.int64(8), v_ax3 % T.int64(8) % T.int64(8)] - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001) - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001) * (A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_0_resnets_0_norm1_weight[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560)] + unet_up_blocks_0_resnets_0_norm1_bias[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560)])

    @T.prim_func
    def fused_group_norm18_silu14(lv870: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_1_resnets_0_norm1_weight: T.Buffer((T.int64(2560),), "float32"), unet_up_blocks_1_resnets_0_norm1_bias: T.Buffer((T.int64(2560),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(80), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(lv870[((v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(2560), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv870[((v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(2560), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv870[((v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(2560), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * lv870[((v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(2560), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(lv870[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)], A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)], A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)], unet_up_blocks_1_resnets_0_norm1_weight[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560)], unet_up_blocks_1_resnets_0_norm1_bias[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv870[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) // T.int64(2560) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_1_resnets_0_norm1_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560)] + unet_up_blocks_1_resnets_0_norm1_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560)]) * T.sigmoid((lv870[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) // T.int64(2560) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_1_resnets_0_norm1_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560)] + unet_up_blocks_1_resnets_0_norm1_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560)])

    @T.prim_func
    def fused_group_norm19_silu15(lv1078: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_1_resnets_2_norm1_weight: T.Buffer((T.int64(1920),), "float32"), unet_up_blocks_1_resnets_2_norm1_bias: T.Buffer((T.int64(1920),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(60)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(60), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(lv1078[((v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1920), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv1078[((v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1920), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv1078[((v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1920), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * lv1078[((v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1920), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(15)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(491520))
                        v_ax1 = T.axis.spatial(T.int64(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(491520) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(lv1078[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)], A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)], A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)], unet_up_blocks_1_resnets_2_norm1_weight[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920)], unet_up_blocks_1_resnets_2_norm1_bias[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1078[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) // T.int64(1920) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_1_resnets_2_norm1_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920)] + unet_up_blocks_1_resnets_2_norm1_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920)]) * T.sigmoid((lv1078[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) // T.int64(1920) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_1_resnets_2_norm1_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920)] + unet_up_blocks_1_resnets_2_norm1_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920)])

    @T.prim_func
    def fused_group_norm20_silu16(lv1186: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_0_norm1_weight: T.Buffer((T.int64(1920),), "float32"), unet_up_blocks_2_resnets_0_norm1_bias: T.Buffer((T.int64(1920),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(240)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(60), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv1186[((v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1920), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv1186[((v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1920), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv1186[((v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1920), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv1186[((v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1920), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(60)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1966080))
                        v_ax1 = T.axis.spatial(T.int64(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1966080) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv1186[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)], A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)], unet_up_blocks_2_resnets_0_norm1_weight[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920)], unet_up_blocks_2_resnets_0_norm1_bias[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1186[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) // T.int64(1920) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_2_resnets_0_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920)] + unet_up_blocks_2_resnets_0_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920)]) * T.sigmoid((lv1186[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) // T.int64(1920) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_2_resnets_0_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920)] + unet_up_blocks_2_resnets_0_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920)])

    @T.prim_func
    def fused_group_norm21_silu17(lv1290: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_1_norm1_weight: T.Buffer((T.int64(1280),), "float32"), unet_up_blocks_2_resnets_1_norm1_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv1290[((v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1280), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv1290[((v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1280), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv1290[((v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1280), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv1290[((v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1280), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv1290[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)], A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)], unet_up_blocks_2_resnets_1_norm1_weight[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280)], unet_up_blocks_2_resnets_1_norm1_bias[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1290[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_2_resnets_1_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280)] + unet_up_blocks_2_resnets_1_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280)]) * T.sigmoid((lv1290[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_2_resnets_1_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280)] + unet_up_blocks_2_resnets_1_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280)])

    @T.prim_func
    def fused_group_norm22_silu18(lv1394: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_2_norm1_weight: T.Buffer((T.int64(960),), "float32"), unet_up_blocks_2_resnets_2_norm1_bias: T.Buffer((T.int64(960),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(120)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(30), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv1394[((v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(960), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv1394[((v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(960), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv1394[((v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(960), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv1394[((v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(960), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(30)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(983040))
                        v_ax1 = T.axis.spatial(T.int64(960), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(983040) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv1394[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)], A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)], unet_up_blocks_2_resnets_2_norm1_weight[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960)], unet_up_blocks_2_resnets_2_norm1_bias[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1394[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) // T.int64(960) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_2_resnets_2_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960)] + unet_up_blocks_2_resnets_2_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960)]) * T.sigmoid((lv1394[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) // T.int64(960) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_2_resnets_2_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960)] + unet_up_blocks_2_resnets_2_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960)])

    @T.prim_func
    def fused_group_norm23_silu19(lv1502: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_3_resnets_0_norm1_weight: T.Buffer((T.int64(960),), "float32"), unet_up_blocks_3_resnets_0_norm1_bias: T.Buffer((T.int64(960),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(480)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(30), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(lv1502[((v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(960), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv1502[((v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(960), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv1502[((v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(960), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * lv1502[((v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(960), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(120)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3932160))
                        v_ax1 = T.axis.spatial(T.int64(960), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3932160) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(lv1502[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)], A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)], A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)], unet_up_blocks_3_resnets_0_norm1_weight[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960)], unet_up_blocks_3_resnets_0_norm1_bias[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1502[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) // T.int64(960) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06) - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06) * (A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_3_resnets_0_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960)] + unet_up_blocks_3_resnets_0_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960)]) * T.sigmoid((lv1502[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) // T.int64(960) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06) - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06) * (A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_3_resnets_0_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960)] + unet_up_blocks_3_resnets_0_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960)])

    @T.prim_func
    def fused_group_norm24_silu20(lv1606: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_3_resnets_1_norm1_weight: T.Buffer((T.int64(640),), "float32"), unet_up_blocks_3_resnets_1_norm1_bias: T.Buffer((T.int64(640),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(320)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(20), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(lv1606[((v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(640), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv1606[((v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(640), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv1606[((v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(640), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * lv1606[((v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(640), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(80)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2621440))
                        v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2621440) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(lv1606[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)], A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)], A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)], unet_up_blocks_3_resnets_1_norm1_weight[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640)], unet_up_blocks_3_resnets_1_norm1_bias[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1606[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05) - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_3_resnets_1_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640)] + unet_up_blocks_3_resnets_1_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640)]) * T.sigmoid((lv1606[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05) - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_3_resnets_1_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640)] + unet_up_blocks_3_resnets_1_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640)])

    @T.prim_func
    def fused_group_norm2_silu1(lv107: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), vae_decoder_up_blocks_1_resnets_0_norm1_weight: T.Buffer((T.int64(512),), "float32"), vae_decoder_up_blocks_1_resnets_0_norm1_bias: T.Buffer((T.int64(512),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(2048)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(16384))
                        v_k3 = T.axis.reduce(T.int64(128), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(16384) // T.int64(128))
                        v_k4 = T.axis.reduce(T.int64(128), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(128))
                        T.reads(lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(128) + v_k3) // T.int64(128) + v_k2) % T.int64(512), (v_k4 // T.int64(128) + v_k3) % T.int64(128), v_k4 % T.int64(128)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(128) + v_k3) // T.int64(128) + v_k2) % T.int64(512), (v_k4 // T.int64(128) + v_k3) % T.int64(128), v_k4 % T.int64(128)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(128) + v_k3) // T.int64(128) + v_k2) % T.int64(512), (v_k4 // T.int64(128) + v_k3) % T.int64(128), v_k4 % T.int64(128)] * lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(128) + v_k3) // T.int64(128) + v_k2) % T.int64(512), (v_k4 // T.int64(128) + v_k3) % T.int64(128), v_k4 % T.int64(128)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(128)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16384))
                        v_ax2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16384) // T.int64(128))
                        v_ax3 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                        T.reads(lv107[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512), (v_ax3 // T.int64(128) + v_ax2) % T.int64(128), v_ax3 % T.int64(128)], A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)], A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)], vae_decoder_up_blocks_1_resnets_0_norm1_weight[((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512)], vae_decoder_up_blocks_1_resnets_0_norm1_bias[((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv107[T.int64(0), (((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(128) // T.int64(128) + (v_ax3 // T.int64(128) + v_ax2) % T.int64(128)) // T.int64(128) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(128) // T.int64(128) + (v_ax3 // T.int64(128) + v_ax2) % T.int64(128)) % T.int64(128), v_ax3 % T.int64(128) % T.int64(128)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_1_resnets_0_norm1_weight[(((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512)] + vae_decoder_up_blocks_1_resnets_0_norm1_bias[(((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512)]) * T.sigmoid((lv107[T.int64(0), (((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(128) // T.int64(128) + (v_ax3 // T.int64(128) + v_ax2) % T.int64(128)) // T.int64(128) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(128) // T.int64(128) + (v_ax3 // T.int64(128) + v_ax2) % T.int64(128)) % T.int64(128), v_ax3 % T.int64(128) % T.int64(128)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_1_resnets_0_norm1_weight[(((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512)] + vae_decoder_up_blocks_1_resnets_0_norm1_bias[(((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512)])

    @T.prim_func
    def fused_group_norm3_silu2(lv147: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_norm1_weight: T.Buffer((T.int64(512),), "float32"), vae_decoder_up_blocks_2_resnets_0_norm1_bias: T.Buffer((T.int64(512),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(8192)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(65536))
                        v_k3 = T.axis.reduce(T.int64(256), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(65536) // T.int64(256))
                        v_k4 = T.axis.reduce(T.int64(256), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(256))
                        T.reads(lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(512), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(512), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(512), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)] * lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(512), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(512)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(65536))
                        v_ax2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(65536) // T.int64(256))
                        v_ax3 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256))
                        T.reads(lv147[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512), (v_ax3 // T.int64(256) + v_ax2) % T.int64(256), v_ax3 % T.int64(256)], A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)], A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)], vae_decoder_up_blocks_2_resnets_0_norm1_weight[((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512)], vae_decoder_up_blocks_2_resnets_0_norm1_bias[((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv147[T.int64(0), (((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) // T.int64(256) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) % T.int64(256), v_ax3 % T.int64(256) % T.int64(256)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_2_resnets_0_norm1_weight[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512)] + vae_decoder_up_blocks_2_resnets_0_norm1_bias[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512)]) * T.sigmoid((lv147[T.int64(0), (((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) // T.int64(256) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) % T.int64(256), v_ax3 % T.int64(256) % T.int64(256)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_2_resnets_0_norm1_weight[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512)] + vae_decoder_up_blocks_2_resnets_0_norm1_bias[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512)])

    @T.prim_func
    def fused_group_norm4_silu3(lv152: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_norm2_weight: T.Buffer((T.int64(256),), "float32"), vae_decoder_up_blocks_2_resnets_0_norm2_bias: T.Buffer((T.int64(256),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(4096)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(65536))
                        v_k3 = T.axis.reduce(T.int64(256), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(65536) // T.int64(256))
                        v_k4 = T.axis.reduce(T.int64(256), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(256))
                        T.reads(lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(256), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(256), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(256), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)] * lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(256), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(256)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(65536))
                        v_ax2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(65536) // T.int64(256))
                        v_ax3 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256))
                        T.reads(lv152[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256), (v_ax3 // T.int64(256) + v_ax2) % T.int64(256), v_ax3 % T.int64(256)], A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)], A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)], vae_decoder_up_blocks_2_resnets_0_norm2_weight[((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256)], vae_decoder_up_blocks_2_resnets_0_norm2_bias[((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv152[T.int64(0), (((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) // T.int64(256) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256), (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) % T.int64(256), v_ax3 % T.int64(256) % T.int64(256)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_2_resnets_0_norm2_weight[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256)] + vae_decoder_up_blocks_2_resnets_0_norm2_bias[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256)]) * T.sigmoid((lv152[T.int64(0), (((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) // T.int64(256) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256), (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) % T.int64(256), v_ax3 % T.int64(256) % T.int64(256)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_2_resnets_0_norm2_weight[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256)] + vae_decoder_up_blocks_2_resnets_0_norm2_bias[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256)])

    @T.prim_func
    def fused_group_norm5_silu4(lv190: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_norm1_weight: T.Buffer((T.int64(256),), "float32"), vae_decoder_up_blocks_3_resnets_0_norm1_bias: T.Buffer((T.int64(256),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(16384)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(262144))
                        v_k3 = T.axis.reduce(T.int64(512), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(262144) // T.int64(512))
                        v_k4 = T.axis.reduce(T.int64(512), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(512))
                        T.reads(lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(256), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(256), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(256), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)] * lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(256), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1024)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(262144))
                        v_ax2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(262144) // T.int64(512))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv190[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256), (v_ax3 // T.int64(512) + v_ax2) % T.int64(512), v_ax3 % T.int64(512)], A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)], A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)], vae_decoder_up_blocks_3_resnets_0_norm1_weight[((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256)], vae_decoder_up_blocks_3_resnets_0_norm1_bias[((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv190[T.int64(0), (((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) // T.int64(512) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256), (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) % T.int64(512), v_ax3 % T.int64(512) % T.int64(512)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_3_resnets_0_norm1_weight[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256)] + vae_decoder_up_blocks_3_resnets_0_norm1_bias[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256)]) * T.sigmoid((lv190[T.int64(0), (((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) // T.int64(512) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256), (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) % T.int64(512), v_ax3 % T.int64(512) % T.int64(512)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_3_resnets_0_norm1_weight[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256)] + vae_decoder_up_blocks_3_resnets_0_norm1_bias[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256)])

    @T.prim_func
    def fused_group_norm6_silu5(lv195: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_norm2_weight: T.Buffer((T.int64(128),), "float32"), vae_decoder_up_blocks_3_resnets_0_norm2_bias: T.Buffer((T.int64(128),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(8192)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(4), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(262144))
                        v_k3 = T.axis.reduce(T.int64(512), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(262144) // T.int64(512))
                        v_k4 = T.axis.reduce(T.int64(512), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(512))
                        T.reads(lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(128), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(128), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(128), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)] * lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(128), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(512)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(262144))
                        v_ax2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(262144) // T.int64(512))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv195[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128), (v_ax3 // T.int64(512) + v_ax2) % T.int64(512), v_ax3 % T.int64(512)], A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)], A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)], vae_decoder_up_blocks_3_resnets_0_norm2_weight[((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128)], vae_decoder_up_blocks_3_resnets_0_norm2_bias[((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv195[T.int64(0), (((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) // T.int64(512) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128), (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) % T.int64(512), v_ax3 % T.int64(512) % T.int64(512)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_3_resnets_0_norm2_weight[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128)] + vae_decoder_up_blocks_3_resnets_0_norm2_bias[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128)]) * T.sigmoid((lv195[T.int64(0), (((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) // T.int64(512) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128), (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) % T.int64(512), v_ax3 % T.int64(512) % T.int64(512)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_3_resnets_0_norm2_weight[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128)] + vae_decoder_up_blocks_3_resnets_0_norm2_bias[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128)])

    @T.prim_func
    def fused_group_norm7_silu7(lv24: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_resnets_0_norm1_weight: T.Buffer((T.int64(320),), "float32"), unet_down_blocks_0_resnets_0_norm1_bias: T.Buffer((T.int64(320),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(10), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(lv24[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv24[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv24[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * lv24[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(lv24[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)], A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)], A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)], unet_down_blocks_0_resnets_0_norm1_weight[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320)], unet_down_blocks_0_resnets_0_norm1_bias[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv24[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) // T.int64(320) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05) - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_0_resnets_0_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320)] + unet_down_blocks_0_resnets_0_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320)]) * T.sigmoid((lv24[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) // T.int64(320) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05) - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_0_resnets_0_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320)] + unet_down_blocks_0_resnets_0_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320)])

    @T.prim_func
    def fused_group_norm9_silu8(lv227: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_norm1_weight: T.Buffer((T.int64(320),), "float32"), unet_down_blocks_1_resnets_0_norm1_bias: T.Buffer((T.int64(320),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(40)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(10), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv227[((v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(320), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv227[((v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(320), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv227[((v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(320), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv227[((v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(320), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv227[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)], A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)], unet_down_blocks_1_resnets_0_norm1_weight[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320)], unet_down_blocks_1_resnets_0_norm1_bias[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv227[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) // T.int64(320) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_1_resnets_0_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320)] + unet_down_blocks_1_resnets_0_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320)]) * T.sigmoid((lv227[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) // T.int64(320) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_1_resnets_0_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320)] + unet_down_blocks_1_resnets_0_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320)])

    @T.prim_func
    def fused_group_norm_silu(lv6: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), vae_decoder_mid_block_resnets_0_norm1_weight: T.Buffer((T.int64(512),), "float32"), vae_decoder_mid_block_resnets_0_norm1_bias: T.Buffer((T.int64(512),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(512)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(lv6[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv6[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv6[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * lv6[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(lv6[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)], A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)], A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)], vae_decoder_mid_block_resnets_0_norm1_weight[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512)], vae_decoder_mid_block_resnets_0_norm1_bias[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv6[T.int64(0), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * vae_decoder_mid_block_resnets_0_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512)] + vae_decoder_mid_block_resnets_0_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512)]) * T.sigmoid((lv6[T.int64(0), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * vae_decoder_mid_block_resnets_0_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512)] + vae_decoder_mid_block_resnets_0_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512)])

    @T.prim_func
    def fused_matmul10_add22_strided_slice3(lv30: T.Buffer((T.int64(2), T.int64(1280)), "float32"), lv31: T.Buffer((T.int64(1280), T.int64(320)), "float32"), unet_down_blocks_0_resnets_0_time_emb_proj_bias: T.Buffer((T.int64(320),), "float32"), var_T_strided_slice_with_axes_intermediate: T.Buffer((T.int64(2), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320)), scope="local")
        lv30_shared = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="shared")
        lv31_shared = T.alloc_buffer((T.int64(1280), T.int64(320)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(20) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(320), i0_0_i1_0_fused * T.int64(20) + i0_2_i1_2_fused % T.int64(20) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(8)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("lv30_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(1280), k_0 * T.int64(160) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) % T.int64(160))
                                    T.reads(lv30[v0, v1])
                                    T.writes(lv30_shared[v0, v1])
                                    lv30_shared[v0, v1] = lv30[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(20)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv31_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(160) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(20))
                                        v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_fused * T.int64(20) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(20))
                                        T.reads(lv31[v0, v1])
                                        T.writes(lv31_shared[v0, v1])
                                        lv31_shared[v0, v1] = lv31[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(10), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(20) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(320), i0_0_i1_0_fused * T.int64(20) + i0_2_i1_2_fused % T.int64(20) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(160) + k_1 * T.int64(16) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1], lv30_shared[v_i0, v_k], lv31_shared[v_k, v_i1])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1] = var_matmul_intermediate_local[v_i0, v_i1] + lv30_shared[v_i0, v_k] * lv31_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(20) + ax0)
                            v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_fused * T.int64(20) + i0_2_i1_2_fused % T.int64(20) + ax1)
                            T.reads(var_matmul_intermediate_local[v0, v1], unet_down_blocks_0_resnets_0_time_emb_proj_bias[v1])
                            T.writes(var_T_strided_slice_with_axes_intermediate[v0, v1])
                            var_T_strided_slice_with_axes_intermediate[v0, v1] = var_matmul_intermediate_local[v0, v1] + unet_down_blocks_0_resnets_0_time_emb_proj_bias[v1]

    @T.prim_func
    def fused_matmul11_add25_add26(lv73: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), lv74: T.Buffer((T.int64(320), T.int64(320)), "float32"), unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: T.Buffer((T.int64(320),), "float32"), lv49: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="local")
        lv73_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="shared")
        lv74_shared = T.alloc_buffer((T.int64(320), T.int64(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2560), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1280) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(1280) // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3_init * T.int64(4) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(20)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv73_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1280))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(1280) // T.int64(10) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(320), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(lv73[v0, v1, v2])
                                    T.writes(lv73_shared[v0, v1, v2])
                                    lv73_shared[v0, v1, v2] = lv73[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv74_shared"):
                                        v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(lv74[v0, v1])
                                        T.writes(lv74_shared[v0, v1])
                                        lv74_shared[v0, v1] = lv74[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(8), T.int64(1), T.int64(4), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1280) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(1280) // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3 * T.int64(4) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(16) + k_1 * T.int64(8) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv73_shared[v_i0, v_i1, v_k], lv74_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv73_shared[v_i0, v_i1, v_k] * lv74_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1280) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(1280) // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2], lv49[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2] + lv49[v0, v1, v2]

    @T.prim_func
    def fused_matmul12_multiply14(lv59: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32"), lv66: T.Buffer((T.int64(16), T.int64(40), T.int64(4096)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(4096)), scope="local")
        lv59_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="shared")
        lv66_shared = T.alloc_buffer((T.int64(16), T.int64(40), T.int64(4096)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(262144), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(16384) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(16384) // T.int64(256) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(5)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv59_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(16384))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(16384) // T.int64(256) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(40), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.reads(lv59[v0, v1, v2])
                                        T.writes(lv59_shared[v0, v1, v2])
                                        lv59_shared[v0, v1, v2] = lv59[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv66_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(16384))
                                        v1 = T.axis.spatial(T.int64(40), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(16))
                                        T.reads(lv66[v0, v1, v2])
                                        T.writes(lv66_shared[v0, v1, v2])
                                        lv66_shared[v0, v1, v2] = lv66[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(8), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(16384) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(16384) // T.int64(256) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(40), k_0 * T.int64(8) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv59_shared[v_i0, v_i1, v_k], lv66_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv59_shared[v_i0, v_i1, v_k] * lv66_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(16384) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(16384) // T.int64(256) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.15811388194561005)

    @T.prim_func
    def fused_matmul15_multiply15(lv87: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32"), lv94: T.Buffer((T.int64(16), T.int64(40), T.int64(77)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(4096), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(77)), scope="local")
        lv87_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="shared")
        lv94_shared = T.alloc_buffer((T.int64(16), T.int64(40), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(11) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(11) * T.int64(7) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(5)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv87_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(352) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(40), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(352) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(128))
                                        T.reads(lv87[v0, v1, v2])
                                        T.writes(lv87_shared[v0, v1, v2])
                                        lv87_shared[v0, v1, v2] = lv87[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                with T.block("lv94_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(40), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) // T.int64(77))
                                    v2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) % T.int64(77))
                                    T.reads(lv94[v0, v1, v2])
                                    T.writes(lv94_shared[v0, v1, v2])
                                    lv94_shared[v0, v1, v2] = lv94[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(11) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(11) * T.int64(7) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(40), k_0 * T.int64(8) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv87_shared[v_i0, v_i1, v_k], lv94_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv87_shared[v_i0, v_i1, v_k] * lv94_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(7)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(11) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(11) * T.int64(7) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.15811388194561005)

    @T.prim_func
    def fused_matmul17_add27_gelu(lv106: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), lv110: T.Buffer((T.int64(320), T.int64(1280)), "float32"), unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(4096), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(1280)))
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(1280)), scope="local")
        lv106_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="shared")
        lv110_shared = T.alloc_buffer((T.int64(320), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(10240), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(16) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(8) + i1_3_init * T.int64(4) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(20)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("lv106_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(128) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(320), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(lv106[v0, v1, v2])
                                    T.writes(lv106_shared[v0, v1, v2])
                                    lv106_shared[v0, v1, v2] = lv106[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("lv110_shared"):
                                    v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(lv110[v0, v1])
                                    T.writes(lv110_shared[v0, v1])
                                    lv110_shared[v0, v1] = lv110[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(16) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(8) + i1_3 * T.int64(4) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(16) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv106_shared[v_i0, v_i1, v_k], lv110_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv106_shared[v_i0, v_i1, v_k] * lv110_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_matmul_intermediate[v0, v1, v2])
                            var_matmul_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(160)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(5242880))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(5242880) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1280))
                        T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = (var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * (T.float32(0.5) + T.erf((var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul17_add27_multiply16(lv106: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), lv107: T.Buffer((T.int64(320), T.int64(1280)), "float32"), unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias: T.Buffer((T.int64(1280),), "float32"), lv113: T.Buffer((T.int64(2), T.int64(4096), T.int64(1280)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(4096), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(1280)), scope="local")
        lv106_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="shared")
        lv107_shared = T.alloc_buffer((T.int64(320), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(2) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(40) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(160) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(80) + i0_2_i1_2_i2_2_fused % T.int64(40) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv106_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(32) // T.int64(4))
                                        v2 = T.axis.spatial(T.int64(320), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(4))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(64))
                                        T.reads(lv106[v0, v1, v2])
                                        T.writes(lv106_shared[v0, v1, v2])
                                        lv106_shared[v0, v1, v2] = lv106[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv107_shared"):
                                        v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(640) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(160))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(160) + (ax0_ax1_fused_0 * T.int64(640) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(160))
                                        T.reads(lv107[v0, v1])
                                        T.writes(lv107_shared[v0, v1])
                                        lv107_shared[v0, v1] = lv107[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(2) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(40) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(160) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(80) + i0_2_i1_2_i2_2_fused % T.int64(40) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(4) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv106_shared[v_i0, v_i1, v_k], lv107_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv106_shared[v_i0, v_i1, v_k] * lv107_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(40) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(160) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(80) + i0_2_i1_2_i2_2_fused % T.int64(40) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2], lv113[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = (var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2]) * lv113[v0, v1, v2]

    @T.prim_func
    def fused_matmul18_add25_add26(lv114: T.Buffer((T.int64(2), T.int64(4096), T.int64(1280)), "float32"), lv115: T.Buffer((T.int64(1280), T.int64(320)), "float32"), unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias: T.Buffer((T.int64(320),), "float32"), lv105: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="local")
        lv114_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(1280)), scope="shared")
        lv115_shared = T.alloc_buffer((T.int64(1280), T.int64(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(80) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(80) // T.int64(20) * T.int64(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv114_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(640))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(640) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.reads(lv114[v0, v1, v2])
                                        T.writes(lv114_shared[v0, v1, v2])
                                        lv114_shared[v0, v1, v2] = lv114[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv115_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(640) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(40))
                                        v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(40) + (ax0_ax1_fused_0 * T.int64(640) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(40))
                                        T.where((ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(1600))
                                        T.reads(lv115[v0, v1])
                                        T.writes(lv115_shared[v0, v1])
                                        lv115_shared[v0, v1] = lv115[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(40), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(80) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(80) // T.int64(20) * T.int64(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(40) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv114_shared[v_i0, v_i1, v_k], lv115_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv114_shared[v_i0, v_i1, v_k] * lv115_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(80) // T.int64(20) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias[v2], lv105[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias[v2] + lv105[v0, v1, v2]

    @T.prim_func
    def fused_matmul19_add30_strided_slice4(lv233: T.Buffer((T.int64(2), T.int64(1280)), "float32"), lv234: T.Buffer((T.int64(1280), T.int64(640)), "float32"), unet_down_blocks_1_resnets_0_time_emb_proj_bias: T.Buffer((T.int64(640),), "float32"), var_T_strided_slice_with_axes_intermediate: T.Buffer((T.int64(2), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640)), scope="local")
        lv233_shared = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="shared")
        lv234_shared = T.alloc_buffer((T.int64(1280), T.int64(640)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(4) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(640), i0_0_i1_0_fused % T.int64(4) * T.int64(160) + i0_1_i1_1_fused * T.int64(80) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(40)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv233_shared"):
                                        v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(1280), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(32))
                                        T.reads(lv233[v0, v1])
                                        T.writes(lv233_shared[v0, v1])
                                        lv233_shared[v0, v1] = lv233[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv234_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(320) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(160))
                                        v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_fused % T.int64(4) * T.int64(160) + (ax0_ax1_fused_0 * T.int64(320) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(160))
                                        T.reads(lv234[v0, v1])
                                        T.writes(lv234_shared[v0, v1])
                                        lv234_shared[v0, v1] = lv234[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(4) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(640), i0_0_i1_0_fused % T.int64(4) * T.int64(160) + i0_1_i1_1_fused * T.int64(80) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(32) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1], lv233_shared[v_i0, v_k], lv234_shared[v_k, v_i1])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1] = var_matmul_intermediate_local[v_i0, v_i1] + lv233_shared[v_i0, v_k] * lv234_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(4) + ax0)
                            v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_fused % T.int64(4) * T.int64(160) + i0_1_i1_1_fused * T.int64(80) + i0_2_i1_2_fused + ax1)
                            T.reads(var_matmul_intermediate_local[v0, v1], unet_down_blocks_1_resnets_0_time_emb_proj_bias[v1])
                            T.writes(var_T_strided_slice_with_axes_intermediate[v0, v1])
                            var_T_strided_slice_with_axes_intermediate[v0, v1] = var_matmul_intermediate_local[v0, v1] + unet_down_blocks_1_resnets_0_time_emb_proj_bias[v1]

    @T.prim_func
    def fused_matmul1_multiply7(lv34: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32"), lv41: T.Buffer((T.int64(1), T.int64(1), T.int64(512), T.int64(4096)), "float32"), param_0: T.Buffer((), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), scope="local")
        lv34_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), scope="shared")
        lv41_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(512), T.int64(4096)), scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i3_3_init, i0_4_init, i1_4_init, i2_4_init, i3_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1), i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_i3_2_fused // T.int64(8) * T.int64(4) + i2_3_init + i2_4_init)
                            v_i3 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused % T.int64(32) * T.int64(128) + i0_1_i1_1_i2_1_i3_1_fused * T.int64(64) + i0_2_i1_2_i2_2_i3_2_fused % T.int64(8) * T.int64(8) + i3_3_init * T.int64(4) + i3_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2, v_i3])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv34_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(32))
                                        T.reads(lv34[v0, v1, v2, v3])
                                        T.writes(lv34_shared[v0, v1, v2, v3])
                                        lv34_shared[v0, v1, v2, v3] = lv34[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv41_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v3 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused % T.int64(32) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        T.reads(lv41[v0, v1, v2, v3])
                                        T.writes(lv41_shared[v0, v1, v2, v3])
                                        lv41_shared[v0, v1, v2, v3] = lv41[v0, v1, v2, v3]
                        for k_1, i0_3, i1_3, i2_3, i3_3, k_2, i0_4, i1_4, i2_4, i3_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1), i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_i3_2_fused // T.int64(8) * T.int64(4) + i2_3 + i2_4)
                                v_i3 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused % T.int64(32) * T.int64(128) + i0_1_i1_1_i2_1_i3_1_fused * T.int64(64) + i0_2_i1_2_i2_2_i3_2_fused % T.int64(8) * T.int64(8) + i3_3 * T.int64(4) + i3_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2, v_i3], lv34_shared[v_i0, v_i1, v_i2, v_k], lv41_shared[v_i0, v_i1, v_k, v_i3])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2, v_i3])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2, v_i3] = var_matmul_intermediate_local[v_i0, v_i1, v_i2, v_i3] + lv34_shared[v_i0, v_i1, v_i2, v_k] * lv41_shared[v_i0, v_i1, v_k, v_i3]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(8)):
                        with T.block("var_matmul_intermediate_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_i3_2_fused // T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused % T.int64(32) * T.int64(128) + i0_1_i1_1_i2_1_i3_1_fused * T.int64(64) + i0_2_i1_2_i2_2_i3_2_fused % T.int64(8) * T.int64(8) + ax3)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2, v3], param_0[()])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2, v3])
                            var_T_multiply_intermediate[v0, v1, v2, v3] = var_matmul_intermediate_local[v0, v1, v2, v3] * param_0[()]

    @T.prim_func
    def fused_matmul20_add33_add34(lv279: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), lv280: T.Buffer((T.int64(640), T.int64(640)), "float32"), unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: T.Buffer((T.int64(640),), "float32"), lv255: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="local")
        lv279_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="shared")
        lv280_shared = T.alloc_buffer((T.int64(640), T.int64(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(5), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv279_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(512))
                                        v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(512) // T.int64(4))
                                        v2 = T.axis.spatial(T.int64(640), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(4))
                                        T.reads(lv279[v0, v1, v2])
                                        T.writes(lv279_shared[v0, v1, v2])
                                        lv279_shared[v0, v1, v2] = lv279[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv280_shared"):
                                        v0 = T.axis.spatial(T.int64(640), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(40))
                                        v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(40))
                                        T.where((ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(160))
                                        T.reads(lv280[v0, v1])
                                        T.writes(lv280_shared[v0, v1])
                                        lv280_shared[v0, v1] = lv280[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(5), T.int64(4), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(640), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv279_shared[v_i0, v_i1, v_k], lv280_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv279_shared[v_i0, v_i1, v_k] * lv280_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(2), T.int64(5)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2], lv255[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2] + lv255[v0, v1, v2]

    @T.prim_func
    def fused_matmul21_multiply17(lv265: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32"), lv272: T.Buffer((T.int64(16), T.int64(80), T.int64(1024)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(1024), T.int64(1024)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(1024)), scope="local")
        lv265_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="shared")
        lv272_shared = T.alloc_buffer((T.int64(16), T.int64(80), T.int64(1024)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2048) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(64) * T.int64(2) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(2048) // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(4) + i2_3_init * T.int64(4) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(5)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv265_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2048) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(2048) // T.int64(32) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(256) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(80), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(16))
                                        T.reads(lv265[v0, v1, v2])
                                        T.writes(lv265_shared[v0, v1, v2])
                                        lv265_shared[v0, v1, v2] = lv265[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("lv272_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2048) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(80), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(512) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.reads(lv272[v0, v1, v2])
                                    T.writes(lv272_shared[v0, v1, v2])
                                    lv272_shared[v0, v1, v2] = lv272[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2048) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(64) * T.int64(2) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(2048) // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(4) + i2_3 * T.int64(4) + i2_4)
                                v_k = T.axis.reduce(T.int64(80), k_0 * T.int64(16) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv265_shared[v_i0, v_i1, v_k], lv272_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv265_shared[v_i0, v_i1, v_k] * lv272_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(1), T.int64(4)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2048) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(64) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(2048) // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(4) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.11180339753627777)

    @T.prim_func
    def fused_matmul24_multiply18(lv293: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32"), lv300: T.Buffer((T.int64(16), T.int64(80), T.int64(77)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(1024), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(77)), scope="local")
        lv293_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="shared")
        lv300_shared = T.alloc_buffer((T.int64(16), T.int64(80), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1408), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(88) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(88) // T.int64(11) * T.int64(128) + i0_1_i1_1_i2_1_fused // T.int64(7) * T.int64(64) + i0_2_i1_2_i2_2_fused + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_1_i1_1_i2_1_fused % T.int64(7) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(10)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv293_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(88))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(88) // T.int64(11) * T.int64(128) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(80), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(lv293[v0, v1, v2])
                                    T.writes(lv293_shared[v0, v1, v2])
                                    lv293_shared[v0, v1, v2] = lv293[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv300_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(88))
                                    v1 = T.axis.spatial(T.int64(80), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(7))
                                    v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 < T.int64(56))
                                    T.reads(lv300[v0, v1, v2])
                                    T.writes(lv300_shared[v0, v1, v2])
                                    lv300_shared[v0, v1, v2] = lv300[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(88) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(88) // T.int64(11) * T.int64(128) + i0_1_i1_1_i2_1_fused // T.int64(7) * T.int64(64) + i0_2_i1_2_i2_2_fused + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_1_i1_1_i2_1_fused % T.int64(7) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(80), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv293_shared[v_i0, v_i1, v_k], lv300_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv293_shared[v_i0, v_i1, v_k] * lv300_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(88) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(88) // T.int64(11) * T.int64(128) + i0_1_i1_1_i2_1_fused // T.int64(7) * T.int64(64) + i0_2_i1_2_i2_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_1_i1_1_i2_1_fused % T.int64(7) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.11180339753627777)

    @T.prim_func
    def fused_matmul26_add35_gelu1(lv312: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), lv316: T.Buffer((T.int64(640), T.int64(2560)), "float32"), unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias: T.Buffer((T.int64(2560),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1024), T.int64(2560)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(2560)))
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(2560)), scope="local")
        lv312_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="shared")
        lv316_shared = T.alloc_buffer((T.int64(640), T.int64(2560)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1024) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(1024) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(10) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(20)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv312_shared"):
                                        v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(1024) // T.int64(32) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(640) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(640), k_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(640) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(1024))
                                        T.reads(lv312[v0, v1, v2])
                                        T.writes(lv312_shared[v0, v1, v2])
                                        lv312_shared[v0, v1, v2] = lv312[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv316_shared"):
                                        v0 = T.axis.spatial(T.int64(640), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(320) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(80))
                                        v1 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(80) + (ax0_ax1_fused_0 * T.int64(320) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(80))
                                        T.reads(lv316[v0, v1])
                                        T.writes(lv316_shared[v0, v1])
                                        lv316_shared[v0, v1] = lv316[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1024) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(1024) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(10) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(640), k_0 * T.int64(32) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv312_shared[v_i0, v_i1, v_k], lv316_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv312_shared[v_i0, v_i1, v_k] * lv316_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(1024) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(10) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_matmul_intermediate[v0, v1, v2])
                            var_matmul_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(80)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(2621440))
                        v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(2621440) // T.int64(2560))
                        v_ax2 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(2560))
                        T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = (var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * (T.float32(0.5) + T.erf((var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul26_add35_multiply19(lv312: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), lv313: T.Buffer((T.int64(640), T.int64(2560)), "float32"), unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias: T.Buffer((T.int64(2560),), "float32"), lv319: T.Buffer((T.int64(2), T.int64(1024), T.int64(2560)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1024), T.int64(2560)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(2560)), scope="local")
        lv312_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="shared")
        lv313_shared = T.alloc_buffer((T.int64(640), T.int64(2560)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(20) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(160) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(8) + i2_3_init * T.int64(8) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("lv312_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(320) // T.int64(20))
                                    v2 = T.axis.spatial(T.int64(640), k_0 * T.int64(20) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(20))
                                    T.reads(lv312[v0, v1, v2])
                                    T.writes(lv312_shared[v0, v1, v2])
                                    lv312_shared[v0, v1, v2] = lv312[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(20)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("lv313_shared"):
                                    v0 = T.axis.spatial(T.int64(640), k_0 * T.int64(20) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(160) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) % T.int64(160))
                                    T.reads(lv313[v0, v1])
                                    T.writes(lv313_shared[v0, v1])
                                    lv313_shared[v0, v1] = lv313[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(20), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(20) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(160) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(8) + i2_3 * T.int64(8) + i2_4)
                                v_k = T.axis.reduce(T.int64(640), k_0 * T.int64(20) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv312_shared[v_i0, v_i1, v_k], lv313_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv312_shared[v_i0, v_i1, v_k] * lv313_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(20) + ax1)
                            v2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(160) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(8) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2], lv319[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = (var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2]) * lv319[v0, v1, v2]

    @T.prim_func
    def fused_matmul27_add33_add34(lv320: T.Buffer((T.int64(2), T.int64(1024), T.int64(2560)), "float32"), lv321: T.Buffer((T.int64(2560), T.int64(640)), "float32"), unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias: T.Buffer((T.int64(640),), "float32"), lv311: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="local")
        lv320_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(2560)), scope="shared")
        lv321_shared = T.alloc_buffer((T.int64(2560), T.int64(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(8) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused % T.int64(8) // T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(128) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(1280)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("lv320_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(64) // T.int64(2))
                                    v2 = T.axis.spatial(T.int64(2560), k_0 * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(2))
                                    T.reads(lv320[v0, v1, v2])
                                    T.writes(lv320_shared[v0, v1, v2])
                                    lv320_shared[v0, v1, v2] = lv320[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("lv321_shared"):
                                    v0 = T.axis.spatial(T.int64(2560), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(lv321[v0, v1])
                                    T.writes(lv321_shared[v0, v1])
                                    lv321_shared[v0, v1] = lv321[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(8) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused % T.int64(8) // T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(128) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(2560), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv320_shared[v_i0, v_i1, v_k], lv321_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv320_shared[v_i0, v_i1, v_k] * lv321_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(8) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused % T.int64(8) // T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(128) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias[v2], lv311[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias[v2] + lv311[v0, v1, v2]

    @T.prim_func
    def fused_matmul28_add40_add41(lv485: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), lv486: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: T.Buffer((T.int64(1280),), "float32"), lv461: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="local")
        lv485_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="shared")
        lv486_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv485_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(1280))
                                        v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(128) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(1280) // T.int64(10))
                                        v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(10) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(10))
                                        T.reads(lv485[v0, v1, v2])
                                        T.writes(lv485_shared[v0, v1, v2])
                                        lv485_shared[v0, v1, v2] = lv485[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv486_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(10) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(16))
                                        T.where((ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(160))
                                        T.reads(lv486[v0, v1])
                                        T.writes(lv486_shared[v0, v1])
                                        lv486_shared[v0, v1] = lv486[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(5), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(10) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv485_shared[v_i0, v_i1, v_k], lv486_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv485_shared[v_i0, v_i1, v_k] * lv486_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2], lv461[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2] + lv461[v0, v1, v2]

    @T.prim_func
    def fused_matmul29_multiply20(lv471: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32"), lv478: T.Buffer((T.int64(16), T.int64(160), T.int64(256)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(256)), scope="local")
        lv471_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="shared")
        lv478_shared = T.alloc_buffer((T.int64(16), T.int64(160), T.int64(256)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(8) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(10)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv471_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(8) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(1024) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(160), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(16))
                                        T.reads(lv471[v0, v1, v2])
                                        T.writes(lv471_shared[v0, v1, v2])
                                        lv471_shared[v0, v1, v2] = lv471[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("lv478_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(160), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(512) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.reads(lv478[v0, v1, v2])
                                    T.writes(lv478_shared[v0, v1, v2])
                                    lv478_shared[v0, v1, v2] = lv478[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(16), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(8) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(160), k_0 * T.int64(16) + k_1 * T.int64(16) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv471_shared[v_i0, v_i1, v_k], lv478_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv471_shared[v_i0, v_i1, v_k] * lv478_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(4)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(8) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul32_multiply21(lv499: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32"), lv506: T.Buffer((T.int64(16), T.int64(160), T.int64(77)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(256), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(77)), scope="local")
        lv499_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="shared")
        lv506_shared = T.alloc_buffer((T.int64(16), T.int64(160), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(112) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(44) * T.int64(4) + i0_3_init * T.int64(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(112) // T.int64(7) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(44) // T.int64(11) * T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(40)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv499_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(112) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(352) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(112) // T.int64(7) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(352) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(64) // T.int64(4))
                                        v2 = T.axis.spatial(T.int64(160), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(352) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(4))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(512))
                                        T.reads(lv499[v0, v1, v2])
                                        T.writes(lv499_shared[v0, v1, v2])
                                        lv499_shared[v0, v1, v2] = lv499[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv506_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(112) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(44))
                                        v1 = T.axis.spatial(T.int64(160), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(44) // T.int64(11))
                                        v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(11))
                                        T.reads(lv506[v0, v1, v2])
                                        T.writes(lv506_shared[v0, v1, v2])
                                        lv506_shared[v0, v1, v2] = lv506[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(2), T.int64(4), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(112) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(44) * T.int64(4) + i0_3 * T.int64(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(112) // T.int64(7) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(44) // T.int64(11) * T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(160), k_0 * T.int64(4) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv499_shared[v_i0, v_i1, v_k], lv506_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv499_shared[v_i0, v_i1, v_k] * lv506_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(4), T.int64(4), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(112) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(44) * T.int64(4) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(112) // T.int64(7) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(44) // T.int64(11) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul34_add42_gelu2(lv518: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), lv522: T.Buffer((T.int64(1280), T.int64(5120)), "float32"), unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias: T.Buffer((T.int64(5120),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(256), T.int64(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(5120)))
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(5120)), scope="local")
        lv518_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="shared")
        lv522_shared = T.alloc_buffer((T.int64(1280), T.int64(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init * T.int64(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv518_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(1024) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(16))
                                        T.reads(lv518[v0, v1, v2])
                                        T.writes(lv518_shared[v0, v1, v2])
                                        lv518_shared[v0, v1, v2] = lv518[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv522_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(lv522[v0, v1])
                                        T.writes(lv522_shared[v0, v1])
                                        lv522_shared[v0, v1] = lv522[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(4), T.int64(2), T.int64(4), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 * T.int64(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(16) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv518_shared[v_i0, v_i1, v_k], lv522_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv518_shared[v_i0, v_i1, v_k] * lv522_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(8), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_matmul_intermediate[v0, v1, v2])
                            var_matmul_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(40)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1310720) // T.int64(5120))
                        v_ax2 = T.axis.spatial(T.int64(5120), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(5120))
                        T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = (var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * (T.float32(0.5) + T.erf((var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul34_add42_multiply22(lv518: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), lv519: T.Buffer((T.int64(1280), T.int64(5120)), "float32"), unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias: T.Buffer((T.int64(5120),), "float32"), lv525: T.Buffer((T.int64(2), T.int64(256), T.int64(5120)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(256), T.int64(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(5120)), scope="local")
        lv518_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="shared")
        lv519_shared = T.alloc_buffer((T.int64(1280), T.int64(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2560), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(32) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(8) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(8) + i2_3_init * T.int64(8) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv518_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(128) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(lv518[v0, v1, v2])
                                    T.writes(lv518_shared[v0, v1, v2])
                                    lv518_shared[v0, v1, v2] = lv518[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv519_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(lv519[v0, v1])
                                        T.writes(lv519_shared[v0, v1])
                                        lv519_shared[v0, v1] = lv519[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(32) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(8) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(8) + i2_3 * T.int64(8) + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(16) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv518_shared[v_i0, v_i1, v_k], lv519_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv518_shared[v_i0, v_i1, v_k] * lv519_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(8)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(8) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2], lv525[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = (var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2]) * lv525[v0, v1, v2]

    @T.prim_func
    def fused_matmul35_add40_add41(lv526: T.Buffer((T.int64(2), T.int64(256), T.int64(5120)), "float32"), lv527: T.Buffer((T.int64(5120), T.int64(1280)), "float32"), unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias: T.Buffer((T.int64(1280),), "float32"), lv517: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="local")
        lv526_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(5120)), scope="shared")
        lv527_shared = T.alloc_buffer((T.int64(5120), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(320) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(8) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(1280)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv526_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(40) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(5120), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(lv526[v0, v1, v2])
                                    T.writes(lv526_shared[v0, v1, v2])
                                    lv526_shared[v0, v1, v2] = lv526[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv527_shared"):
                                        v0 = T.axis.spatial(T.int64(5120), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(lv527[v0, v1])
                                        T.writes(lv527_shared[v0, v1])
                                        lv527_shared[v0, v1] = lv527[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(320) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(8) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(5120), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv526_shared[v_i0, v_i1, v_k], lv527_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv526_shared[v_i0, v_i1, v_k] * lv527_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(320) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(8) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias[v2], lv517[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias[v2] + lv517[v0, v1, v2]

    @T.prim_func
    def fused_matmul36_add46_add47(lv726: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), lv727: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: T.Buffer((T.int64(1280),), "float32"), lv702: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="local")
        lv726_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="shared")
        lv727_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(128) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(5) * T.int64(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(5) * T.int64(8) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv726_shared"):
                                        v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(32) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(40) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(128))
                                        T.reads(lv726[v0, v1, v2])
                                        T.writes(lv726_shared[v0, v1, v2])
                                        lv726_shared[v0, v1, v2] = lv726[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv727_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(40))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(40))
                                        T.reads(lv727[v0, v1])
                                        T.writes(lv727_shared[v0, v1])
                                        lv727_shared[v0, v1] = lv727[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(2), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(128) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(5) * T.int64(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(5) * T.int64(8) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(8) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv726_shared[v_i0, v_i1, v_k], lv727_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv726_shared[v_i0, v_i1, v_k] * lv727_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(8)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(5) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(5) * T.int64(8) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2], lv702[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2] + lv702[v0, v1, v2]

    @T.prim_func
    def fused_matmul37_multiply23(lv712: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32"), lv719: T.Buffer((T.int64(16), T.int64(160), T.int64(64)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(64)), scope="local")
        lv712_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="shared")
        lv719_shared = T.alloc_buffer((T.int64(16), T.int64(160), T.int64(64)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(32) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) // T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(20)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv712_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(640))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) // T.int64(2) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(640) // T.int64(40))
                                    v2 = T.axis.spatial(T.int64(160), k_0 * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(40))
                                    T.reads(lv712[v0, v1, v2])
                                    T.writes(lv712_shared[v0, v1, v2])
                                    lv712_shared[v0, v1, v2] = lv712[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(40)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv719_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(1280))
                                    v1 = T.axis.spatial(T.int64(160), k_0 * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(1280) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.reads(lv719[v0, v1, v2])
                                    T.writes(lv719_shared[v0, v1, v2])
                                    lv719_shared[v0, v1, v2] = lv719[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(40), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(32) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) // T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(160), k_0 * T.int64(40) + k_1 * T.int64(40) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv712_shared[v_i0, v_i1, v_k], lv719_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv712_shared[v_i0, v_i1, v_k] * lv719_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) // T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul39_multiply24(lv740: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32"), lv747: T.Buffer((T.int64(16), T.int64(160), T.int64(77)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(64), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(77)), scope="local")
        lv740_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="shared")
        lv747_shared = T.alloc_buffer((T.int64(16), T.int64(160), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(112), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(44), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(7) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(11) * T.int64(16) + i1_3_init * T.int64(4) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(20)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(44), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv740_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(7))
                                        v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(160), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(44) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(512))
                                        T.reads(lv740[v0, v1, v2])
                                        T.writes(lv740_shared[v0, v1, v2])
                                        lv740_shared[v0, v1, v2] = lv740[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(44), thread="threadIdx.x"):
                                with T.block("lv747_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(7))
                                    v1 = T.axis.spatial(T.int64(160), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(44) + ax0_ax1_ax2_fused_1) // T.int64(11))
                                    v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(44) + ax0_ax1_ax2_fused_1) % T.int64(11))
                                    T.reads(lv747[v0, v1, v2])
                                    T.writes(lv747_shared[v0, v1, v2])
                                    lv747_shared[v0, v1, v2] = lv747[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(7) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(11) * T.int64(16) + i1_3 * T.int64(4) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(160), k_0 * T.int64(8) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv740_shared[v_i0, v_i1, v_k], lv747_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv740_shared[v_i0, v_i1, v_k] * lv747_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(16), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(7) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(11) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul3_add17(lv21: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), lv26: T.Buffer((T.int64(768), T.int64(768)), "float32"), self_clip_text_model_encoder_layers_0_self_attn_k_proj_bias: T.Buffer((T.int64(768),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="local")
        lv21_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="shared")
        lv26_shared = T.alloc_buffer((T.int64(768), T.int64(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(231), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(3) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(24) + i0_1_i1_1_i2_1_fused * T.int64(3) + i0_2_i1_2_i2_2_fused % T.int64(3) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(48)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(231), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv21_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(462) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(462) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(16))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(231) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(1232))
                                        T.reads(lv21[v0, v1, v2])
                                        T.writes(lv21_shared[v0, v1, v2])
                                        lv21_shared[v0, v1, v2] = lv21[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(231), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv26_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(924) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(24) + (ax0_ax1_fused_0 * T.int64(924) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(24))
                                        T.where((ax0_ax1_fused_0 * T.int64(231) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(384))
                                        T.reads(lv26[v0, v1])
                                        T.writes(lv26_shared[v0, v1])
                                        lv26_shared[v0, v1] = lv26[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(3) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(24) + i0_1_i1_1_i2_1_fused * T.int64(3) + i0_2_i1_2_i2_2_fused % T.int64(3) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(16) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv21_shared[v_i0, v_i1, v_k], lv26_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv21_shared[v_i0, v_i1, v_k] * lv26_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(3) + ax1)
                            v2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(24) + i0_1_i1_1_i2_1_fused * T.int64(3) + i0_2_i1_2_i2_2_fused % T.int64(3) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], self_clip_text_model_encoder_layers_0_self_attn_k_proj_bias[v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + self_clip_text_model_encoder_layers_0_self_attn_k_proj_bias[v2]

    @T.prim_func
    def fused_matmul3_add17_add15(lv50: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), lv51: T.Buffer((T.int64(768), T.int64(768)), "float32"), self_clip_text_model_encoder_layers_0_self_attn_out_proj_bias: T.Buffer((T.int64(768),), "float32"), lv9: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="local")
        lv50_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="shared")
        lv51_shared = T.alloc_buffer((T.int64(768), T.int64(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(264), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(24) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(24) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv50_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(24) * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(24))
                                        v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(24) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(24))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(56) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(168))
                                        T.reads(lv50[v0, v1, v2])
                                        T.writes(lv50_shared[v0, v1, v2])
                                        lv50_shared[v0, v1, v2] = lv50[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(5)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("lv51_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(24) + (ax0_ax1_fused_0 * T.int64(168) + ax0_ax1_fused_1 * T.int64(3) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(24) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(168) + ax0_ax1_fused_1 * T.int64(3) + ax0_ax1_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_fused_0 * T.int64(56) + ax0_ax1_fused_1) * T.int64(3) + ax0_ax1_fused_2 < T.int64(768))
                                        T.reads(lv51[v0, v1])
                                        T.writes(lv51_shared[v0, v1])
                                        lv51_shared[v0, v1] = lv51[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(24) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(24) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(24) + k_1 * T.int64(8) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv50_shared[v_i0, v_i1, v_k], lv51_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv50_shared[v_i0, v_i1, v_k] * lv51_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(24) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(24) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + ax2)
                            T.reads(lv9[v0, v1, v2], var_matmul_intermediate_local[v0, v1, v2], self_clip_text_model_encoder_layers_0_self_attn_out_proj_bias[v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = lv9[v0, v1, v2] + (var_matmul_intermediate_local[v0, v1, v2] + self_clip_text_model_encoder_layers_0_self_attn_out_proj_bias[v2])

    @T.prim_func
    def fused_matmul3_add17_multiply9(lv21: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), lv22: T.Buffer((T.int64(768), T.int64(768)), "float32"), self_clip_text_model_encoder_layers_0_self_attn_q_proj_bias: T.Buffer((T.int64(768),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="local")
        lv21_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="shared")
        lv22_shared = T.alloc_buffer((T.int64(768), T.int64(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(84), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(12) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(16) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(12) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                                with T.block("lv21_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(12) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) // T.int64(12))
                                    v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(12) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) % T.int64(12))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 < T.int64(132))
                                    T.reads(lv21[v0, v1, v2])
                                    T.writes(lv21_shared[v0, v1, v2])
                                    lv21_shared[v0, v1, v2] = lv21[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv22_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(12) + (ax0_ax1_fused_0 * T.int64(704) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(12) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(704) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.where((ax0_ax1_fused_0 * T.int64(176) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(768))
                                        T.reads(lv22[v0, v1])
                                        T.writes(lv22_shared[v0, v1])
                                        lv22_shared[v0, v1] = lv22[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(12), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(12) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(16) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(12) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(12) + k_1 * T.int64(12) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv21_shared[v_i0, v_i1, v_k], lv22_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv21_shared[v_i0, v_i1, v_k] * lv22_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(12) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(12) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], self_clip_text_model_encoder_layers_0_self_attn_q_proj_bias[v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = (var_matmul_intermediate_local[v0, v1, v2] + self_clip_text_model_encoder_layers_0_self_attn_q_proj_bias[v2]) * T.float32(0.125)

    @T.prim_func
    def fused_matmul41_add48_gelu3(lv759: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), lv763: T.Buffer((T.int64(1280), T.int64(5120)), "float32"), unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias: T.Buffer((T.int64(5120),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(64), T.int64(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(5120)))
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(5120)), scope="local")
        lv759_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="shared")
        lv763_shared = T.alloc_buffer((T.int64(1280), T.int64(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(16) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv759_shared"):
                                        v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(512))
                                        T.reads(lv759[v0, v1, v2])
                                        T.writes(lv759_shared[v0, v1, v2])
                                        lv759_shared[v0, v1, v2] = lv759[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("lv763_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1) // T.int64(80))
                                    v1 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(80) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1) % T.int64(80))
                                    T.reads(lv763[v0, v1])
                                    T.writes(lv763_shared[v0, v1])
                                    lv763_shared[v0, v1] = lv763[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(16), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(16) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(8) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv759_shared[v_i0, v_i1, v_k], lv763_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv759_shared[v_i0, v_i1, v_k] * lv763_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(16), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_matmul_intermediate[v0, v1, v2])
                            var_matmul_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(10)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(327680) // T.int64(5120))
                        v_ax2 = T.axis.spatial(T.int64(5120), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(5120))
                        T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = (var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * (T.float32(0.5) + T.erf((var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul41_add48_multiply25(lv759: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), lv760: T.Buffer((T.int64(1280), T.int64(5120)), "float32"), unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias: T.Buffer((T.int64(5120),), "float32"), lv766: T.Buffer((T.int64(2), T.int64(64), T.int64(5120)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(64), T.int64(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(5120)), scope="local")
        lv759_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="shared")
        lv760_shared = T.alloc_buffer((T.int64(1280), T.int64(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(4) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(320) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(4) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(640)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("lv759_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(64) // T.int64(2))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(2))
                                    T.reads(lv759[v0, v1, v2])
                                    T.writes(lv759_shared[v0, v1, v2])
                                    lv759_shared[v0, v1, v2] = lv759[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("lv760_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(320) * T.int64(16) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(16))
                                    T.reads(lv760[v0, v1])
                                    T.writes(lv760_shared[v0, v1])
                                    lv760_shared[v0, v1] = lv760[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(4) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(320) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(4) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv759_shared[v_i0, v_i1, v_k], lv760_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv759_shared[v_i0, v_i1, v_k] * lv760_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(320) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(4) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2], lv766[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = (var_matmul_intermediate_local[v0, v1, v2] + unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2]) * lv766[v0, v1, v2]

    @T.prim_func
    def fused_matmul42_add46_add47(lv767: T.Buffer((T.int64(2), T.int64(64), T.int64(5120)), "float32"), lv768: T.Buffer((T.int64(5120), T.int64(1280)), "float32"), unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias: T.Buffer((T.int64(1280),), "float32"), lv758: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="local")
        lv767_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(5120)), scope="shared")
        lv768_shared = T.alloc_buffer((T.int64(5120), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv767_shared"):
                                        v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(32) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(640) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(5120), k_0 * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(640) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.reads(lv767[v0, v1, v2])
                                        T.writes(lv767_shared[v0, v1, v2])
                                        lv767_shared[v0, v1, v2] = lv767[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv768_shared"):
                                        v0 = T.axis.spatial(T.int64(5120), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(640) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(40))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + (ax0_ax1_fused_0 * T.int64(640) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(40))
                                        T.where((ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(1600))
                                        T.reads(lv768[v0, v1])
                                        T.writes(lv768_shared[v0, v1])
                                        lv768_shared[v0, v1] = lv768[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(40), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(5120), k_0 * T.int64(40) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv767_shared[v_i0, v_i1, v_k], lv768_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv767_shared[v_i0, v_i1, v_k] * lv768_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias[v2], lv758[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias[v2] + lv758[v0, v1, v2]

    @T.prim_func
    def fused_matmul6_add19_multiply10_tir_sigmoid_multiply11(lv55: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), lv56: T.Buffer((T.int64(768), T.int64(3072)), "float32"), self_clip_text_model_encoder_layers_0_mlp_fc1_bias: T.Buffer((T.int64(3072),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(3072)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(3072)))
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(3072)), scope="local")
        lv55_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="shared")
        lv56_shared = T.alloc_buffer((T.int64(768), T.int64(3072)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(528), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(7), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(48) * T.int64(7) + i1_3_init * T.int64(7) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(3072), i0_0_i1_0_i2_0_fused % T.int64(48) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv55_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(48) * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(12))
                                        v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(12) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(12))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(84))
                                        T.reads(lv55[v0, v1, v2])
                                        T.writes(lv55_shared[v0, v1, v2])
                                        lv55_shared[v0, v1, v2] = lv55[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(6)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv56_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(12) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(3072), i0_0_i1_0_i2_0_fused % T.int64(48) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(lv56[v0, v1])
                                        T.writes(lv56_shared[v0, v1])
                                        lv56_shared[v0, v1] = lv56[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(3), T.int64(1), T.int64(7), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(48) * T.int64(7) + i1_3 * T.int64(7) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(3072), i0_0_i1_0_i2_0_fused % T.int64(48) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(12) + k_1 * T.int64(3) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv55_shared[v_i0, v_i1, v_k], lv56_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv55_shared[v_i0, v_i1, v_k] * lv56_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(7), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(48) * T.int64(7) + ax1)
                            v2 = T.axis.spatial(T.int64(3072), i0_0_i1_0_i2_0_fused % T.int64(48) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_matmul_intermediate[v0, v1, v2])
                            var_matmul_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                    with T.block("T_multiply_1"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(3072))
                        v_ax2 = T.axis.spatial(T.int64(3072), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(3072))
                        T.where((ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) * T.int64(256) + ax0_ax1_ax2_fused_2 < T.int64(236544))
                        T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], self_clip_text_model_encoder_layers_0_mlp_fc1_bias[v_ax2])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = (var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_mlp_fc1_bias[v_ax2]) * T.sigmoid(T.float32(1.7020000219345093) * (var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_mlp_fc1_bias[v_ax2]))

    @T.prim_func
    def fused_matmul7_add17_add15(lv61: T.Buffer((T.int64(1), T.int64(77), T.int64(3072)), "float32"), lv62: T.Buffer((T.int64(3072), T.int64(768)), "float32"), self_clip_text_model_encoder_layers_0_mlp_fc2_bias: T.Buffer((T.int64(768),), "float32"), lv54: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="local")
        lv61_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(3072)), scope="shared")
        lv62_shared = T.alloc_buffer((T.int64(3072), T.int64(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(462), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(6) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(6) * T.int64(128) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(384)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv61_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(6))
                                        v2 = T.axis.spatial(T.int64(3072), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(8))
                                        T.reads(lv61[v0, v1, v2])
                                        T.writes(lv61_shared[v0, v1, v2])
                                        lv61_shared[v0, v1, v2] = lv61[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv62_shared"):
                                    v0 = T.axis.spatial(T.int64(3072), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(6) * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(lv62[v0, v1])
                                    T.writes(lv62_shared[v0, v1])
                                    lv62_shared[v0, v1] = lv62[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(6) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(6) * T.int64(128) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(3072), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv61_shared[v_i0, v_i1, v_k], lv62_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv61_shared[v_i0, v_i1, v_k] * lv62_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(6) + ax1)
                            v2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(6) * T.int64(128) + i0_2_i1_2_i2_2_fused * T.int64(2) + ax2)
                            T.reads(lv54[v0, v1, v2], var_matmul_intermediate_local[v0, v1, v2], self_clip_text_model_encoder_layers_0_mlp_fc2_bias[v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = lv54[v0, v1, v2] + (var_matmul_intermediate_local[v0, v1, v2] + self_clip_text_model_encoder_layers_0_mlp_fc2_bias[v2])

    @T.prim_func
    def fused_matmul8_add20_silu6(lv14: T.Buffer((T.int64(2), T.int64(320)), "float32"), lv15: T.Buffer((T.int64(320), T.int64(1280)), "float32"), unet_time_embedding_linear_1_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(2), T.int64(1280)))
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="local")
        lv14_shared = T.alloc_buffer((T.int64(2), T.int64(320)), scope="shared")
        lv15_shared = T.alloc_buffer((T.int64(320), T.int64(1280)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused * T.int64(160) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(8)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("lv14_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) // T.int64(40))
                                    v1 = T.axis.spatial(T.int64(320), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) % T.int64(40))
                                    T.where(ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 < T.int64(80))
                                    T.reads(lv14[v0, v1])
                                    T.writes(lv14_shared[v0, v1])
                                    lv14_shared[v0, v1] = lv14[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(40)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("lv15_shared"):
                                    v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused * T.int64(160) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) % T.int64(160))
                                    T.reads(lv15[v0, v1])
                                    T.writes(lv15_shared[v0, v1])
                                    lv15_shared[v0, v1] = lv15[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(10), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused * T.int64(160) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(40) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1], lv14_shared[v_i0, v_k], lv15_shared[v_k, v_i1])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1] = var_matmul_intermediate_local[v_i0, v_i1] + lv14_shared[v_i0, v_k] * lv15_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(2), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused * T.int64(160) + i0_2_i1_2_fused + ax1)
                            T.reads(var_matmul_intermediate_local[v0, v1])
                            T.writes(var_matmul_intermediate[v0, v1])
                            var_matmul_intermediate[v0, v1] = var_matmul_intermediate_local[v0, v1]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) // T.int64(1280))
                    v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) % T.int64(1280))
                    T.reads(var_matmul_intermediate[v_ax0, v_ax1], unet_time_embedding_linear_1_bias[v_ax1])
                    T.writes(var_T_multiply_intermediate[v_ax0, v_ax1])
                    var_T_multiply_intermediate[v_ax0, v_ax1] = (var_matmul_intermediate[v_ax0, v_ax1] + unet_time_embedding_linear_1_bias[v_ax1]) * T.sigmoid(var_matmul_intermediate[v_ax0, v_ax1] + unet_time_embedding_linear_1_bias[v_ax1])

    @T.prim_func
    def fused_matmul9_add20(lv18: T.Buffer((T.int64(2), T.int64(1280)), "float32"), lv19: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), unet_time_embedding_linear_2_bias: T.Buffer((T.int64(1280),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="local")
        lv18_shared = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="shared")
        lv19_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("lv18_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1280), k_0 * T.int64(80) + ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1)
                                    T.reads(lv18[v0, v1])
                                    T.writes(lv18_shared[v0, v1])
                                    lv18_shared[v0, v1] = lv18[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(40)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv19_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(80) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(80))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(80))
                                        T.reads(lv19[v0, v1])
                                        T.writes(lv19_shared[v0, v1])
                                        lv19_shared[v0, v1] = lv19[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(80), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(80) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1], lv18_shared[v_i0, v_k], lv19_shared[v_k, v_i1])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1] = var_matmul_intermediate_local[v_i0, v_i1] + lv18_shared[v_i0, v_k] * lv19_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + i0_2_i1_2_fused + ax1)
                            T.reads(var_matmul_intermediate_local[v0, v1], unet_time_embedding_linear_2_bias[v1])
                            T.writes(var_T_add_intermediate[v0, v1])
                            var_T_add_intermediate[v0, v1] = var_matmul_intermediate_local[v0, v1] + unet_time_embedding_linear_2_bias[v1]

    @T.prim_func
    def fused_matmul9_add20_strided_slice5(lv439: T.Buffer((T.int64(2), T.int64(1280)), "float32"), lv440: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), unet_down_blocks_2_resnets_0_time_emb_proj_bias: T.Buffer((T.int64(1280),), "float32"), var_T_strided_slice_with_axes_intermediate: T.Buffer((T.int64(2), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="local")
        lv439_shared = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="shared")
        lv440_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("lv439_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1280), k_0 * T.int64(40) + ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1)
                                    T.reads(lv439[v0, v1])
                                    T.writes(lv439_shared[v0, v1])
                                    lv439_shared[v0, v1] = lv439[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(20)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv440_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(80))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(80))
                                        T.reads(lv440[v0, v1])
                                        T.writes(lv440_shared[v0, v1])
                                        lv440_shared[v0, v1] = lv440[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(40) + k_1 * T.int64(5) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1], lv439_shared[v_i0, v_k], lv440_shared[v_k, v_i1])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1] = var_matmul_intermediate_local[v_i0, v_i1] + lv439_shared[v_i0, v_k] * lv440_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(var_matmul_intermediate_local[v0, v1], unet_down_blocks_2_resnets_0_time_emb_proj_bias[v1])
                            T.writes(var_T_strided_slice_with_axes_intermediate[v0, v1])
                            var_T_strided_slice_with_axes_intermediate[v0, v1] = var_matmul_intermediate_local[v0, v1] + unet_down_blocks_2_resnets_0_time_emb_proj_bias[v1]

    @T.prim_func
    def fused_matmul_add4(lv23: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), lv24: T.Buffer((T.int64(512), T.int64(512)), "float32"), vae_decoder_mid_block_attentions_0_to_q_bias: T.Buffer((T.int64(512),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(512)), scope="local")
        lv23_shared = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(512)), scope="shared")
        lv24_shared = T.alloc_buffer((T.int64(512), T.int64(512)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + i1_3_init * T.int64(4) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv23_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(512), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.reads(lv23[v0, v1, v2])
                                        T.writes(lv23_shared[v0, v1, v2])
                                        lv23_shared[v0, v1, v2] = lv23[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv24_shared"):
                                        v0 = T.axis.spatial(T.int64(512), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(lv24[v0, v1])
                                        T.writes(lv24_shared[v0, v1])
                                        lv24_shared[v0, v1] = lv24[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(2), T.int64(4), T.int64(2), T.int64(1), T.int64(4), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + i1_3 * T.int64(4) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(8) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv23_shared[v_i0, v_i1, v_k], lv24_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv23_shared[v_i0, v_i1, v_k] * lv24_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(4)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], vae_decoder_mid_block_attentions_0_to_q_bias[v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + vae_decoder_mid_block_attentions_0_to_q_bias[v2]

    @T.prim_func
    def fused_reshape11_reshape11_add15(lv3: T.Buffer((T.int64(77), T.int64(768)), "float32"), lv7: T.Buffer((T.int64(77), T.int64(768)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(1848), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(768))
                    v_ax2 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(768))
                    T.reads(lv3[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)], lv7[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)])
                    T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                    var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = lv3[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)] + lv7[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)]

    @T.prim_func
    def fused_reshape14_transpose8_reshape15(lv33: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1848), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(768))
                    v_ax2 = T.axis.spatial(T.int64(12), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(768) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(lv33[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)])
                    T.writes(var_T_reshape_intermediate[v_ax2, v_ax1, v_ax3])
                    var_T_reshape_intermediate[v_ax2, v_ax1, v_ax3] = lv33[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)]

    @T.prim_func
    def fused_reshape14_transpose8_reshape15_transpose9(lv28: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(12), T.int64(64), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1848), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(768))
                    v_ax2 = T.axis.spatial(T.int64(12), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(768) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(lv28[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)])
                    T.writes(var_T_transpose_intermediate[v_ax2, v_ax3, v_ax1])
                    var_T_transpose_intermediate[v_ax2, v_ax3, v_ax1] = lv28[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)]

    @T.prim_func
    def fused_reshape16_add18_reshape17(lv42: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32"), param_0: T.Buffer((T.int64(1), T.int64(1), T.int64(77), T.int64(77)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(12), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(5929))
                        v_ax2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(5929) // T.int64(77))
                        v_ax3 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(77))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(71148))
                        T.reads(lv42[((v_ax3 // T.int64(77) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(77) + v_ax2) % T.int64(77), v_ax3 % T.int64(77)], param_0[v_ax0, T.int64(0), v_ax2, v_ax3])
                        T.writes(var_T_reshape_intermediate[v_ax1, v_ax2, v_ax3])
                        var_T_reshape_intermediate[v_ax1, v_ax2, v_ax3] = lv42[((v_ax3 // T.int64(77) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(77) + v_ax2) % T.int64(77), v_ax3 % T.int64(77)] + param_0[v_ax0, T.int64(0), v_ax2, v_ax3]

    @T.prim_func
    def fused_reshape18_transpose10_reshape19(lv47: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1848), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(12), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4928))
                    v_ax2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4928) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(lv47[((v_ax3 // T.int64(64) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(64) + v_ax2) % T.int64(77), v_ax3 % T.int64(64)])
                    T.writes(var_T_reshape_intermediate[T.int64(0), v_ax2, v_ax3 + v_ax1 * T.int64(64)])
                    var_T_reshape_intermediate[T.int64(0), v_ax2, v_ax3 + v_ax1 * T.int64(64)] = lv47[((v_ax3 // T.int64(64) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(64) + v_ax2) % T.int64(77), v_ax3 % T.int64(64)]

    @T.prim_func
    def fused_reshape25_transpose18_reshape26(lv52: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(320))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(320) // T.int64(40))
                        v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40))
                        T.reads(lv52[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(4096) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(4096), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)])
                        T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                        var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv52[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(4096) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(4096), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)]

    @T.prim_func
    def fused_reshape25_transpose18_reshape26_transpose19(lv54: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(40), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(320))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(320) // T.int64(40))
                        v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40))
                        T.reads(lv54[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(4096) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(4096), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)])
                        T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv54[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(4096) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(4096), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)]

    @T.prim_func
    def fused_reshape27_transpose20_reshape28(lv70: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(163840))
                        v_ax2 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(163840) // T.int64(40))
                        v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40))
                        T.reads(lv70[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(40) + v_ax2) // T.int64(4096) + v_ax1) % T.int64(16), (v_ax3 // T.int64(40) + v_ax2) % T.int64(4096), v_ax3 % T.int64(40)])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(40)])
                        var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(40)] = lv70[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(40) + v_ax2) // T.int64(4096) + v_ax1) % T.int64(16), (v_ax3 // T.int64(40) + v_ax2) % T.int64(4096), v_ax3 % T.int64(40)]

    @T.prim_func
    def fused_reshape29_transpose22_reshape30(lv84: T.Buffer((T.int64(2), T.int64(77), T.int64(320)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(77), T.int64(40)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1540), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(24640))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(24640) // T.int64(320))
                    v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(320) // T.int64(40))
                    v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(40))
                    T.reads(lv84[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(77), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)])
                    T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                    var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv84[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(77), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)]

    @T.prim_func
    def fused_reshape29_transpose22_reshape30_transpose23(lv82: T.Buffer((T.int64(2), T.int64(77), T.int64(320)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(40), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(385), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(24640))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(24640) // T.int64(320))
                    v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(320) // T.int64(40))
                    v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(40))
                    T.reads(lv82[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(77), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)])
                    T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                    var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv82[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(77), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)]

    @T.prim_func
    def fused_reshape2_transpose_transpose1(lv18: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(4096))
                        T.reads(lv18[T.int64(0), (v_ax2 // T.int64(4096) + v_ax1) % T.int64(512), v_ax2 % T.int64(4096) // T.int64(64), v_ax2 % T.int64(64)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2])
                        var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2] = lv18[T.int64(0), (v_ax2 // T.int64(4096) + v_ax1) % T.int64(512), v_ax2 % T.int64(4096) // T.int64(64), v_ax2 % T.int64(64)]

    @T.prim_func
    def fused_reshape31_transpose24(lv118: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(320))
                        v_ax3 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(320))
                        T.reads(lv118[((v_ax1 * T.int64(64) + v_ax3 // T.int64(320) + v_ax2) // T.int64(4096) + v_ax0) % T.int64(2), (v_ax1 * T.int64(64) + v_ax3 // T.int64(320) + v_ax2) % T.int64(4096), v_ax3 % T.int64(320)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2])
                        var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2] = lv118[((v_ax1 * T.int64(64) + v_ax3 // T.int64(320) + v_ax2) // T.int64(4096) + v_ax0) % T.int64(2), (v_ax1 * T.int64(64) + v_ax3 // T.int64(320) + v_ax2) % T.int64(4096), v_ax3 % T.int64(320)]

    @T.prim_func
    def fused_reshape35_transpose28_reshape36(lv258: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(640))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640) // T.int64(80))
                        v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(80))
                        T.reads(lv258[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(1024) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)])
                        T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                        var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv258[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(1024) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)]

    @T.prim_func
    def fused_reshape35_transpose28_reshape36_transpose29(lv260: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(80), T.int64(1024)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(640))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640) // T.int64(80))
                        v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(80))
                        T.reads(lv260[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(1024) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)])
                        T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv260[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(1024) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)]

    @T.prim_func
    def fused_reshape37_transpose30_reshape38(lv276: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(81920))
                        v_ax2 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(80))
                        v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(80))
                        T.reads(lv276[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(80) + v_ax2) // T.int64(1024) + v_ax1) % T.int64(16), (v_ax3 // T.int64(80) + v_ax2) % T.int64(1024), v_ax3 % T.int64(80)])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(80)])
                        var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(80)] = lv276[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(80) + v_ax2) // T.int64(1024) + v_ax1) % T.int64(16), (v_ax3 // T.int64(80) + v_ax2) % T.int64(1024), v_ax3 % T.int64(80)]

    @T.prim_func
    def fused_reshape39_transpose32_reshape40(lv290: T.Buffer((T.int64(2), T.int64(77), T.int64(640)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(77), T.int64(80)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49280))
                        v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49280) // T.int64(640))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640) // T.int64(80))
                        v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(80))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(98560))
                        T.reads(lv290[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)])
                        T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                        var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv290[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)]

    @T.prim_func
    def fused_reshape39_transpose32_reshape40_transpose33(lv288: T.Buffer((T.int64(2), T.int64(77), T.int64(640)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(80), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49280))
                        v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49280) // T.int64(640))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640) // T.int64(80))
                        v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(80))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(98560))
                        T.reads(lv288[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)])
                        T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv288[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)]

    @T.prim_func
    def fused_reshape3_transpose3(lv26: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                        v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv26[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                        var_T_transpose_intermediate[v_ax0, v_ax2, v_ax1, v_ax3] = lv26[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)]

    @T.prim_func
    def fused_reshape3_transpose3_transpose4(lv29: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(512), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                        v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv29[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax2, v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax0, v_ax2, v_ax3, v_ax1] = lv29[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)]

    @T.prim_func
    def fused_reshape41_transpose36(lv324: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(640))
                        v_ax3 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640))
                        T.reads(lv324[((v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) // T.int64(1024) + v_ax0) % T.int64(2), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2])
                        var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2] = lv324[((v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) // T.int64(1024) + v_ax0) % T.int64(2), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)]

    @T.prim_func
    def fused_reshape45_transpose38_reshape46(lv464: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.reads(lv464[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(256) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                        var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv464[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(256) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape45_transpose38_reshape46_transpose39(lv466: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(160), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.reads(lv466[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(256) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv466[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(256) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape47_transpose40_reshape48(lv482: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(40960))
                        v_ax2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40960) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.reads(lv482[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(160) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16), (v_ax3 // T.int64(160) + v_ax2) % T.int64(256), v_ax3 % T.int64(160)])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(160)])
                        var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(160)] = lv482[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(160) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16), (v_ax3 // T.int64(160) + v_ax2) % T.int64(256), v_ax3 % T.int64(160)]

    @T.prim_func
    def fused_reshape49_transpose42_reshape50(lv496: T.Buffer((T.int64(2), T.int64(77), T.int64(1280)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(77), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98560))
                        v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98560) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(197120))
                        T.reads(lv496[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                        var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv496[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape49_transpose42_reshape50_transpose43(lv494: T.Buffer((T.int64(2), T.int64(77), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(160), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98560))
                        v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98560) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(197120))
                        T.reads(lv494[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv494[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape51_transpose46(lv530: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(1280))
                        v_ax3 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280))
                        T.reads(lv530[((v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) // T.int64(256) + v_ax0) % T.int64(2), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2])
                        var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2] = lv530[((v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) // T.int64(256) + v_ax0) % T.int64(2), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)]

    @T.prim_func
    def fused_reshape53_transpose48_reshape54(lv705: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv705[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(64) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(64), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                        var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv705[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(64) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(64), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape53_transpose48_reshape54_transpose49(lv707: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(160), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv707[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(64) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(64), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv707[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(64) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(64), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape55_transpose50_reshape56(lv723: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(10240))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10240) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv723[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(160) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16), (v_ax3 // T.int64(160) + v_ax2) % T.int64(64), v_ax3 % T.int64(160)])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(160)])
                        var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(160)] = lv723[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(160) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16), (v_ax3 // T.int64(160) + v_ax2) % T.int64(64), v_ax3 % T.int64(160)]

    @T.prim_func
    def fused_reshape57_transpose51(lv771: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(10240))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10240) // T.int64(1280))
                        v_ax3 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv771[((v_ax1 * T.int64(8) + v_ax3 // T.int64(1280) + v_ax2) // T.int64(64) + v_ax0) % T.int64(2), (v_ax1 * T.int64(8) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(64), v_ax3 % T.int64(1280)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2])
                        var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2] = lv771[((v_ax1 * T.int64(8) + v_ax3 // T.int64(1280) + v_ax2) // T.int64(64) + v_ax0) % T.int64(2), (v_ax1 * T.int64(8) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(64), v_ax3 % T.int64(1280)]

    @T.prim_func
    def fused_reshape9_cast_reshape10(inp_0: T.Buffer((T.int64(1), T.int64(77)), "int32"), var_T_reshape_intermediate: T.Buffer((T.int64(77),), "int32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(77))
                    T.reads(inp_0[T.int64(0), v_ax1 % T.int64(77)])
                    T.writes(var_T_reshape_intermediate[v_ax1])
                    var_T_reshape_intermediate[v_ax1] = inp_0[T.int64(0), v_ax1 % T.int64(77)]

    @T.prim_func
    def fused_split_subtract_multiply26_add(lv1818: T.Buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(lv1818[v_ax0:v_ax0 + T.int64(2), v_ax1, v_ax2, v_ax3])
                    T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                    var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv1818[v_ax0, v_ax1, v_ax2, v_ax3] + T.float32(7.5) * (lv1818[v_ax0 + T.int64(1), v_ax1, v_ax2, v_ax3] - lv1818[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def fused_transpose16_reshape24(lv47: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(320))
                        v_ax3 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(320))
                        T.reads(lv47[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(64), v_ax3])
                        var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(64), v_ax3] = lv47[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose1_reshape5_add3_divide1(lv50: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), lv18: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(32)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(4096))
                        T.reads(lv50[v_ax0, v_ax2, v_ax1], lv18[T.int64(0), v_ax1, v_ax2 // T.int64(64), v_ax2 % T.int64(64)])
                        T.writes(var_T_divide_intermediate[T.int64(0), v_ax1, v_ax2 // T.int64(64), v_ax2 % T.int64(64)])
                        var_T_divide_intermediate[T.int64(0), v_ax1, v_ax2 // T.int64(64), v_ax2 % T.int64(64)] = lv50[v_ax0, v_ax2, v_ax1] + lv18[T.int64(0), v_ax1, v_ax2 // T.int64(64), v_ax2 % T.int64(64)]

    @T.prim_func
    def fused_transpose26_reshape34(lv253: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(640))
                        v_ax3 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640))
                        T.reads(lv253[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(32), v_ax3])
                        var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(32), v_ax3] = lv253[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose37_reshape44(lv459: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(1280))
                        v_ax3 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280))
                        T.reads(lv459[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(16), v_ax3])
                        var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(16), v_ax3] = lv459[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose47_reshape52(lv700: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(10240))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10240) // T.int64(1280))
                        v_ax3 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv700[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(8), v_ax3])
                        var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(8), v_ax3] = lv700[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose5_reshape4(lv45: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                        v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv45[v_ax0, v_ax2, v_ax1, v_ax3])
                        T.writes(var_T_reshape_intermediate[T.int64(0), v_ax1, v_ax3])
                        var_T_reshape_intermediate[T.int64(0), v_ax1, v_ax3] = lv45[v_ax0, v_ax2, v_ax1, v_ax3]

    @T.prim_func
    def fused_transpose6_multiply8_tir_round(lv237: T.Buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(512), T.int64(3)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(12)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1536))
                        v_ax2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1536) // T.int64(3))
                        v_ax3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                        T.reads(lv237[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(var_compute_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_compute_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T.round(lv237[v_ax0, v_ax3, v_ax1, v_ax2] * T.float32(255))

    @T.prim_func
    def group_norm1(A: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32"), B: T.Buffer((T.int64(512),), "float32"), C: T.Buffer((T.int64(512),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_fused_0 in range(T.int64(512)):
                for k2_k3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(16), (k2_k3_fused_0 * T.int64(128) + k2_k3_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(4096), (k2_k3_fused_0 * T.int64(128) + k2_k3_fused_1) % T.int64(4096))
                        T.reads(A[T.int64(0), (v_ax1 * T.int64(16) + v_k3 // T.int64(4096) + v_k2) % T.int64(512), v_k3 % T.int64(4096)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[T.int64(0), (v_ax1 * T.int64(16) + v_k3 // T.int64(4096) + v_k2) % T.int64(512), v_k3 % T.int64(4096)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[T.int64(0), (v_ax1 * T.int64(16) + v_k3 // T.int64(4096) + v_k2) % T.int64(512), v_k3 % T.int64(4096)] * A[T.int64(0), (v_ax1 * T.int64(16) + v_k3 // T.int64(4096) + v_k2) % T.int64(512), v_k3 % T.int64(4096)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(65536))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(65536) // T.int64(4096))
                        v_ax3 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096))
                        T.reads(A[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(4096) + v_ax2) % T.int64(512), v_ax3 % T.int64(4096)], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[(v_ax1 * T.int64(16) + v_ax2) % T.int64(512)], C[(v_ax1 * T.int64(16) + v_ax2) % T.int64(512)])
                        T.writes(T_reshape[T.int64(0), v_ax2 + v_ax1 * T.int64(16), v_ax3])
                        T_reshape[T.int64(0), v_ax2 + v_ax1 * T.int64(16), v_ax3] = (A[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(4096) + v_ax2) % T.int64(512), v_ax3 % T.int64(4096)] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(1.52587890625e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * B[(v_ax1 * T.int64(16) + v_ax2) % T.int64(512)] + C[(v_ax1 * T.int64(16) + v_ax2) % T.int64(512)]

    @T.prim_func
    def group_norm11(A: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), B: T.Buffer((T.int64(640),), "float32"), C: T.Buffer((T.int64(640),), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(20), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(A[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * A[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int64(20)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(655360) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(20), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(20480) // T.int64(1024))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax4 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(32))
                        T.reads(A[((v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(640), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[(v_ax1 * T.int64(20) + v_ax2) % T.int64(640)], C[(v_ax1 * T.int64(20) + v_ax2) % T.int64(640)])
                        T.writes(T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(20), v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(20), v_ax3, v_ax4] = (A[((v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(640), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) + T.float32(9.9999999999999995e-07)) * B[(v_ax1 * T.int64(20) + v_ax2) % T.int64(640)] + C[(v_ax1 * T.int64(20) + v_ax2) % T.int64(640)]

    @T.prim_func
    def group_norm14(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), B: T.Buffer((T.int64(1280),), "float32"), C: T.Buffer((T.int64(1280),), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(80)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int64(10)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(327680) // T.int64(10240))
                        v_ax2 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(10240) // T.int64(256))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(256) // T.int64(16))
                        v_ax4 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(16))
                        T.reads(A[((v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)], C[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)])
                        T.writes(T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(40), v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(40), v_ax3, v_ax4] = (A[((v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) + T.float32(9.9999999999999995e-07)) * B[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)] + C[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)]

    @T.prim_func
    def group_norm16(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), B: T.Buffer((T.int64(1280),), "float32"), C: T.Buffer((T.int64(1280),), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) // T.int64(64))
                        v_k3 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) % T.int64(64) // T.int64(8))
                        v_k4 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) % T.int64(8))
                        T.reads(A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)] * A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int64(3)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(81920) // T.int64(2560))
                        v_ax2 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(2560) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(64) // T.int64(8))
                        v_ax4 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(8))
                        T.where((ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2 < T.int64(163840))
                        T.reads(A[((v_ax1 * T.int64(40) + (v_ax4 // T.int64(8) + v_ax3) // T.int64(8) + v_ax2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(8) + v_ax3) // T.int64(8) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(8) + v_ax3) % T.int64(8), v_ax4 % T.int64(8)], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)], C[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)])
                        T.writes(T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(40), v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(40), v_ax3, v_ax4] = (A[((v_ax1 * T.int64(40) + (v_ax4 // T.int64(8) + v_ax3) // T.int64(8) + v_ax2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(8) + v_ax3) // T.int64(8) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(8) + v_ax3) % T.int64(8), v_ax4 % T.int64(8)] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00039062500000000002) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) + T.float32(9.9999999999999995e-07)) * B[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)] + C[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)]

    @T.prim_func
    def group_norm8(A: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(320),), "float32"), C: T.Buffer((T.int64(320),), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(10), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(A[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * A[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int64(40)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(1310720) // T.int64(40960))
                        v_ax2 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(40960) // T.int64(4096))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax4 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(64))
                        T.reads(A[((v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(320), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[(v_ax1 * T.int64(10) + v_ax2) % T.int64(320)], C[(v_ax1 * T.int64(10) + v_ax2) % T.int64(320)])
                        T.writes(T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(10), v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(10), v_ax3, v_ax4] = (A[((v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(320), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05)) + T.float32(9.9999999999999995e-07)) * B[(v_ax1 * T.int64(10) + v_ax2) % T.int64(320)] + C[(v_ax1 * T.int64(10) + v_ax2) % T.int64(320)]

    @T.prim_func
    def layer_norm(A: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), B: T.Buffer((T.int64(768),), "float32"), C: T.Buffer((T.int64(768),), "float32"), T_layer_norm: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(77)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(77)))
        for ax0_ax1_fused in T.thread_binding(T.int64(77), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_0 in range(T.int64(96)):
                for k2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(77), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(768), k2_0 * T.int64(8) + k2_1)
                        T.reads(A[v_ax0, v_ax1, v_k2])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(462), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_layer_norm"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(768))
                    v_ax2 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(768))
                    T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                    T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                    T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0013020833333333333) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func
    def layer_norm1(A: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), B: T.Buffer((T.int64(320),), "float32"), C: T.Buffer((T.int64(320),), "float32"), T_layer_norm: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(4096)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(4096)))
        for ax0_ax1_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_0 in range(T.int64(10)):
                for k2_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(4096))
                        v_ax1 = T.axis.spatial(T.int64(4096), ax0_ax1_fused % T.int64(4096))
                        v_k2 = T.axis.reduce(T.int64(320), k2_0 * T.int64(32) + k2_1)
                        T.reads(A[v_ax0, v_ax1, v_k2])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(40)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1310720) // T.int64(320))
                        v_ax2 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(320))
                        T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0031250000000000002)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0031250000000000002) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0031250000000000002) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0031250000000000002)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func
    def layer_norm2(A: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), B: T.Buffer((T.int64(640),), "float32"), C: T.Buffer((T.int64(640),), "float32"), T_layer_norm: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(1024)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(1024)))
        for ax0_ax1_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_0 in range(T.int64(80)):
                for k2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(1024))
                        v_ax1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024))
                        v_k2 = T.axis.reduce(T.int64(640), k2_0 * T.int64(8) + k2_1)
                        T.reads(A[v_ax0, v_ax1, v_k2])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(20)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(655360) // T.int64(640))
                        v_ax2 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(640))
                        T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0015625000000000001) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func
    def layer_norm3(A: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1280),), "float32"), C: T.Buffer((T.int64(1280),), "float32"), T_layer_norm: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(256)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(256)))
        for ax0_ax1_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_0 in range(T.int64(80)):
                for k2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(256))
                        v_ax1 = T.axis.spatial(T.int64(256), ax0_ax1_fused % T.int64(256))
                        v_k2 = T.axis.reduce(T.int64(1280), k2_0 * T.int64(16) + k2_1)
                        T.reads(A[v_ax0, v_ax1, v_k2])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(10)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(327680) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1280))
                        T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00078125000000000004) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func
    def layer_norm4(A: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1280),), "float32"), C: T.Buffer((T.int64(1280),), "float32"), T_layer_norm: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(64)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(64)))
        for ax0_ax1_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_0 in range(T.int64(320)):
                for k2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(64))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_fused % T.int64(64))
                        v_k2 = T.axis.reduce(T.int64(1280), k2_0 * T.int64(4) + k2_1)
                        T.reads(A[v_ax0, v_ax1, v_k2])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(3)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(81920) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1280))
                        T.where((ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) * T.int64(256) + ax0_ax1_ax2_fused_2 < T.int64(163840))
                        T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00078125000000000004) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func
    def matmul11(A: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), B: T.Buffer((T.int64(320), T.int64(320)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(320), T.int64(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init * T.int64(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(2560))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(2560) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(320), k_0 * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(B[v0, v1])
                                    T.writes(B_shared[v0, v1])
                                    B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(10), T.int64(1), T.int64(8), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 * T.int64(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(40) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(8), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul13(A: T.Buffer((T.int64(16), T.int64(4096), T.int64(4096)), "float32"), B: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(4096)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(5), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(128) * T.int64(2) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(2) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(40), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(5) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(128) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(2) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(1024) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(128) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(320) // T.int64(20))
                                    v2 = T.axis.spatial(T.int64(40), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(20) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(20))
                                    T.reads(B[v0, v1, v2])
                                    T.writes(B_shared[v0, v1, v2])
                                    B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(2), T.int64(1), T.int64(5), T.int64(4), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(128) * T.int64(2) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(2) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(40), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(5) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(16) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(2), T.int64(5)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(128) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(2) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(40), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(5) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul14(A: T.Buffer((T.int64(2), T.int64(77), T.int64(768)), "float32"), B: T.Buffer((T.int64(768), T.int64(320)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(77), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(320)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(768)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(768), T.int64(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(112), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(11), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(20) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + i0_1_i1_1_i2_1_fused + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(20) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(18)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(120) + ax0_ax1_ax2_fused_1 * T.int64(3) + ax0_ax1_ax2_fused_2) // T.int64(1056))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(120) + ax0_ax1_ax2_fused_1 * T.int64(3) + ax0_ax1_ax2_fused_2) % T.int64(1056) // T.int64(96))
                                        v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(96) + (ax0_ax1_ax2_fused_0 * T.int64(120) + ax0_ax1_ax2_fused_1 * T.int64(3) + ax0_ax1_ax2_fused_2) % T.int64(96))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(40) + ax0_ax1_ax2_fused_1) * T.int64(3) + ax0_ax1_ax2_fused_2 < T.int64(2112))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(12)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(96) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(20))
                                        v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(20) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(20))
                                        T.reads(B[v0, v1])
                                        T.writes(B_shared[v0, v1])
                                        B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(12), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(20) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + i0_1_i1_1_i2_1_fused + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(20) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(96) + k_1 * T.int64(12) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(20) + ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + i0_1_i1_1_i2_1_fused + ax1)
                            v2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(20) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul16(A: T.Buffer((T.int64(16), T.int64(4096), T.int64(77)), "float32"), B: T.Buffer((T.int64(16), T.int64(77), T.int64(40)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(77)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(77), T.int64(40)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(512) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(2) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(512) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(20) // T.int64(5) * T.int64(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(40), i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(5) * T.int64(4) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(11)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(512) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(512) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(56) // T.int64(7))
                                        v2 = T.axis.spatial(T.int64(77), k_0 * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(7))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(40) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(224))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(14)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(512) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(280))
                                        v1 = T.axis.spatial(T.int64(77), k_0 * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(280) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.reads(B[v0, v1, v2])
                                        T.writes(B_shared[v0, v1, v2])
                                        B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(7), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(512) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(2) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(512) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(20) // T.int64(5) * T.int64(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(40), i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(5) * T.int64(4) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(7) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(512) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(512) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(20) // T.int64(5) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(40), i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(5) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul2(A: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), "float32"), B: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), scope="local")
        A_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i3_3_init, i0_4_init, i1_4_init, i2_4_init, i3_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1), i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_i3_1_fused * T.int64(64) + i0_2_i1_2_i2_2_i3_2_fused // T.int64(8) * T.int64(4) + i2_3_init * T.int64(2) + i2_4_init)
                            v_i3 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_i3_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_i3_2_fused % T.int64(8) * T.int64(4) + i3_3_init + i3_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2, v_i3])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                    for k_0 in range(T.int64(1024)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(16) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v3 = T.axis.spatial(T.int64(4096), k_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        T.reads(A[v0, v1, v2, v3])
                                        T.writes(A_shared[v0, v1, v2, v3])
                                        A_shared[v0, v1, v2, v3] = A[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2 = T.axis.spatial(T.int64(4096), k_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_i3_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(128))
                                        T.reads(B[v0, v1, v2, v3])
                                        T.writes(B_shared[v0, v1, v2, v3])
                                        B_shared[v0, v1, v2, v3] = B[v0, v1, v2, v3]
                        for k_1, i0_3, i1_3, i2_3, i3_3, k_2, i0_4, i1_4, i2_4, i3_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1), i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_i3_1_fused * T.int64(64) + i0_2_i1_2_i2_2_i3_2_fused // T.int64(8) * T.int64(4) + i2_3 * T.int64(2) + i2_4)
                                v_i3 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_i3_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_i3_2_fused % T.int64(8) * T.int64(4) + i3_3 + i3_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2, v_i3], A_shared[v_i0, v_i1, v_i2, v_k], B_shared[v_i0, v_i1, v_k, v_i3])
                                T.writes(matmul_local[v_i0, v_i1, v_i2, v_i3])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2, v_i3] = matmul_local[v_i0, v_i1, v_i2, v_i3] + A_shared[v_i0, v_i1, v_i2, v_k] * B_shared[v_i0, v_i1, v_k, v_i3]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("matmul_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_i3_1_fused * T.int64(64) + i0_2_i1_2_i2_2_i3_2_fused // T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_i3_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_i3_2_fused % T.int64(8) * T.int64(4) + ax3)
                            T.reads(matmul_local[v0, v1, v2, v3])
                            T.writes(matmul[v0, v1, v2, v3])
                            matmul[v0, v1, v2, v3] = matmul_local[v0, v1, v2, v3]

    @T.prim_func
    def matmul20(A: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), B: T.Buffer((T.int64(640), T.int64(640)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(640), T.int64(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(128) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(128) // T.int64(16) * T.int64(4) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + i2_3_init * T.int64(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(10)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(2048))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(2048) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(640), k_0 * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(64))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(640), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(B[v0, v1])
                                    T.writes(B_shared[v0, v1])
                                    B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(16), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(128) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(128) // T.int64(16) * T.int64(4) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + i2_3 * T.int64(4) + i2_4)
                                v_k = T.axis.reduce(T.int64(640), k_0 * T.int64(64) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(128) // T.int64(16) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul22(A: T.Buffer((T.int64(16), T.int64(1024), T.int64(1024)), "float32"), B: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(1024)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(80) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(80))
                                        v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(5) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(1024), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(80))
                                        v1 = T.axis.spatial(T.int64(1024), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(16))
                                        T.reads(B[v0, v1, v2])
                                        T.writes(B_shared[v0, v1, v2])
                                        B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(8), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(80) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(1024), k_0 * T.int64(8) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul23(A: T.Buffer((T.int64(2), T.int64(77), T.int64(768)), "float32"), B: T.Buffer((T.int64(768), T.int64(640)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(77), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(640)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(768)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(768), T.int64(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(112), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(10), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(44), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(4) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(44), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(528))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(528) // T.int64(48))
                                        v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(48) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(48))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(11)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(44), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(48) + (ax0_ax1_fused_0 * T.int64(176) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(40))
                                        v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + (ax0_ax1_fused_0 * T.int64(176) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(40))
                                        T.where((ax0_ax1_fused_0 * T.int64(44) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(1920))
                                        T.reads(B[v0, v1])
                                        T.writes(B_shared[v0, v1])
                                        B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(16), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(4) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(48) + k_1 * T.int64(3) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul25(A: T.Buffer((T.int64(16), T.int64(1024), T.int64(77)), "float32"), B: T.Buffer((T.int64(16), T.int64(77), T.int64(80)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(77)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(77), T.int64(80)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(80) * T.int64(4) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(256) // T.int64(2) * T.int64(8) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(80) // T.int64(40) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(40) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(7)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) // T.int64(88))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(256) // T.int64(2) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(88) // T.int64(11))
                                    v2 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(11))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1 < T.int64(704))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(22)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) // T.int64(440))
                                    v1 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(440) // T.int64(40))
                                    v2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(40))
                                    T.reads(B[v0, v1, v2])
                                    T.writes(B_shared[v0, v1, v2])
                                    B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(11), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(80) * T.int64(4) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(256) // T.int64(2) * T.int64(8) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(80) // T.int64(40) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(40) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(11) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(4), T.int64(2), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(80) * T.int64(4) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(256) // T.int64(2) * T.int64(8) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(80) // T.int64(40) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(40) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul28(A: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(640))
                                        v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(640) // T.int64(20))
                                        v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(20) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(20))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(20) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(16))
                                    T.where(ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 < T.int64(320))
                                    T.reads(B[v0, v1])
                                    T.writes(B_shared[v0, v1])
                                    B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(10), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(20) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul30(A: T.Buffer((T.int64(16), T.int64(256), T.int64(256)), "float32"), B: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(256)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(5)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(32) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(4) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(4) * T.int64(4) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(5) + i2_3_init * T.int64(5) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(4) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(256) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(256), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(320))
                                        v1 = T.axis.spatial(T.int64(256), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(320) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.reads(B[v0, v1, v2])
                                        T.writes(B_shared[v0, v1, v2])
                                        B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(5)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(32) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(4) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(4) * T.int64(4) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(5) + i2_3 * T.int64(5) + i2_4)
                                v_k = T.axis.reduce(T.int64(256), k_0 * T.int64(8) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(5)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(4) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(5) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul31(A: T.Buffer((T.int64(2), T.int64(77), T.int64(768)), "float32"), B: T.Buffer((T.int64(768), T.int64(1280)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(77), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(1280)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(768)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(768), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(55), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(32) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(256) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(8) + i2_3_init * T.int64(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(448) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(84))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(448) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(84) // T.int64(12))
                                        v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(12) + (ax0_ax1_ax2_fused_0 * T.int64(448) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(12))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(168))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(12) + (ax0_ax1_fused_0 * T.int64(896) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(256) + (ax0_ax1_fused_0 * T.int64(896) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.where((ax0_ax1_fused_0 * T.int64(224) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(3072))
                                        T.reads(B[v0, v1])
                                        T.writes(B_shared[v0, v1])
                                        B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(6), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(32) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(256) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(8) + i2_3 * T.int64(4) + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(12) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(8)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused + ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(256) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(8) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul33(A: T.Buffer((T.int64(16), T.int64(256), T.int64(77)), "float32"), B: T.Buffer((T.int64(16), T.int64(77), T.int64(160)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(77)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(77), T.int64(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(40) // T.int64(5) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(7)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(352))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(40) // T.int64(5) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(352) // T.int64(11))
                                    v2 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(11))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 < T.int64(1408))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(352))
                                    v1 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(352) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 < T.int64(1408))
                                    T.reads(B[v0, v1, v2])
                                    T.writes(B_shared[v0, v1, v2])
                                    B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(11), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(40) // T.int64(5) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(11) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(40) // T.int64(5) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul36(A: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(80), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init * T.int64(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(8) + i0_1_i1_1_i2_1_fused // T.int64(2) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(128) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(64) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(320) // T.int64(40))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(40))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(40)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(B[v0, v1])
                                    T.writes(B_shared[v0, v1])
                                    B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 * T.int64(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(8) + i0_1_i1_1_i2_1_fused // T.int64(2) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(128) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(64) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(40) + k_1 * T.int64(5) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(8) + i0_1_i1_1_i2_1_fused // T.int64(2) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(128) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(64) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul38(A: T.Buffer((T.int64(16), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(64)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(40) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(40) // T.int64(10) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1) % T.int64(16) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(64), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 < T.int64(32))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(160))
                                        v1 = T.axis.spatial(T.int64(64), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(160) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.reads(B[v0, v1, v2])
                                        T.writes(B_shared[v0, v1, v2])
                                        B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(40) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(40) // T.int64(10) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(64), k_0 * T.int64(4) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(40) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(40) // T.int64(10) + ax1)
                            v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul4(A: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32"), B: T.Buffer((T.int64(12), T.int64(64), T.int64(77)), "float32"), matmul: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(77)), scope="local")
        A_shared = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(64)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(12), T.int64(64), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(44), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(7), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(231), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(3) + i0_2_i1_2_i2_2_fused // T.int64(77) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_1_i1_1_i2_1_fused + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(77) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(2)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(231), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(3) + (ax0_ax1_ax2_fused_0 * T.int64(924) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(224))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(924) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(224) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(64), k_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(924) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(231) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(672))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(231), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(3) + (ax0_ax1_ax2_fused_0 * T.int64(231) + ax0_ax1_ax2_fused_1) // T.int64(2464))
                                    v1 = T.axis.spatial(T.int64(64), k_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(231) + ax0_ax1_ax2_fused_1) % T.int64(2464) // T.int64(77))
                                    v2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(231) + ax0_ax1_ax2_fused_1) % T.int64(77))
                                    T.reads(B[v0, v1, v2])
                                    T.writes(B_shared[v0, v1, v2])
                                    B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(3) + i0_2_i1_2_i2_2_fused // T.int64(77) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_1_i1_1_i2_1_fused + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(77) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(64), k_0 * T.int64(32) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(3) + i0_2_i1_2_i2_2_fused // T.int64(77) + ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_1_i1_1_i2_1_fused + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(77) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul40(A: T.Buffer((T.int64(16), T.int64(64), T.int64(77)), "float32"), B: T.Buffer((T.int64(16), T.int64(77), T.int64(160)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(77)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(77), T.int64(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(5)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(4) * T.int64(2) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + i2_3_init * T.int64(5) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(7)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(4) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(704))
                                        v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(704) // T.int64(11))
                                        v2 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(11))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(1408))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(4) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(440))
                                        v1 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(440) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(880))
                                        T.reads(B[v0, v1, v2])
                                        T.writes(B_shared[v0, v1, v2])
                                        B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(11), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(5)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(4) * T.int64(2) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + i2_3 * T.int64(5) + i2_4)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(11) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(1), T.int64(5)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(4) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul5(A: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32"), B: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32"), matmul: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(64)), scope="local")
        A_shared = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(77)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(64)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(66), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(56) * T.int64(3) + i0_2_i1_2_i2_2_fused // T.int64(22) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(56) // T.int64(8) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(22) // T.int64(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(2) * T.int64(4) + i2_3_init * T.int64(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(1)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(39)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(66), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(56) * T.int64(3) + (ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1) // T.int64(847))
                                    v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(56) // T.int64(8) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1) % T.int64(847) // T.int64(77))
                                    v2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1) % T.int64(77))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1 < T.int64(2541))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(28)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(66), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(56) * T.int64(3) + (ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1) // T.int64(616))
                                    v1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1) % T.int64(616) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(B[v0, v1, v2])
                                    T.writes(B_shared[v0, v1, v2])
                                    B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(11), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(56) * T.int64(3) + i0_2_i1_2_i2_2_fused // T.int64(22) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(56) // T.int64(8) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(22) // T.int64(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(2) * T.int64(4) + i2_3 * T.int64(4) + i2_4)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(77) + k_1 * T.int64(11) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(56) * T.int64(3) + i0_2_i1_2_i2_2_fused // T.int64(22) + ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(56) // T.int64(8) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(22) // T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(2) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def multiply(A: T.Buffer((), "float32"), B: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[()], B[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = A[()] * B[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply1(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(55) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply2(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(59) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply27(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(23) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply28(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(16) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply29(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(5) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply3(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(37) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply30(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(3) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply4(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(9) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply5(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0.041666667908430099) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply6(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(7.6775431632995605) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def reshape23(A: T.Buffer((T.int64(2), T.int64(320)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(320), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(320))
                    v_ax1 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(320))
                    v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax3 = T.axis.spatial(T.int64(1), T.int64(0))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(640))
                    T.reads(A[((v_ax1 + v_ax2 + v_ax3) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(320)])
                    T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = A[((v_ax1 + v_ax2 + v_ax3) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(320)]

    @T.prim_func
    def reshape33(A: T.Buffer((T.int64(2), T.int64(640)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(640))
                    v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(640))
                    v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax3 = T.axis.spatial(T.int64(1), T.int64(0))
                    T.reads(A[((v_ax1 + v_ax2 + v_ax3) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(640)])
                    T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = A[((v_ax1 + v_ax2 + v_ax3) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(640)]

    @T.prim_func
    def reshape43(A: T.Buffer((T.int64(2), T.int64(1280)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1280))
                    v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1280))
                    v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax3 = T.axis.spatial(T.int64(1), T.int64(0))
                    T.reads(A[((v_ax1 + v_ax2 + v_ax3) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(1280)])
                    T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = A[((v_ax1 + v_ax2 + v_ax3) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(1280)]

    @T.prim_func
    def resize2d(A: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), resize: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(128)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i1 = T.axis.spatial(T.int64(512), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(16384))
                        v_i2 = T.axis.spatial(T.int64(128), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(16384) // T.int64(128))
                        v_i3 = T.axis.spatial(T.int64(128), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(128))
                        T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(63)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(63)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(63)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(63)), T.int64(0))]

    @T.prim_func
    def resize2d1(A: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), resize: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(512)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i1 = T.axis.spatial(T.int64(512), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(65536))
                        v_i2 = T.axis.spatial(T.int64(256), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(65536) // T.int64(256))
                        v_i3 = T.axis.spatial(T.int64(256), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(256))
                        T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(127)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(127)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(127)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(127)), T.int64(0))]

    @T.prim_func
    def resize2d2(A: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), resize: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(1024)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i1 = T.axis.spatial(T.int64(256), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(262144))
                        v_i2 = T.axis.spatial(T.int64(512), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(262144) // T.int64(512))
                        v_i3 = T.axis.spatial(T.int64(512), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(512))
                        T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(255)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(255)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(255)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(255)), T.int64(0))]

    @T.prim_func
    def resize2d3(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), resize: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(10)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(2), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(327680))
                        v_i1 = T.axis.spatial(T.int64(1280), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(327680) // T.int64(256))
                        v_i2 = T.axis.spatial(T.int64(16), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(256) // T.int64(16))
                        v_i3 = T.axis.spatial(T.int64(16), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(16))
                        T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(7)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(7)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(7)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(7)), T.int64(0))]

    @T.prim_func
    def resize2d4(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), resize: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(40)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(2), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(1310720))
                        v_i1 = T.axis.spatial(T.int64(1280), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(1310720) // T.int64(1024))
                        v_i2 = T.axis.spatial(T.int64(32), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(1024) // T.int64(32))
                        v_i3 = T.axis.spatial(T.int64(32), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(32))
                        T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(15)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(15)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(15)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(15)), T.int64(0))]

    @T.prim_func
    def resize2d5(A: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), resize: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(80)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(2), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(2621440))
                        v_i1 = T.axis.spatial(T.int64(640), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(2621440) // T.int64(4096))
                        v_i2 = T.axis.spatial(T.int64(64), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(4096) // T.int64(64))
                        v_i3 = T.axis.spatial(T.int64(64), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(64))
                        T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(31)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(31)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(31)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(31)), T.int64(0))]

    @T.prim_func
    def silu6(A: T.Buffer((T.int64(2), T.int64(1280)), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x"):
            for i0_i1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("compute"):
                    v_i0 = T.axis.spatial(T.int64(2), (i0_i1_fused_0 * T.int64(256) + i0_i1_fused_1) // T.int64(1280))
                    v_i1 = T.axis.spatial(T.int64(1280), (i0_i1_fused_0 * T.int64(256) + i0_i1_fused_1) % T.int64(1280))
                    T.reads(A[v_i0, v_i1])
                    T.writes(T_multiply[v_i0, v_i1])
                    T_multiply[v_i0, v_i1] = A[v_i0, v_i1] * T.sigmoid(A[v_i0, v_i1])

    @T.prim_func
    def softmax(A: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), "float32"), T_softmax_norm: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)), scope="shared")
        for i0_i1_i2_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0, v_i1 = T.axis.remap("SS", [ax0, ax1])
                        v_i2 = T.axis.spatial(T.int64(4096), i0_i1_i2_fused + ax2)
                        v_k = T.axis.reduce(T.int64(4096), ax3_0 * T.int64(256) + ax3_1)
                        T.reads(A[v_i0, v_i1, v_i2, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1, v_i2])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1, v_i2] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1, v_i2] = T.max(T_softmax_maxelem_shared[v_i0, v_i1, v_i2], A[v_i0, v_i1, v_i2, v_k])
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0, v_i1 = T.axis.remap("SS", [ax0, ax1])
                        v_i2 = T.axis.spatial(T.int64(4096), i0_i1_i2_fused + ax2)
                        v_k = T.axis.reduce(T.int64(4096), ax3_0 * T.int64(256) + ax3_1)
                        T.reads(A[v_i0, v_i1, v_i2, v_k], T_softmax_maxelem_shared[v_i0, v_i1, v_i2])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1, v_i2])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1, v_i2] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1, v_i2] = T_softmax_expsum_shared[v_i0, v_i1, v_i2] + T.exp(A[v_i0, v_i1, v_i2, v_k] - T_softmax_maxelem_shared[v_i0, v_i1, v_i2])
            for i3_0 in range(T.int64(16)):
                for i3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i1 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i2 = T.axis.spatial(T.int64(4096), i0_i1_i2_fused)
                        v_i3 = T.axis.spatial(T.int64(4096), i3_0 * T.int64(256) + i3_1)
                        T.reads(A[v_i0, v_i1, v_i2, v_i3], T_softmax_maxelem_shared[v_i0, v_i1, v_i2], T_softmax_expsum_shared[v_i0, v_i1, v_i2])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2, v_i3])
                        T.block_attr({"axis": 3})
                        T_softmax_norm[v_i0, v_i1, v_i2, v_i3] = T.exp(A[v_i0, v_i1, v_i2, v_i3] - T_softmax_maxelem_shared[v_i0, v_i1, v_i2]) / T_softmax_expsum_shared[v_i0, v_i1, v_i2]

    @T.prim_func
    def softmax1(A: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(12), T.int64(77)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(12), T.int64(77)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(924), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(20)):
                for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(12), i0_i1_fused // T.int64(77) + ax0)
                        v_i1 = T.axis.spatial(T.int64(77), i0_i1_fused % T.int64(77) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(4) + ax2_1)
                        T.where(ax2_0 * T.int64(4) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(20)):
                for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(12), i0_i1_fused // T.int64(77) + ax0)
                        v_i1 = T.axis.spatial(T.int64(77), i0_i1_fused % T.int64(77) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(4) + ax2_1)
                        T.where(ax2_0 * T.int64(4) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(20)):
                for i2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(12), i0_i1_fused // T.int64(77))
                        v_i1 = T.axis.spatial(T.int64(77), i0_i1_fused % T.int64(77))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(4) + i2_1)
                        T.where(i2_0 * T.int64(4) + i2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax2(A: T.Buffer((T.int64(16), T.int64(4096), T.int64(4096)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(4096)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(4096)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(65536), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096) + ax1)
                        v_k = T.axis.reduce(T.int64(4096), ax2_0 * T.int64(256) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096) + ax1)
                        v_k = T.axis.reduce(T.int64(4096), ax2_0 * T.int64(256) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(16)):
                for i2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096))
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096))
                        v_i2 = T.axis.spatial(T.int64(4096), i2_0 * T.int64(256) + i2_1)
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax3(A: T.Buffer((T.int64(16), T.int64(4096), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(4096), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(4096)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(4096)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(65536), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(10)):
                for ax2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(8) + ax2_1)
                        T.where(ax2_0 * T.int64(8) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(10)):
                for ax2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(8) + ax2_1)
                        T.where(ax2_0 * T.int64(8) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(10)):
                for i2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096))
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(8) + i2_1)
                        T.where(i2_0 * T.int64(8) + i2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax4(A: T.Buffer((T.int64(16), T.int64(1024), T.int64(1024)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(1024), T.int64(1024)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(1024)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(1024)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(16384), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024) + ax1)
                        v_k = T.axis.reduce(T.int64(1024), ax2_0 * T.int64(256) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024) + ax1)
                        v_k = T.axis.reduce(T.int64(1024), ax2_0 * T.int64(256) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(4)):
                for i2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024))
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024))
                        v_i2 = T.axis.spatial(T.int64(1024), i2_0 * T.int64(256) + i2_1)
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax5(A: T.Buffer((T.int64(16), T.int64(1024), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(1024), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(1024)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(1024)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(16384), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(5)):
                for ax2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(16) + ax2_1)
                        T.where(ax2_0 * T.int64(16) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(5)):
                for ax2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(16) + ax2_1)
                        T.where(ax2_0 * T.int64(16) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(5)):
                for i2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024))
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(16) + i2_1)
                        T.where(i2_0 * T.int64(16) + i2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax6(A: T.Buffer((T.int64(16), T.int64(256), T.int64(256)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(256)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(256)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256) + ax0)
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256) + ax1)
                        v_k = T.axis.reduce(T.int64(256), ax2_0 * T.int64(16) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256) + ax0)
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256) + ax1)
                        v_k = T.axis.reduce(T.int64(256), ax2_0 * T.int64(16) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(16)):
                for i2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256))
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256))
                        v_i2 = T.axis.spatial(T.int64(256), i2_0 * T.int64(16) + i2_1)
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax7(A: T.Buffer((T.int64(16), T.int64(256), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(256), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(256)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(256)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(2)):
                for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256) + ax0)
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(2)):
                for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256) + ax0)
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(2)):
                for i2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256))
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(64) + i2_1)
                        T.where(i2_0 * T.int64(64) + i2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax8(A: T.Buffer((T.int64(16), T.int64(64), T.int64(64)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(64)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(64)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64) + ax0)
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64) + ax1)
                        v_k = T.axis.reduce(T.int64(64), ax2_0 * T.int64(4) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64) + ax0)
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64) + ax1)
                        v_k = T.axis.reduce(T.int64(64), ax2_0 * T.int64(4) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(16)):
                for i2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64))
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64))
                        v_i2 = T.axis.spatial(T.int64(64), i2_0 * T.int64(4) + i2_1)
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax9(A: T.Buffer((T.int64(16), T.int64(64), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(64), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(64)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(64)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(20)):
                for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64) + ax0)
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(4) + ax2_1)
                        T.where(ax2_0 * T.int64(4) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(20)):
                for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64) + ax0)
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(4) + ax2_1)
                        T.where(ax2_0 * T.int64(4) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(20)):
                for i2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64))
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(4) + i2_1)
                        T.where(i2_0 * T.int64(4) + i2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def subtract(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_subtract: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_subtract"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3], B[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_subtract[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_subtract[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] - B[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def take(A: T.Buffer((T.int64(49408), T.int64(768)), "float32"), B: T.Buffer((T.int64(77),), "int32"), T_take: T.Buffer((T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 8, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(462), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_take"):
                    v_ax0 = T.axis.spatial(T.int64(77), (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(768))
                    v_ax1 = T.axis.spatial(T.int64(768), (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(768))
                    T.reads(A[B[v_ax0], v_ax1], B[v_ax0])
                    T.writes(T_take[v_ax0, v_ax1])
                    T_take[v_ax0, v_ax1] = A[B[v_ax0], v_ax1]

    @T.prim_func
    def tir_image_to_rgba(A: T.Buffer((T.int64(1), T.int64(512), T.int64(512), T.int64(3)), "float32"), image_to_rgba: T.Buffer((T.int64(512), T.int64(512)), "uint32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for y_x_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for y_x_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for y_x_fused_0 in range(T.int64(4)):
                    with T.block("image_to_rgba"):
                        v_y = T.axis.spatial(T.int64(512), (y_x_fused_0 * T.int64(65536) + y_x_fused_1 * T.int64(256) + y_x_fused_2) // T.int64(512))
                        v_x = T.axis.spatial(T.int64(512), (y_x_fused_0 * T.int64(65536) + y_x_fused_1 * T.int64(256) + y_x_fused_2) % T.int64(512))
                        T.reads(A[T.int64(0), v_y, v_x, T.int64(0):T.int64(3)])
                        T.writes(image_to_rgba[v_y, v_x])
                        image_to_rgba[v_y, v_x] = T.bitwise_or(T.bitwise_or(T.bitwise_or(T.Cast("uint32", A[T.int64(0), v_y, v_x, T.int64(0)]), T.shift_left(T.Cast("uint32", A[T.int64(0), v_y, v_x, T.int64(1)]), T.uint32(8))), T.shift_left(T.Cast("uint32", A[T.int64(0), v_y, v_x, T.int64(2)]), T.uint32(16))), T.uint32(4278190080))

    @T.prim_func
    def transpose(A: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(32)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(512))
                        v_ax2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(512))
                        T.reads(A[v_ax0, v_ax2, v_ax1])
                        T.writes(T_transpose[v_ax0, v_ax1, v_ax2])
                        T_transpose[v_ax0, v_ax1, v_ax2] = A[v_ax0, v_ax2, v_ax1]

    @R.function
    def clip(inp_0: R.Tensor((1, 77), dtype="int32"), transformed_param_0: R.Tensor((49408, 768), dtype="float32"), transformed_param_1: R.Tensor((768,), dtype="float32"), transformed_param_2: R.Tensor((768,), dtype="float32"), transformed_param_3: R.Tensor((768,), dtype="float32"), transformed_param_4: R.Tensor((768,), dtype="float32"), transformed_param_5: R.Tensor((3072,), dtype="float32"), transformed_param_6: R.Tensor((768,), dtype="float32"), transformed_param_7: R.Tensor((768,), dtype="float32"), transformed_param_8: R.Tensor((768,), dtype="float32"), transformed_param_9: R.Tensor((768,), dtype="float32"), transformed_param_10: R.Tensor((768,), dtype="float32"), transformed_param_11: R.Tensor((768,), dtype="float32"), transformed_param_12: R.Tensor((768,), dtype="float32"), transformed_param_13: R.Tensor((768,), dtype="float32"), transformed_param_14: R.Tensor((768,), dtype="float32"), transformed_param_15: R.Tensor((3072,), dtype="float32"), transformed_param_16: R.Tensor((768,), dtype="float32"), transformed_param_17: R.Tensor((768,), dtype="float32"), transformed_param_18: R.Tensor((768,), dtype="float32"), transformed_param_19: R.Tensor((768,), dtype="float32"), transformed_param_20: R.Tensor((768,), dtype="float32"), transformed_param_21: R.Tensor((768,), dtype="float32"), transformed_param_22: R.Tensor((768,), dtype="float32"), transformed_param_23: R.Tensor((768,), dtype="float32"), transformed_param_24: R.Tensor((768,), dtype="float32"), transformed_param_25: R.Tensor((3072,), dtype="float32"), transformed_param_26: R.Tensor((768,), dtype="float32"), transformed_param_27: R.Tensor((768,), dtype="float32"), transformed_param_28: R.Tensor((768,), dtype="float32"), transformed_param_29: R.Tensor((768,), dtype="float32"), transformed_param_30: R.Tensor((768,), dtype="float32"), transformed_param_31: R.Tensor((768,), dtype="float32"), transformed_param_32: R.Tensor((768,), dtype="float32"), transformed_param_33: R.Tensor((768,), dtype="float32"), transformed_param_34: R.Tensor((768,), dtype="float32"), transformed_param_35: R.Tensor((3072,), dtype="float32"), transformed_param_36: R.Tensor((768,), dtype="float32"), transformed_param_37: R.Tensor((768,), dtype="float32"), transformed_param_38: R.Tensor((768,), dtype="float32"), transformed_param_39: R.Tensor((768,), dtype="float32"), transformed_param_40: R.Tensor((768,), dtype="float32"), transformed_param_41: R.Tensor((768,), dtype="float32"), transformed_param_42: R.Tensor((768,), dtype="float32"), transformed_param_43: R.Tensor((768,), dtype="float32"), transformed_param_44: R.Tensor((768,), dtype="float32"), transformed_param_45: R.Tensor((3072,), dtype="float32"), transformed_param_46: R.Tensor((768,), dtype="float32"), transformed_param_47: R.Tensor((768,), dtype="float32"), transformed_param_48: R.Tensor((768,), dtype="float32"), transformed_param_49: R.Tensor((768,), dtype="float32"), transformed_param_50: R.Tensor((768,), dtype="float32"), transformed_param_51: R.Tensor((768,), dtype="float32"), transformed_param_52: R.Tensor((768,), dtype="float32"), transformed_param_53: R.Tensor((768,), dtype="float32"), transformed_param_54: R.Tensor((768,), dtype="float32"), transformed_param_55: R.Tensor((3072,), dtype="float32"), transformed_param_56: R.Tensor((768,), dtype="float32"), transformed_param_57: R.Tensor((768,), dtype="float32"), transformed_param_58: R.Tensor((768,), dtype="float32"), transformed_param_59: R.Tensor((768,), dtype="float32"), transformed_param_60: R.Tensor((768,), dtype="float32"), transformed_param_61: R.Tensor((768,), dtype="float32"), transformed_param_62: R.Tensor((768,), dtype="float32"), transformed_param_63: R.Tensor((768,), dtype="float32"), transformed_param_64: R.Tensor((768,), dtype="float32"), transformed_param_65: R.Tensor((3072,), dtype="float32"), transformed_param_66: R.Tensor((768,), dtype="float32"), transformed_param_67: R.Tensor((768,), dtype="float32"), transformed_param_68: R.Tensor((768,), dtype="float32"), transformed_param_69: R.Tensor((768,), dtype="float32"), transformed_param_70: R.Tensor((768,), dtype="float32"), transformed_param_71: R.Tensor((768,), dtype="float32"), transformed_param_72: R.Tensor((768,), dtype="float32"), transformed_param_73: R.Tensor((768,), dtype="float32"), transformed_param_74: R.Tensor((768,), dtype="float32"), transformed_param_75: R.Tensor((3072,), dtype="float32"), transformed_param_76: R.Tensor((768,), dtype="float32"), transformed_param_77: R.Tensor((768,), dtype="float32"), transformed_param_78: R.Tensor((768,), dtype="float32"), transformed_param_79: R.Tensor((768,), dtype="float32"), transformed_param_80: R.Tensor((768,), dtype="float32"), transformed_param_81: R.Tensor((768,), dtype="float32"), transformed_param_82: R.Tensor((768,), dtype="float32"), transformed_param_83: R.Tensor((768,), dtype="float32"), transformed_param_84: R.Tensor((768,), dtype="float32"), transformed_param_85: R.Tensor((3072,), dtype="float32"), transformed_param_86: R.Tensor((768,), dtype="float32"), transformed_param_87: R.Tensor((768,), dtype="float32"), transformed_param_88: R.Tensor((768,), dtype="float32"), transformed_param_89: R.Tensor((768,), dtype="float32"), transformed_param_90: R.Tensor((768,), dtype="float32"), transformed_param_91: R.Tensor((768,), dtype="float32"), transformed_param_92: R.Tensor((768,), dtype="float32"), transformed_param_93: R.Tensor((768,), dtype="float32"), transformed_param_94: R.Tensor((768,), dtype="float32"), transformed_param_95: R.Tensor((3072,), dtype="float32"), transformed_param_96: R.Tensor((768,), dtype="float32"), transformed_param_97: R.Tensor((768,), dtype="float32"), transformed_param_98: R.Tensor((768,), dtype="float32"), transformed_param_99: R.Tensor((768,), dtype="float32"), transformed_param_100: R.Tensor((768,), dtype="float32"), transformed_param_101: R.Tensor((768,), dtype="float32"), transformed_param_102: R.Tensor((768,), dtype="float32"), transformed_param_103: R.Tensor((768,), dtype="float32"), transformed_param_104: R.Tensor((768,), dtype="float32"), transformed_param_105: R.Tensor((3072,), dtype="float32"), transformed_param_106: R.Tensor((768,), dtype="float32"), transformed_param_107: R.Tensor((768,), dtype="float32"), transformed_param_108: R.Tensor((768,), dtype="float32"), transformed_param_109: R.Tensor((768,), dtype="float32"), transformed_param_110: R.Tensor((768,), dtype="float32"), transformed_param_111: R.Tensor((768,), dtype="float32"), transformed_param_112: R.Tensor((768,), dtype="float32"), transformed_param_113: R.Tensor((768,), dtype="float32"), transformed_param_114: R.Tensor((768,), dtype="float32"), transformed_param_115: R.Tensor((3072,), dtype="float32"), transformed_param_116: R.Tensor((768,), dtype="float32"), transformed_param_117: R.Tensor((768,), dtype="float32"), transformed_param_118: R.Tensor((768,), dtype="float32"), transformed_param_119: R.Tensor((768,), dtype="float32"), transformed_param_120: R.Tensor((768,), dtype="float32"), transformed_param_121: R.Tensor((768,), dtype="float32"), transformed_param_122: R.Tensor((768,), dtype="float32"), transformed_param_123: R.Tensor((77, 768), dtype="float32"), transformed_param_124: R.Tensor((768, 768), dtype="float32"), transformed_param_125: R.Tensor((768, 768), dtype="float32"), transformed_param_126: R.Tensor((768, 768), dtype="float32"), transformed_param_127: R.Tensor((768, 768), dtype="float32"), transformed_param_128: R.Tensor((768, 3072), dtype="float32"), transformed_param_129: R.Tensor((3072, 768), dtype="float32"), transformed_param_130: R.Tensor((768, 768), dtype="float32"), transformed_param_131: R.Tensor((768, 768), dtype="float32"), transformed_param_132: R.Tensor((768, 768), dtype="float32"), transformed_param_133: R.Tensor((768, 768), dtype="float32"), transformed_param_134: R.Tensor((768, 3072), dtype="float32"), transformed_param_135: R.Tensor((3072, 768), dtype="float32"), transformed_param_136: R.Tensor((768, 768), dtype="float32"), transformed_param_137: R.Tensor((768, 768), dtype="float32"), transformed_param_138: R.Tensor((768, 768), dtype="float32"), transformed_param_139: R.Tensor((768, 768), dtype="float32"), transformed_param_140: R.Tensor((768, 3072), dtype="float32"), transformed_param_141: R.Tensor((3072, 768), dtype="float32"), transformed_param_142: R.Tensor((768, 768), dtype="float32"), transformed_param_143: R.Tensor((768, 768), dtype="float32"), transformed_param_144: R.Tensor((768, 768), dtype="float32"), transformed_param_145: R.Tensor((768, 768), dtype="float32"), transformed_param_146: R.Tensor((768, 3072), dtype="float32"), transformed_param_147: R.Tensor((3072, 768), dtype="float32"), transformed_param_148: R.Tensor((768, 768), dtype="float32"), transformed_param_149: R.Tensor((768, 768), dtype="float32"), transformed_param_150: R.Tensor((768, 768), dtype="float32"), transformed_param_151: R.Tensor((768, 768), dtype="float32"), transformed_param_152: R.Tensor((768, 3072), dtype="float32"), transformed_param_153: R.Tensor((3072, 768), dtype="float32"), transformed_param_154: R.Tensor((768, 768), dtype="float32"), transformed_param_155: R.Tensor((768, 768), dtype="float32"), transformed_param_156: R.Tensor((768, 768), dtype="float32"), transformed_param_157: R.Tensor((768, 768), dtype="float32"), transformed_param_158: R.Tensor((768, 3072), dtype="float32"), transformed_param_159: R.Tensor((3072, 768), dtype="float32"), transformed_param_160: R.Tensor((768, 768), dtype="float32"), transformed_param_161: R.Tensor((768, 768), dtype="float32"), transformed_param_162: R.Tensor((768, 768), dtype="float32"), transformed_param_163: R.Tensor((768, 768), dtype="float32"), transformed_param_164: R.Tensor((768, 3072), dtype="float32"), transformed_param_165: R.Tensor((3072, 768), dtype="float32"), transformed_param_166: R.Tensor((768, 768), dtype="float32"), transformed_param_167: R.Tensor((768, 768), dtype="float32"), transformed_param_168: R.Tensor((768, 768), dtype="float32"), transformed_param_169: R.Tensor((768, 768), dtype="float32"), transformed_param_170: R.Tensor((768, 3072), dtype="float32"), transformed_param_171: R.Tensor((3072, 768), dtype="float32"), transformed_param_172: R.Tensor((768, 768), dtype="float32"), transformed_param_173: R.Tensor((768, 768), dtype="float32"), transformed_param_174: R.Tensor((768, 768), dtype="float32"), transformed_param_175: R.Tensor((768, 768), dtype="float32"), transformed_param_176: R.Tensor((768, 3072), dtype="float32"), transformed_param_177: R.Tensor((3072, 768), dtype="float32"), transformed_param_178: R.Tensor((768, 768), dtype="float32"), transformed_param_179: R.Tensor((768, 768), dtype="float32"), transformed_param_180: R.Tensor((768, 768), dtype="float32"), transformed_param_181: R.Tensor((768, 768), dtype="float32"), transformed_param_182: R.Tensor((768, 3072), dtype="float32"), transformed_param_183: R.Tensor((3072, 768), dtype="float32"), transformed_param_184: R.Tensor((768, 768), dtype="float32"), transformed_param_185: R.Tensor((768, 768), dtype="float32"), transformed_param_186: R.Tensor((768, 768), dtype="float32"), transformed_param_187: R.Tensor((768, 768), dtype="float32"), transformed_param_188: R.Tensor((768, 3072), dtype="float32"), transformed_param_189: R.Tensor((3072, 768), dtype="float32"), transformed_param_190: R.Tensor((768, 768), dtype="float32"), transformed_param_191: R.Tensor((768, 768), dtype="float32"), transformed_param_192: R.Tensor((768, 768), dtype="float32"), transformed_param_193: R.Tensor((768, 768), dtype="float32"), transformed_param_194: R.Tensor((768, 3072), dtype="float32"), transformed_param_195: R.Tensor((3072, 768), dtype="float32")) -> R.Tensor((1, 77, 768), dtype="float32"):
        R.func_attr({"global_symbol": "subgraph_0", "num_input": 1})
        cls = Module
        with R.dataflow():
            lv518 = R.call_tir(cls.fused_reshape9_cast_reshape10, (inp_0,), out_sinfo=R.Tensor((77,), dtype="int32"))
            lv: R.Tensor((49408, 768), dtype="float32") = transformed_param_0
            lv3 = R.call_tir(cls.take, (lv, lv518), out_sinfo=R.Tensor((77, 768), dtype="float32"))
            lv1: R.Tensor((77, 768), dtype="float32") = transformed_param_123
            lv519 = R.call_tir(cls.fused_reshape11_reshape11_add15, (lv3, lv1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv2: R.Tensor((768,), dtype="float32") = transformed_param_2
            lv3_1: R.Tensor((768,), dtype="float32") = transformed_param_1
            lv21 = R.call_tir(cls.layer_norm, (lv519, lv2, lv3_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv4: R.Tensor((768, 768), dtype="float32") = transformed_param_124
            lv5: R.Tensor((768,), dtype="float32") = transformed_param_9
            lv520 = R.call_tir(cls.fused_matmul3_add17_multiply9, (lv21, lv4, lv5), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv6: R.Tensor((768, 768), dtype="float32") = transformed_param_125
            lv7: R.Tensor((768,), dtype="float32") = transformed_param_7
            lv521 = R.call_tir(cls.fused_matmul3_add17, (lv21, lv6, lv7), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv522 = R.call_tir(cls.fused_reshape14_transpose8_reshape15_transpose9, (lv521,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv8: R.Tensor((768, 768), dtype="float32") = transformed_param_126
            lv9: R.Tensor((768,), dtype="float32") = transformed_param_10
            lv523 = R.call_tir(cls.fused_matmul3_add17, (lv21, lv8, lv9), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv524 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv523,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv525 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv520,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv42 = R.call_tir(cls.matmul4, (lv525, lv522), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv526 = R.call_tir(cls.fused_reshape16_add18_reshape17, (lv42, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv46 = R.call_tir(cls.softmax1, (lv526,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv47 = R.call_tir(cls.matmul5, (lv46, lv524), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv527 = R.call_tir(cls.fused_reshape18_transpose10_reshape19, (lv47,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv10: R.Tensor((768, 768), dtype="float32") = transformed_param_127
            lv11: R.Tensor((768,), dtype="float32") = transformed_param_8
            lv528 = R.call_tir(cls.fused_matmul3_add17_add15, (lv527, lv10, lv11, lv519), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv12: R.Tensor((768,), dtype="float32") = transformed_param_4
            lv13: R.Tensor((768,), dtype="float32") = transformed_param_3
            lv55 = R.call_tir(cls.layer_norm, (lv528, lv12, lv13), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv14: R.Tensor((768, 3072), dtype="float32") = transformed_param_128
            lv15: R.Tensor((3072,), dtype="float32") = transformed_param_5
            lv529 = R.call_tir(cls.fused_matmul6_add19_multiply10_tir_sigmoid_multiply11, (lv55, lv14, lv15), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv16: R.Tensor((3072, 768), dtype="float32") = transformed_param_129
            lv17: R.Tensor((768,), dtype="float32") = transformed_param_6
            lv530 = R.call_tir(cls.fused_matmul7_add17_add15, (lv529, lv16, lv17, lv528), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv18: R.Tensor((768,), dtype="float32") = transformed_param_32
            lv19: R.Tensor((768,), dtype="float32") = transformed_param_31
            lv66 = R.call_tir(cls.layer_norm, (lv530, lv18, lv19), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv20: R.Tensor((768, 768), dtype="float32") = transformed_param_130
            lv21_1: R.Tensor((768,), dtype="float32") = transformed_param_39
            lv531 = R.call_tir(cls.fused_matmul3_add17_multiply9, (lv66, lv20, lv21_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv22: R.Tensor((768, 768), dtype="float32") = transformed_param_131
            lv23: R.Tensor((768,), dtype="float32") = transformed_param_37
            lv532 = R.call_tir(cls.fused_matmul3_add17, (lv66, lv22, lv23), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv533 = R.call_tir(cls.fused_reshape14_transpose8_reshape15_transpose9, (lv532,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv24: R.Tensor((768, 768), dtype="float32") = transformed_param_132
            lv25: R.Tensor((768,), dtype="float32") = transformed_param_40
            lv534 = R.call_tir(cls.fused_matmul3_add17, (lv66, lv24, lv25), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv535 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv534,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv536 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv531,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv87 = R.call_tir(cls.matmul4, (lv536, lv533), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv537 = R.call_tir(cls.fused_reshape16_add18_reshape17, (lv87, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv91 = R.call_tir(cls.softmax1, (lv537,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv92 = R.call_tir(cls.matmul5, (lv91, lv535), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv538 = R.call_tir(cls.fused_reshape18_transpose10_reshape19, (lv92,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv26: R.Tensor((768, 768), dtype="float32") = transformed_param_133
            lv27: R.Tensor((768,), dtype="float32") = transformed_param_38
            lv539 = R.call_tir(cls.fused_matmul3_add17_add15, (lv538, lv26, lv27, lv530), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv28: R.Tensor((768,), dtype="float32") = transformed_param_34
            lv29: R.Tensor((768,), dtype="float32") = transformed_param_33
            lv100 = R.call_tir(cls.layer_norm, (lv539, lv28, lv29), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv30: R.Tensor((768, 3072), dtype="float32") = transformed_param_134
            lv31: R.Tensor((3072,), dtype="float32") = transformed_param_35
            lv540 = R.call_tir(cls.fused_matmul6_add19_multiply10_tir_sigmoid_multiply11, (lv100, lv30, lv31), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv32: R.Tensor((3072, 768), dtype="float32") = transformed_param_135
            lv33: R.Tensor((768,), dtype="float32") = transformed_param_36
            lv541 = R.call_tir(cls.fused_matmul7_add17_add15, (lv540, lv32, lv33, lv539), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv34: R.Tensor((768,), dtype="float32") = transformed_param_42
            lv35: R.Tensor((768,), dtype="float32") = transformed_param_41
            lv111 = R.call_tir(cls.layer_norm, (lv541, lv34, lv35), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv36: R.Tensor((768, 768), dtype="float32") = transformed_param_136
            lv37: R.Tensor((768,), dtype="float32") = transformed_param_49
            lv542 = R.call_tir(cls.fused_matmul3_add17_multiply9, (lv111, lv36, lv37), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv38: R.Tensor((768, 768), dtype="float32") = transformed_param_137
            lv39: R.Tensor((768,), dtype="float32") = transformed_param_47
            lv543 = R.call_tir(cls.fused_matmul3_add17, (lv111, lv38, lv39), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv544 = R.call_tir(cls.fused_reshape14_transpose8_reshape15_transpose9, (lv543,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv40: R.Tensor((768, 768), dtype="float32") = transformed_param_138
            lv41: R.Tensor((768,), dtype="float32") = transformed_param_50
            lv545 = R.call_tir(cls.fused_matmul3_add17, (lv111, lv40, lv41), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv546 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv545,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv547 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv542,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv132 = R.call_tir(cls.matmul4, (lv547, lv544), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv548 = R.call_tir(cls.fused_reshape16_add18_reshape17, (lv132, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv136 = R.call_tir(cls.softmax1, (lv548,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv137 = R.call_tir(cls.matmul5, (lv136, lv546), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv549 = R.call_tir(cls.fused_reshape18_transpose10_reshape19, (lv137,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv42_1: R.Tensor((768, 768), dtype="float32") = transformed_param_139
            lv43: R.Tensor((768,), dtype="float32") = transformed_param_48
            lv550 = R.call_tir(cls.fused_matmul3_add17_add15, (lv549, lv42_1, lv43, lv541), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv44: R.Tensor((768,), dtype="float32") = transformed_param_44
            lv45: R.Tensor((768,), dtype="float32") = transformed_param_43
            lv145 = R.call_tir(cls.layer_norm, (lv550, lv44, lv45), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv46_1: R.Tensor((768, 3072), dtype="float32") = transformed_param_140
            lv47_1: R.Tensor((3072,), dtype="float32") = transformed_param_45
            lv551 = R.call_tir(cls.fused_matmul6_add19_multiply10_tir_sigmoid_multiply11, (lv145, lv46_1, lv47_1), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv48: R.Tensor((3072, 768), dtype="float32") = transformed_param_141
            lv49: R.Tensor((768,), dtype="float32") = transformed_param_46
            lv552 = R.call_tir(cls.fused_matmul7_add17_add15, (lv551, lv48, lv49, lv550), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv50: R.Tensor((768,), dtype="float32") = transformed_param_52
            lv51: R.Tensor((768,), dtype="float32") = transformed_param_51
            lv156 = R.call_tir(cls.layer_norm, (lv552, lv50, lv51), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv52: R.Tensor((768, 768), dtype="float32") = transformed_param_142
            lv53: R.Tensor((768,), dtype="float32") = transformed_param_59
            lv553 = R.call_tir(cls.fused_matmul3_add17_multiply9, (lv156, lv52, lv53), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv54: R.Tensor((768, 768), dtype="float32") = transformed_param_143
            lv55_1: R.Tensor((768,), dtype="float32") = transformed_param_57
            lv554 = R.call_tir(cls.fused_matmul3_add17, (lv156, lv54, lv55_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv555 = R.call_tir(cls.fused_reshape14_transpose8_reshape15_transpose9, (lv554,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv56: R.Tensor((768, 768), dtype="float32") = transformed_param_144
            lv57: R.Tensor((768,), dtype="float32") = transformed_param_60
            lv556 = R.call_tir(cls.fused_matmul3_add17, (lv156, lv56, lv57), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv557 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv556,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv558 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv553,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv177 = R.call_tir(cls.matmul4, (lv558, lv555), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv559 = R.call_tir(cls.fused_reshape16_add18_reshape17, (lv177, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv181 = R.call_tir(cls.softmax1, (lv559,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv182 = R.call_tir(cls.matmul5, (lv181, lv557), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv560 = R.call_tir(cls.fused_reshape18_transpose10_reshape19, (lv182,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv58: R.Tensor((768, 768), dtype="float32") = transformed_param_145
            lv59: R.Tensor((768,), dtype="float32") = transformed_param_58
            lv561 = R.call_tir(cls.fused_matmul3_add17_add15, (lv560, lv58, lv59, lv552), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv60: R.Tensor((768,), dtype="float32") = transformed_param_54
            lv61: R.Tensor((768,), dtype="float32") = transformed_param_53
            lv190 = R.call_tir(cls.layer_norm, (lv561, lv60, lv61), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv62: R.Tensor((768, 3072), dtype="float32") = transformed_param_146
            lv63: R.Tensor((3072,), dtype="float32") = transformed_param_55
            lv562 = R.call_tir(cls.fused_matmul6_add19_multiply10_tir_sigmoid_multiply11, (lv190, lv62, lv63), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv64: R.Tensor((3072, 768), dtype="float32") = transformed_param_147
            lv65: R.Tensor((768,), dtype="float32") = transformed_param_56
            lv563 = R.call_tir(cls.fused_matmul7_add17_add15, (lv562, lv64, lv65, lv561), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv66_1: R.Tensor((768,), dtype="float32") = transformed_param_62
            lv67: R.Tensor((768,), dtype="float32") = transformed_param_61
            lv201 = R.call_tir(cls.layer_norm, (lv563, lv66_1, lv67), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv68: R.Tensor((768, 768), dtype="float32") = transformed_param_148
            lv69: R.Tensor((768,), dtype="float32") = transformed_param_69
            lv564 = R.call_tir(cls.fused_matmul3_add17_multiply9, (lv201, lv68, lv69), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv70: R.Tensor((768, 768), dtype="float32") = transformed_param_149
            lv71: R.Tensor((768,), dtype="float32") = transformed_param_67
            lv565 = R.call_tir(cls.fused_matmul3_add17, (lv201, lv70, lv71), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv566 = R.call_tir(cls.fused_reshape14_transpose8_reshape15_transpose9, (lv565,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv72: R.Tensor((768, 768), dtype="float32") = transformed_param_150
            lv73: R.Tensor((768,), dtype="float32") = transformed_param_70
            lv567 = R.call_tir(cls.fused_matmul3_add17, (lv201, lv72, lv73), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv568 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv567,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv569 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv564,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv222 = R.call_tir(cls.matmul4, (lv569, lv566), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv570 = R.call_tir(cls.fused_reshape16_add18_reshape17, (lv222, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv226 = R.call_tir(cls.softmax1, (lv570,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv227 = R.call_tir(cls.matmul5, (lv226, lv568), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv571 = R.call_tir(cls.fused_reshape18_transpose10_reshape19, (lv227,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv74: R.Tensor((768, 768), dtype="float32") = transformed_param_151
            lv75: R.Tensor((768,), dtype="float32") = transformed_param_68
            lv572 = R.call_tir(cls.fused_matmul3_add17_add15, (lv571, lv74, lv75, lv563), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv76: R.Tensor((768,), dtype="float32") = transformed_param_64
            lv77: R.Tensor((768,), dtype="float32") = transformed_param_63
            lv235 = R.call_tir(cls.layer_norm, (lv572, lv76, lv77), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv78: R.Tensor((768, 3072), dtype="float32") = transformed_param_152
            lv79: R.Tensor((3072,), dtype="float32") = transformed_param_65
            lv573 = R.call_tir(cls.fused_matmul6_add19_multiply10_tir_sigmoid_multiply11, (lv235, lv78, lv79), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv80: R.Tensor((3072, 768), dtype="float32") = transformed_param_153
            lv81: R.Tensor((768,), dtype="float32") = transformed_param_66
            lv574 = R.call_tir(cls.fused_matmul7_add17_add15, (lv573, lv80, lv81, lv572), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv82: R.Tensor((768,), dtype="float32") = transformed_param_72
            lv83: R.Tensor((768,), dtype="float32") = transformed_param_71
            lv246 = R.call_tir(cls.layer_norm, (lv574, lv82, lv83), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv84: R.Tensor((768, 768), dtype="float32") = transformed_param_154
            lv85: R.Tensor((768,), dtype="float32") = transformed_param_79
            lv575 = R.call_tir(cls.fused_matmul3_add17_multiply9, (lv246, lv84, lv85), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv86: R.Tensor((768, 768), dtype="float32") = transformed_param_155
            lv87_1: R.Tensor((768,), dtype="float32") = transformed_param_77
            lv576 = R.call_tir(cls.fused_matmul3_add17, (lv246, lv86, lv87_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv577 = R.call_tir(cls.fused_reshape14_transpose8_reshape15_transpose9, (lv576,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv88: R.Tensor((768, 768), dtype="float32") = transformed_param_156
            lv89: R.Tensor((768,), dtype="float32") = transformed_param_80
            lv578 = R.call_tir(cls.fused_matmul3_add17, (lv246, lv88, lv89), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv579 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv578,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv580 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv575,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv267 = R.call_tir(cls.matmul4, (lv580, lv577), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv581 = R.call_tir(cls.fused_reshape16_add18_reshape17, (lv267, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv271 = R.call_tir(cls.softmax1, (lv581,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv272 = R.call_tir(cls.matmul5, (lv271, lv579), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv582 = R.call_tir(cls.fused_reshape18_transpose10_reshape19, (lv272,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv90: R.Tensor((768, 768), dtype="float32") = transformed_param_157
            lv91_1: R.Tensor((768,), dtype="float32") = transformed_param_78
            lv583 = R.call_tir(cls.fused_matmul3_add17_add15, (lv582, lv90, lv91_1, lv574), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv92_1: R.Tensor((768,), dtype="float32") = transformed_param_74
            lv93: R.Tensor((768,), dtype="float32") = transformed_param_73
            lv280 = R.call_tir(cls.layer_norm, (lv583, lv92_1, lv93), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv94: R.Tensor((768, 3072), dtype="float32") = transformed_param_158
            lv95: R.Tensor((3072,), dtype="float32") = transformed_param_75
            lv584 = R.call_tir(cls.fused_matmul6_add19_multiply10_tir_sigmoid_multiply11, (lv280, lv94, lv95), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv96: R.Tensor((3072, 768), dtype="float32") = transformed_param_159
            lv97: R.Tensor((768,), dtype="float32") = transformed_param_76
            lv585 = R.call_tir(cls.fused_matmul7_add17_add15, (lv584, lv96, lv97, lv583), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv98: R.Tensor((768,), dtype="float32") = transformed_param_82
            lv99: R.Tensor((768,), dtype="float32") = transformed_param_81
            lv291 = R.call_tir(cls.layer_norm, (lv585, lv98, lv99), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv100_1: R.Tensor((768, 768), dtype="float32") = transformed_param_160
            lv101: R.Tensor((768,), dtype="float32") = transformed_param_89
            lv586 = R.call_tir(cls.fused_matmul3_add17_multiply9, (lv291, lv100_1, lv101), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv102: R.Tensor((768, 768), dtype="float32") = transformed_param_161
            lv103: R.Tensor((768,), dtype="float32") = transformed_param_87
            lv587 = R.call_tir(cls.fused_matmul3_add17, (lv291, lv102, lv103), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv588 = R.call_tir(cls.fused_reshape14_transpose8_reshape15_transpose9, (lv587,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv104: R.Tensor((768, 768), dtype="float32") = transformed_param_162
            lv105: R.Tensor((768,), dtype="float32") = transformed_param_90
            lv589 = R.call_tir(cls.fused_matmul3_add17, (lv291, lv104, lv105), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv590 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv589,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv591 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv586,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv312 = R.call_tir(cls.matmul4, (lv591, lv588), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv592 = R.call_tir(cls.fused_reshape16_add18_reshape17, (lv312, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv316 = R.call_tir(cls.softmax1, (lv592,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv317 = R.call_tir(cls.matmul5, (lv316, lv590), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv593 = R.call_tir(cls.fused_reshape18_transpose10_reshape19, (lv317,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv106: R.Tensor((768, 768), dtype="float32") = transformed_param_163
            lv107: R.Tensor((768,), dtype="float32") = transformed_param_88
            lv594 = R.call_tir(cls.fused_matmul3_add17_add15, (lv593, lv106, lv107, lv585), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv108: R.Tensor((768,), dtype="float32") = transformed_param_84
            lv109: R.Tensor((768,), dtype="float32") = transformed_param_83
            lv325 = R.call_tir(cls.layer_norm, (lv594, lv108, lv109), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv110: R.Tensor((768, 3072), dtype="float32") = transformed_param_164
            lv111_1: R.Tensor((3072,), dtype="float32") = transformed_param_85
            lv595 = R.call_tir(cls.fused_matmul6_add19_multiply10_tir_sigmoid_multiply11, (lv325, lv110, lv111_1), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv112: R.Tensor((3072, 768), dtype="float32") = transformed_param_165
            lv113: R.Tensor((768,), dtype="float32") = transformed_param_86
            lv596 = R.call_tir(cls.fused_matmul7_add17_add15, (lv595, lv112, lv113, lv594), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv114: R.Tensor((768,), dtype="float32") = transformed_param_92
            lv115: R.Tensor((768,), dtype="float32") = transformed_param_91
            lv336 = R.call_tir(cls.layer_norm, (lv596, lv114, lv115), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv116: R.Tensor((768, 768), dtype="float32") = transformed_param_166
            lv117: R.Tensor((768,), dtype="float32") = transformed_param_99
            lv597 = R.call_tir(cls.fused_matmul3_add17_multiply9, (lv336, lv116, lv117), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv118: R.Tensor((768, 768), dtype="float32") = transformed_param_167
            lv119: R.Tensor((768,), dtype="float32") = transformed_param_97
            lv598 = R.call_tir(cls.fused_matmul3_add17, (lv336, lv118, lv119), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv599 = R.call_tir(cls.fused_reshape14_transpose8_reshape15_transpose9, (lv598,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv120: R.Tensor((768, 768), dtype="float32") = transformed_param_168
            lv121: R.Tensor((768,), dtype="float32") = transformed_param_100
            lv600 = R.call_tir(cls.fused_matmul3_add17, (lv336, lv120, lv121), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv601 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv600,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv602 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv597,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv357 = R.call_tir(cls.matmul4, (lv602, lv599), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv603 = R.call_tir(cls.fused_reshape16_add18_reshape17, (lv357, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv361 = R.call_tir(cls.softmax1, (lv603,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv362 = R.call_tir(cls.matmul5, (lv361, lv601), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv604 = R.call_tir(cls.fused_reshape18_transpose10_reshape19, (lv362,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv122: R.Tensor((768, 768), dtype="float32") = transformed_param_169
            lv123: R.Tensor((768,), dtype="float32") = transformed_param_98
            lv605 = R.call_tir(cls.fused_matmul3_add17_add15, (lv604, lv122, lv123, lv596), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv124: R.Tensor((768,), dtype="float32") = transformed_param_94
            lv125: R.Tensor((768,), dtype="float32") = transformed_param_93
            lv370 = R.call_tir(cls.layer_norm, (lv605, lv124, lv125), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv126: R.Tensor((768, 3072), dtype="float32") = transformed_param_170
            lv127: R.Tensor((3072,), dtype="float32") = transformed_param_95
            lv606 = R.call_tir(cls.fused_matmul6_add19_multiply10_tir_sigmoid_multiply11, (lv370, lv126, lv127), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv128: R.Tensor((3072, 768), dtype="float32") = transformed_param_171
            lv129: R.Tensor((768,), dtype="float32") = transformed_param_96
            lv607 = R.call_tir(cls.fused_matmul7_add17_add15, (lv606, lv128, lv129, lv605), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv130: R.Tensor((768,), dtype="float32") = transformed_param_102
            lv131: R.Tensor((768,), dtype="float32") = transformed_param_101
            lv381 = R.call_tir(cls.layer_norm, (lv607, lv130, lv131), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv132_1: R.Tensor((768, 768), dtype="float32") = transformed_param_172
            lv133: R.Tensor((768,), dtype="float32") = transformed_param_109
            lv608 = R.call_tir(cls.fused_matmul3_add17_multiply9, (lv381, lv132_1, lv133), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv134: R.Tensor((768, 768), dtype="float32") = transformed_param_173
            lv135: R.Tensor((768,), dtype="float32") = transformed_param_107
            lv609 = R.call_tir(cls.fused_matmul3_add17, (lv381, lv134, lv135), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv610 = R.call_tir(cls.fused_reshape14_transpose8_reshape15_transpose9, (lv609,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv136_1: R.Tensor((768, 768), dtype="float32") = transformed_param_174
            lv137_1: R.Tensor((768,), dtype="float32") = transformed_param_110
            lv611 = R.call_tir(cls.fused_matmul3_add17, (lv381, lv136_1, lv137_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv612 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv611,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv613 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv608,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv402 = R.call_tir(cls.matmul4, (lv613, lv610), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv614 = R.call_tir(cls.fused_reshape16_add18_reshape17, (lv402, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv406 = R.call_tir(cls.softmax1, (lv614,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv407 = R.call_tir(cls.matmul5, (lv406, lv612), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv615 = R.call_tir(cls.fused_reshape18_transpose10_reshape19, (lv407,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv138: R.Tensor((768, 768), dtype="float32") = transformed_param_175
            lv139: R.Tensor((768,), dtype="float32") = transformed_param_108
            lv616 = R.call_tir(cls.fused_matmul3_add17_add15, (lv615, lv138, lv139, lv607), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv140: R.Tensor((768,), dtype="float32") = transformed_param_104
            lv141: R.Tensor((768,), dtype="float32") = transformed_param_103
            lv415 = R.call_tir(cls.layer_norm, (lv616, lv140, lv141), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv142: R.Tensor((768, 3072), dtype="float32") = transformed_param_176
            lv143: R.Tensor((3072,), dtype="float32") = transformed_param_105
            lv617 = R.call_tir(cls.fused_matmul6_add19_multiply10_tir_sigmoid_multiply11, (lv415, lv142, lv143), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv144: R.Tensor((3072, 768), dtype="float32") = transformed_param_177
            lv145_1: R.Tensor((768,), dtype="float32") = transformed_param_106
            lv618 = R.call_tir(cls.fused_matmul7_add17_add15, (lv617, lv144, lv145_1, lv616), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv146: R.Tensor((768,), dtype="float32") = transformed_param_112
            lv147: R.Tensor((768,), dtype="float32") = transformed_param_111
            lv426 = R.call_tir(cls.layer_norm, (lv618, lv146, lv147), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv148: R.Tensor((768, 768), dtype="float32") = transformed_param_178
            lv149: R.Tensor((768,), dtype="float32") = transformed_param_119
            lv619 = R.call_tir(cls.fused_matmul3_add17_multiply9, (lv426, lv148, lv149), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv150: R.Tensor((768, 768), dtype="float32") = transformed_param_179
            lv151: R.Tensor((768,), dtype="float32") = transformed_param_117
            lv620 = R.call_tir(cls.fused_matmul3_add17, (lv426, lv150, lv151), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv621 = R.call_tir(cls.fused_reshape14_transpose8_reshape15_transpose9, (lv620,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv152: R.Tensor((768, 768), dtype="float32") = transformed_param_180
            lv153: R.Tensor((768,), dtype="float32") = transformed_param_120
            lv622 = R.call_tir(cls.fused_matmul3_add17, (lv426, lv152, lv153), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv623 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv622,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv624 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv619,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv447 = R.call_tir(cls.matmul4, (lv624, lv621), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv625 = R.call_tir(cls.fused_reshape16_add18_reshape17, (lv447, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv451 = R.call_tir(cls.softmax1, (lv625,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv452 = R.call_tir(cls.matmul5, (lv451, lv623), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv626 = R.call_tir(cls.fused_reshape18_transpose10_reshape19, (lv452,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv154: R.Tensor((768, 768), dtype="float32") = transformed_param_181
            lv155: R.Tensor((768,), dtype="float32") = transformed_param_118
            lv627 = R.call_tir(cls.fused_matmul3_add17_add15, (lv626, lv154, lv155, lv618), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv156_1: R.Tensor((768,), dtype="float32") = transformed_param_114
            lv157: R.Tensor((768,), dtype="float32") = transformed_param_113
            lv460 = R.call_tir(cls.layer_norm, (lv627, lv156_1, lv157), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv158: R.Tensor((768, 3072), dtype="float32") = transformed_param_182
            lv159: R.Tensor((3072,), dtype="float32") = transformed_param_115
            lv628 = R.call_tir(cls.fused_matmul6_add19_multiply10_tir_sigmoid_multiply11, (lv460, lv158, lv159), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv160: R.Tensor((3072, 768), dtype="float32") = transformed_param_183
            lv161: R.Tensor((768,), dtype="float32") = transformed_param_116
            lv629 = R.call_tir(cls.fused_matmul7_add17_add15, (lv628, lv160, lv161, lv627), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv162: R.Tensor((768,), dtype="float32") = transformed_param_12
            lv163: R.Tensor((768,), dtype="float32") = transformed_param_11
            lv471 = R.call_tir(cls.layer_norm, (lv629, lv162, lv163), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv164: R.Tensor((768, 768), dtype="float32") = transformed_param_184
            lv165: R.Tensor((768,), dtype="float32") = transformed_param_19
            lv630 = R.call_tir(cls.fused_matmul3_add17_multiply9, (lv471, lv164, lv165), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv166: R.Tensor((768, 768), dtype="float32") = transformed_param_185
            lv167: R.Tensor((768,), dtype="float32") = transformed_param_17
            lv631 = R.call_tir(cls.fused_matmul3_add17, (lv471, lv166, lv167), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv632 = R.call_tir(cls.fused_reshape14_transpose8_reshape15_transpose9, (lv631,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv168: R.Tensor((768, 768), dtype="float32") = transformed_param_186
            lv169: R.Tensor((768,), dtype="float32") = transformed_param_20
            lv633 = R.call_tir(cls.fused_matmul3_add17, (lv471, lv168, lv169), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv634 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv633,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv635 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv630,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv492 = R.call_tir(cls.matmul4, (lv635, lv632), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv636 = R.call_tir(cls.fused_reshape16_add18_reshape17, (lv492, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv496 = R.call_tir(cls.softmax1, (lv636,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv497 = R.call_tir(cls.matmul5, (lv496, lv634), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv637 = R.call_tir(cls.fused_reshape18_transpose10_reshape19, (lv497,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv170: R.Tensor((768, 768), dtype="float32") = transformed_param_187
            lv171: R.Tensor((768,), dtype="float32") = transformed_param_18
            lv638 = R.call_tir(cls.fused_matmul3_add17_add15, (lv637, lv170, lv171, lv629), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv172: R.Tensor((768,), dtype="float32") = transformed_param_14
            lv173: R.Tensor((768,), dtype="float32") = transformed_param_13
            lv505 = R.call_tir(cls.layer_norm, (lv638, lv172, lv173), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv174: R.Tensor((768, 3072), dtype="float32") = transformed_param_188
            lv175: R.Tensor((3072,), dtype="float32") = transformed_param_15
            lv639 = R.call_tir(cls.fused_matmul6_add19_multiply10_tir_sigmoid_multiply11, (lv505, lv174, lv175), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv176: R.Tensor((3072, 768), dtype="float32") = transformed_param_189
            lv177_1: R.Tensor((768,), dtype="float32") = transformed_param_16
            lv640 = R.call_tir(cls.fused_matmul7_add17_add15, (lv639, lv176, lv177_1, lv638), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv178: R.Tensor((768,), dtype="float32") = transformed_param_22
            lv179: R.Tensor((768,), dtype="float32") = transformed_param_21
            lv516 = R.call_tir(cls.layer_norm, (lv640, lv178, lv179), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv180: R.Tensor((768, 768), dtype="float32") = transformed_param_190
            lv181_1: R.Tensor((768,), dtype="float32") = transformed_param_29
            lv641 = R.call_tir(cls.fused_matmul3_add17_multiply9, (lv516, lv180, lv181_1), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv182_1: R.Tensor((768, 768), dtype="float32") = transformed_param_191
            lv183: R.Tensor((768,), dtype="float32") = transformed_param_27
            lv642 = R.call_tir(cls.fused_matmul3_add17, (lv516, lv182_1, lv183), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv643 = R.call_tir(cls.fused_reshape14_transpose8_reshape15_transpose9, (lv642,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv184: R.Tensor((768, 768), dtype="float32") = transformed_param_192
            lv185: R.Tensor((768,), dtype="float32") = transformed_param_30
            lv644 = R.call_tir(cls.fused_matmul3_add17, (lv516, lv184, lv185), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv645 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv644,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv646 = R.call_tir(cls.fused_reshape14_transpose8_reshape15, (lv641,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv537_1 = R.call_tir(cls.matmul4, (lv646, lv643), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv647 = R.call_tir(cls.fused_reshape16_add18_reshape17, (lv537_1, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv541_1 = R.call_tir(cls.softmax1, (lv647,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv542_1 = R.call_tir(cls.matmul5, (lv541_1, lv645), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv648 = R.call_tir(cls.fused_reshape18_transpose10_reshape19, (lv542_1,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv186: R.Tensor((768, 768), dtype="float32") = transformed_param_193
            lv187: R.Tensor((768,), dtype="float32") = transformed_param_28
            lv649 = R.call_tir(cls.fused_matmul3_add17_add15, (lv648, lv186, lv187, lv640), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv188: R.Tensor((768,), dtype="float32") = transformed_param_24
            lv189: R.Tensor((768,), dtype="float32") = transformed_param_23
            lv550_1 = R.call_tir(cls.layer_norm, (lv649, lv188, lv189), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv190_1: R.Tensor((768, 3072), dtype="float32") = transformed_param_194
            lv191: R.Tensor((3072,), dtype="float32") = transformed_param_25
            lv650 = R.call_tir(cls.fused_matmul6_add19_multiply10_tir_sigmoid_multiply11, (lv550_1, lv190_1, lv191), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv192: R.Tensor((3072, 768), dtype="float32") = transformed_param_195
            lv193: R.Tensor((768,), dtype="float32") = transformed_param_26
            lv651 = R.call_tir(cls.fused_matmul7_add17_add15, (lv650, lv192, lv193, lv649), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv194: R.Tensor((768,), dtype="float32") = transformed_param_122
            lv195: R.Tensor((768,), dtype="float32") = transformed_param_121
            lv561_1 = R.call_tir(cls.layer_norm, (lv651, lv194, lv195), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            gv: R.Tensor((1, 77, 768), dtype="float32") = lv561_1
            R.output(gv)
        return gv

    @R.function
    def concat_embeddings(cond_embeddings: R.Tensor((1, 77, 768), dtype="float32"), uncond_embeddings: R.Tensor((1, 77, 768), dtype="float32")) -> R.Tensor((2, 77, 768), dtype="float32"):
        cls = Module
        with R.dataflow():
            gv = R.call_tir(cls.concatenate, (cond_embeddings, uncond_embeddings), out_sinfo=R.Tensor((2, 77, 768), dtype="float32"))
            R.output(gv)
        return gv

    @R.function
    def dpm_solver_multistep_scheduler_convert_model_output(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), alpha: R.Tensor((), dtype="float32"), sigma: R.Tensor((), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv = R.call_tir(cls.multiply, (sigma, model_output), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv1 = R.call_tir(cls.subtract, (sample, gv), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        converted_model_output = R.call_tir(cls.divide, (gv1, alpha), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return converted_model_output

    @R.function
    def dpm_solver_multistep_scheduler_step(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), last_model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), c0: R.Tensor((), dtype="float32"), c1: R.Tensor((), dtype="float32"), c2: R.Tensor((), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv2 = R.call_tir(cls.multiply, (c0, sample), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv3 = R.call_tir(cls.multiply, (c1, model_output), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv4 = R.call_tir(cls.subtract, (gv2, gv3), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv5 = R.call_tir(cls.subtract, (model_output, last_model_output), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv6 = R.call_tir(cls.multiply, (c2, gv5), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        prev_sample = R.call_tir(cls.subtract, (gv4, gv6), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return prev_sample

    @R.function
    def image_to_rgba(x: R.Tensor((1, 512, 512, 3), dtype="float32")) -> R.Tensor((512, 512), dtype="uint32"):
        cls = Module
        with R.dataflow():
            gv = R.call_tir(cls.tir_image_to_rgba, (x,), out_sinfo=R.Tensor((512, 512), dtype="uint32"))
            R.output(gv)
        return gv

    @R.function
    def pndm_scheduler_step_0(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), sample_coeff: R.Tensor((), dtype="float32"), alpha_diff: R.Tensor((), dtype="float32"), model_output_denom_coeff: R.Tensor((), dtype="float32"), ets0: R.Tensor((1, 4, 64, 64), dtype="float32"), ets1: R.Tensor((1, 4, 64, 64), dtype="float32"), ets2: R.Tensor((1, 4, 64, 64), dtype="float32"), ets3: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv = R.call_tir(cls.multiply, (sample_coeff, sample), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv1 = R.call_tir(cls.multiply, (alpha_diff, model_output), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv2 = R.call_tir(cls.divide, (gv1, model_output_denom_coeff), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        prev_sample = R.call_tir(cls.subtract, (gv, gv2), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return prev_sample

    @R.function
    def pndm_scheduler_step_1(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), sample_coeff: R.Tensor((), dtype="float32"), alpha_diff: R.Tensor((), dtype="float32"), model_output_denom_coeff: R.Tensor((), dtype="float32"), ets0: R.Tensor((1, 4, 64, 64), dtype="float32"), ets1: R.Tensor((1, 4, 64, 64), dtype="float32"), ets2: R.Tensor((1, 4, 64, 64), dtype="float32"), ets3: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv3 = R.call_tir(cls.multiply, (sample_coeff, sample), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv4 = R.call_tir(cls.add, (model_output, ets3), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv5 = R.call_tir(cls.divide12, (gv4,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv6 = R.call_tir(cls.multiply, (alpha_diff, gv5), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv7 = R.call_tir(cls.divide, (gv6, model_output_denom_coeff), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        prev_sample1 = R.call_tir(cls.subtract, (gv3, gv7), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return prev_sample1

    @R.function
    def pndm_scheduler_step_2(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), sample_coeff: R.Tensor((), dtype="float32"), alpha_diff: R.Tensor((), dtype="float32"), model_output_denom_coeff: R.Tensor((), dtype="float32"), ets0: R.Tensor((1, 4, 64, 64), dtype="float32"), ets1: R.Tensor((1, 4, 64, 64), dtype="float32"), ets2: R.Tensor((1, 4, 64, 64), dtype="float32"), ets3: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv8 = R.call_tir(cls.multiply, (sample_coeff, sample), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv9 = R.call_tir(cls.multiply30, (ets3,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv10 = R.call_tir(cls.subtract, (gv9, ets2), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv11 = R.call_tir(cls.divide12, (gv10,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv12 = R.call_tir(cls.multiply, (alpha_diff, gv11), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv13 = R.call_tir(cls.divide, (gv12, model_output_denom_coeff), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        prev_sample2 = R.call_tir(cls.subtract, (gv8, gv13), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return prev_sample2

    @R.function
    def pndm_scheduler_step_3(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), sample_coeff: R.Tensor((), dtype="float32"), alpha_diff: R.Tensor((), dtype="float32"), model_output_denom_coeff: R.Tensor((), dtype="float32"), ets0: R.Tensor((1, 4, 64, 64), dtype="float32"), ets1: R.Tensor((1, 4, 64, 64), dtype="float32"), ets2: R.Tensor((1, 4, 64, 64), dtype="float32"), ets3: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv14 = R.call_tir(cls.multiply, (sample_coeff, sample), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv15 = R.call_tir(cls.multiply27, (ets3,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv16 = R.call_tir(cls.multiply28, (ets2,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv17 = R.call_tir(cls.subtract, (gv15, gv16), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv18 = R.call_tir(cls.multiply29, (ets1,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv19 = R.call_tir(cls.add, (gv17, gv18), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv20 = R.call_tir(cls.divide11, (gv19,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv21 = R.call_tir(cls.multiply, (alpha_diff, gv20), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv22 = R.call_tir(cls.divide, (gv21, model_output_denom_coeff), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        prev_sample3 = R.call_tir(cls.subtract, (gv14, gv22), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return prev_sample3

    @R.function
    def pndm_scheduler_step_4(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), sample_coeff: R.Tensor((), dtype="float32"), alpha_diff: R.Tensor((), dtype="float32"), model_output_denom_coeff: R.Tensor((), dtype="float32"), ets0: R.Tensor((1, 4, 64, 64), dtype="float32"), ets1: R.Tensor((1, 4, 64, 64), dtype="float32"), ets2: R.Tensor((1, 4, 64, 64), dtype="float32"), ets3: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv23 = R.call_tir(cls.multiply, (sample_coeff, sample), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv24 = R.call_tir(cls.multiply1, (ets3,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv25 = R.call_tir(cls.multiply2, (ets2,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv26 = R.call_tir(cls.subtract, (gv24, gv25), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv27 = R.call_tir(cls.multiply3, (ets1,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv28 = R.call_tir(cls.add, (gv26, gv27), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv29 = R.call_tir(cls.multiply4, (ets0,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv30 = R.call_tir(cls.subtract, (gv28, gv29), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv31 = R.call_tir(cls.multiply5, (gv30,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv32 = R.call_tir(cls.multiply, (alpha_diff, gv31), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv33 = R.call_tir(cls.divide, (gv32, model_output_denom_coeff), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        prev_sample4 = R.call_tir(cls.subtract, (gv23, gv33), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return prev_sample4

    @R.function
    def unet(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_1: R.Tensor((), dtype="int32"), inp_2: R.Tensor((2, 77, 768), dtype="float32"), transformed_param_0: R.Tensor((320, 4, 3, 3), dtype="float32"), transformed_param_1: R.Tensor((320,), dtype="float32"), transformed_param_2: R.Tensor((320,), dtype="float32"), transformed_param_3: R.Tensor((4, 320, 3, 3), dtype="float32"), transformed_param_4: R.Tensor((320,), dtype="float32"), transformed_param_5: R.Tensor((320,), dtype="float32"), transformed_param_6: R.Tensor((320, 320, 1, 1), dtype="float32"), transformed_param_7: R.Tensor((320, 320, 1, 1), dtype="float32"), transformed_param_8: R.Tensor((320,), dtype="float32"), transformed_param_9: R.Tensor((320,), dtype="float32"), transformed_param_10: R.Tensor((1280,), dtype="float32"), transformed_param_11: R.Tensor((1280,), dtype="float32"), transformed_param_12: R.Tensor((320,), dtype="float32"), transformed_param_13: R.Tensor((320,), dtype="float32"), transformed_param_14: R.Tensor((320,), dtype="float32"), transformed_param_15: R.Tensor((320,), dtype="float32"), transformed_param_16: R.Tensor((320,), dtype="float32"), transformed_param_17: R.Tensor((320,), dtype="float32"), transformed_param_18: R.Tensor((320,), dtype="float32"), transformed_param_19: R.Tensor((320,), dtype="float32"), transformed_param_20: R.Tensor((320,), dtype="float32"), transformed_param_21: R.Tensor((320, 320, 1, 1), dtype="float32"), transformed_param_22: R.Tensor((320, 320, 1, 1), dtype="float32"), transformed_param_23: R.Tensor((320,), dtype="float32"), transformed_param_24: R.Tensor((320,), dtype="float32"), transformed_param_25: R.Tensor((1280,), dtype="float32"), transformed_param_26: R.Tensor((1280,), dtype="float32"), transformed_param_27: R.Tensor((320,), dtype="float32"), transformed_param_28: R.Tensor((320,), dtype="float32"), transformed_param_29: R.Tensor((320,), dtype="float32"), transformed_param_30: R.Tensor((320,), dtype="float32"), transformed_param_31: R.Tensor((320,), dtype="float32"), transformed_param_32: R.Tensor((320,), dtype="float32"), transformed_param_33: R.Tensor((320,), dtype="float32"), transformed_param_34: R.Tensor((320, 320, 3, 3), dtype="float32"), transformed_param_35: R.Tensor((320, 320, 3, 3), dtype="float32"), transformed_param_36: R.Tensor((320, 320, 3, 3), dtype="float32"), transformed_param_37: R.Tensor((320,), dtype="float32"), transformed_param_38: R.Tensor((320,), dtype="float32"), transformed_param_39: R.Tensor((320,), dtype="float32"), transformed_param_40: R.Tensor((320,), dtype="float32"), transformed_param_41: R.Tensor((320,), dtype="float32"), transformed_param_42: R.Tensor((320, 320, 3, 3), dtype="float32"), transformed_param_43: R.Tensor((320, 320, 3, 3), dtype="float32"), transformed_param_44: R.Tensor((320,), dtype="float32"), transformed_param_45: R.Tensor((320,), dtype="float32"), transformed_param_46: R.Tensor((320,), dtype="float32"), transformed_param_47: R.Tensor((320,), dtype="float32"), transformed_param_48: R.Tensor((320,), dtype="float32"), transformed_param_49: R.Tensor((640,), dtype="float32"), transformed_param_50: R.Tensor((640,), dtype="float32"), transformed_param_51: R.Tensor((640, 640, 1, 1), dtype="float32"), transformed_param_52: R.Tensor((640, 640, 1, 1), dtype="float32"), transformed_param_53: R.Tensor((640,), dtype="float32"), transformed_param_54: R.Tensor((640,), dtype="float32"), transformed_param_55: R.Tensor((2560,), dtype="float32"), transformed_param_56: R.Tensor((2560,), dtype="float32"), transformed_param_57: R.Tensor((640,), dtype="float32"), transformed_param_58: R.Tensor((640,), dtype="float32"), transformed_param_59: R.Tensor((640,), dtype="float32"), transformed_param_60: R.Tensor((640,), dtype="float32"), transformed_param_61: R.Tensor((640,), dtype="float32"), transformed_param_62: R.Tensor((640,), dtype="float32"), transformed_param_63: R.Tensor((640,), dtype="float32"), transformed_param_64: R.Tensor((640,), dtype="float32"), transformed_param_65: R.Tensor((640,), dtype="float32"), transformed_param_66: R.Tensor((640, 640, 1, 1), dtype="float32"), transformed_param_67: R.Tensor((640, 640, 1, 1), dtype="float32"), transformed_param_68: R.Tensor((640,), dtype="float32"), transformed_param_69: R.Tensor((640,), dtype="float32"), transformed_param_70: R.Tensor((2560,), dtype="float32"), transformed_param_71: R.Tensor((2560,), dtype="float32"), transformed_param_72: R.Tensor((640,), dtype="float32"), transformed_param_73: R.Tensor((640,), dtype="float32"), transformed_param_74: R.Tensor((640,), dtype="float32"), transformed_param_75: R.Tensor((640,), dtype="float32"), transformed_param_76: R.Tensor((640,), dtype="float32"), transformed_param_77: R.Tensor((640,), dtype="float32"), transformed_param_78: R.Tensor((640,), dtype="float32"), transformed_param_79: R.Tensor((640, 640, 3, 3), dtype="float32"), transformed_param_80: R.Tensor((640, 320, 3, 3), dtype="float32"), transformed_param_81: R.Tensor((640, 640, 3, 3), dtype="float32"), transformed_param_82: R.Tensor((640, 320, 1, 1), dtype="float32"), transformed_param_83: R.Tensor((320,), dtype="float32"), transformed_param_84: R.Tensor((320,), dtype="float32"), transformed_param_85: R.Tensor((640,), dtype="float32"), transformed_param_86: R.Tensor((640,), dtype="float32"), transformed_param_87: R.Tensor((640,), dtype="float32"), transformed_param_88: R.Tensor((640, 640, 3, 3), dtype="float32"), transformed_param_89: R.Tensor((640, 640, 3, 3), dtype="float32"), transformed_param_90: R.Tensor((640,), dtype="float32"), transformed_param_91: R.Tensor((640,), dtype="float32"), transformed_param_92: R.Tensor((640,), dtype="float32"), transformed_param_93: R.Tensor((640,), dtype="float32"), transformed_param_94: R.Tensor((640,), dtype="float32"), transformed_param_95: R.Tensor((1280,), dtype="float32"), transformed_param_96: R.Tensor((1280,), dtype="float32"), transformed_param_97: R.Tensor((1280, 1280, 1, 1), dtype="float32"), transformed_param_98: R.Tensor((1280, 1280, 1, 1), dtype="float32"), transformed_param_99: R.Tensor((1280,), dtype="float32"), transformed_param_100: R.Tensor((1280,), dtype="float32"), transformed_param_101: R.Tensor((5120,), dtype="float32"), transformed_param_102: R.Tensor((5120,), dtype="float32"), transformed_param_103: R.Tensor((1280,), dtype="float32"), transformed_param_104: R.Tensor((1280,), dtype="float32"), transformed_param_105: R.Tensor((1280,), dtype="float32"), transformed_param_106: R.Tensor((1280,), dtype="float32"), transformed_param_107: R.Tensor((1280,), dtype="float32"), transformed_param_108: R.Tensor((1280,), dtype="float32"), transformed_param_109: R.Tensor((1280,), dtype="float32"), transformed_param_110: R.Tensor((1280,), dtype="float32"), transformed_param_111: R.Tensor((1280,), dtype="float32"), transformed_param_112: R.Tensor((1280, 1280, 1, 1), dtype="float32"), transformed_param_113: R.Tensor((1280, 1280, 1, 1), dtype="float32"), transformed_param_114: R.Tensor((1280,), dtype="float32"), transformed_param_115: R.Tensor((1280,), dtype="float32"), transformed_param_116: R.Tensor((5120,), dtype="float32"), transformed_param_117: R.Tensor((5120,), dtype="float32"), transformed_param_118: R.Tensor((1280,), dtype="float32"), transformed_param_119: R.Tensor((1280,), dtype="float32"), transformed_param_120: R.Tensor((1280,), dtype="float32"), transformed_param_121: R.Tensor((1280,), dtype="float32"), transformed_param_122: R.Tensor((1280,), dtype="float32"), transformed_param_123: R.Tensor((1280,), dtype="float32"), transformed_param_124: R.Tensor((1280,), dtype="float32"), transformed_param_125: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_126: R.Tensor((1280, 640, 3, 3), dtype="float32"), transformed_param_127: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_128: R.Tensor((1280, 640, 1, 1), dtype="float32"), transformed_param_129: R.Tensor((640,), dtype="float32"), transformed_param_130: R.Tensor((640,), dtype="float32"), transformed_param_131: R.Tensor((1280,), dtype="float32"), transformed_param_132: R.Tensor((1280,), dtype="float32"), transformed_param_133: R.Tensor((1280,), dtype="float32"), transformed_param_134: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_135: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_136: R.Tensor((1280,), dtype="float32"), transformed_param_137: R.Tensor((1280,), dtype="float32"), transformed_param_138: R.Tensor((1280,), dtype="float32"), transformed_param_139: R.Tensor((1280,), dtype="float32"), transformed_param_140: R.Tensor((1280,), dtype="float32"), transformed_param_141: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_142: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_143: R.Tensor((1280,), dtype="float32"), transformed_param_144: R.Tensor((1280,), dtype="float32"), transformed_param_145: R.Tensor((1280,), dtype="float32"), transformed_param_146: R.Tensor((1280,), dtype="float32"), transformed_param_147: R.Tensor((1280,), dtype="float32"), transformed_param_148: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_149: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_150: R.Tensor((1280,), dtype="float32"), transformed_param_151: R.Tensor((1280,), dtype="float32"), transformed_param_152: R.Tensor((1280,), dtype="float32"), transformed_param_153: R.Tensor((1280,), dtype="float32"), transformed_param_154: R.Tensor((1280,), dtype="float32"), transformed_param_155: R.Tensor((1280,), dtype="float32"), transformed_param_156: R.Tensor((1280,), dtype="float32"), transformed_param_157: R.Tensor((1280, 1280, 1, 1), dtype="float32"), transformed_param_158: R.Tensor((1280, 1280, 1, 1), dtype="float32"), transformed_param_159: R.Tensor((1280,), dtype="float32"), transformed_param_160: R.Tensor((1280,), dtype="float32"), transformed_param_161: R.Tensor((5120,), dtype="float32"), transformed_param_162: R.Tensor((5120,), dtype="float32"), transformed_param_163: R.Tensor((1280,), dtype="float32"), transformed_param_164: R.Tensor((1280,), dtype="float32"), transformed_param_165: R.Tensor((1280,), dtype="float32"), transformed_param_166: R.Tensor((1280,), dtype="float32"), transformed_param_167: R.Tensor((1280,), dtype="float32"), transformed_param_168: R.Tensor((1280,), dtype="float32"), transformed_param_169: R.Tensor((1280,), dtype="float32"), transformed_param_170: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_171: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_172: R.Tensor((1280,), dtype="float32"), transformed_param_173: R.Tensor((1280,), dtype="float32"), transformed_param_174: R.Tensor((1280,), dtype="float32"), transformed_param_175: R.Tensor((1280,), dtype="float32"), transformed_param_176: R.Tensor((1280,), dtype="float32"), transformed_param_177: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_178: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_179: R.Tensor((1280,), dtype="float32"), transformed_param_180: R.Tensor((1280,), dtype="float32"), transformed_param_181: R.Tensor((1280,), dtype="float32"), transformed_param_182: R.Tensor((1280,), dtype="float32"), transformed_param_183: R.Tensor((1280,), dtype="float32"), transformed_param_184: R.Tensor((1280,), dtype="float32"), transformed_param_185: R.Tensor((1280,), dtype="float32"), transformed_param_186: R.Tensor((1280, 2560, 3, 3), dtype="float32"), transformed_param_187: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_188: R.Tensor((1280, 2560, 1, 1), dtype="float32"), transformed_param_189: R.Tensor((2560,), dtype="float32"), transformed_param_190: R.Tensor((2560,), dtype="float32"), transformed_param_191: R.Tensor((1280,), dtype="float32"), transformed_param_192: R.Tensor((1280,), dtype="float32"), transformed_param_193: R.Tensor((1280,), dtype="float32"), transformed_param_194: R.Tensor((1280, 2560, 3, 3), dtype="float32"), transformed_param_195: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_196: R.Tensor((1280, 2560, 1, 1), dtype="float32"), transformed_param_197: R.Tensor((2560,), dtype="float32"), transformed_param_198: R.Tensor((2560,), dtype="float32"), transformed_param_199: R.Tensor((1280,), dtype="float32"), transformed_param_200: R.Tensor((1280,), dtype="float32"), transformed_param_201: R.Tensor((1280,), dtype="float32"), transformed_param_202: R.Tensor((1280, 2560, 3, 3), dtype="float32"), transformed_param_203: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_204: R.Tensor((1280, 2560, 1, 1), dtype="float32"), transformed_param_205: R.Tensor((2560,), dtype="float32"), transformed_param_206: R.Tensor((2560,), dtype="float32"), transformed_param_207: R.Tensor((1280,), dtype="float32"), transformed_param_208: R.Tensor((1280,), dtype="float32"), transformed_param_209: R.Tensor((1280,), dtype="float32"), transformed_param_210: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_211: R.Tensor((1280,), dtype="float32"), transformed_param_212: R.Tensor((1280,), dtype="float32"), transformed_param_213: R.Tensor((1280, 1280, 1, 1), dtype="float32"), transformed_param_214: R.Tensor((1280, 1280, 1, 1), dtype="float32"), transformed_param_215: R.Tensor((1280,), dtype="float32"), transformed_param_216: R.Tensor((1280,), dtype="float32"), transformed_param_217: R.Tensor((5120,), dtype="float32"), transformed_param_218: R.Tensor((5120,), dtype="float32"), transformed_param_219: R.Tensor((1280,), dtype="float32"), transformed_param_220: R.Tensor((1280,), dtype="float32"), transformed_param_221: R.Tensor((1280,), dtype="float32"), transformed_param_222: R.Tensor((1280,), dtype="float32"), transformed_param_223: R.Tensor((1280,), dtype="float32"), transformed_param_224: R.Tensor((1280,), dtype="float32"), transformed_param_225: R.Tensor((1280,), dtype="float32"), transformed_param_226: R.Tensor((1280,), dtype="float32"), transformed_param_227: R.Tensor((1280,), dtype="float32"), transformed_param_228: R.Tensor((1280, 1280, 1, 1), dtype="float32"), transformed_param_229: R.Tensor((1280, 1280, 1, 1), dtype="float32"), transformed_param_230: R.Tensor((1280,), dtype="float32"), transformed_param_231: R.Tensor((1280,), dtype="float32"), transformed_param_232: R.Tensor((5120,), dtype="float32"), transformed_param_233: R.Tensor((5120,), dtype="float32"), transformed_param_234: R.Tensor((1280,), dtype="float32"), transformed_param_235: R.Tensor((1280,), dtype="float32"), transformed_param_236: R.Tensor((1280,), dtype="float32"), transformed_param_237: R.Tensor((1280,), dtype="float32"), transformed_param_238: R.Tensor((1280,), dtype="float32"), transformed_param_239: R.Tensor((1280,), dtype="float32"), transformed_param_240: R.Tensor((1280,), dtype="float32"), transformed_param_241: R.Tensor((1280,), dtype="float32"), transformed_param_242: R.Tensor((1280,), dtype="float32"), transformed_param_243: R.Tensor((1280, 1280, 1, 1), dtype="float32"), transformed_param_244: R.Tensor((1280, 1280, 1, 1), dtype="float32"), transformed_param_245: R.Tensor((1280,), dtype="float32"), transformed_param_246: R.Tensor((1280,), dtype="float32"), transformed_param_247: R.Tensor((5120,), dtype="float32"), transformed_param_248: R.Tensor((5120,), dtype="float32"), transformed_param_249: R.Tensor((1280,), dtype="float32"), transformed_param_250: R.Tensor((1280,), dtype="float32"), transformed_param_251: R.Tensor((1280,), dtype="float32"), transformed_param_252: R.Tensor((1280,), dtype="float32"), transformed_param_253: R.Tensor((1280,), dtype="float32"), transformed_param_254: R.Tensor((1280,), dtype="float32"), transformed_param_255: R.Tensor((1280,), dtype="float32"), transformed_param_256: R.Tensor((1280, 2560, 3, 3), dtype="float32"), transformed_param_257: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_258: R.Tensor((1280, 2560, 1, 1), dtype="float32"), transformed_param_259: R.Tensor((2560,), dtype="float32"), transformed_param_260: R.Tensor((2560,), dtype="float32"), transformed_param_261: R.Tensor((1280,), dtype="float32"), transformed_param_262: R.Tensor((1280,), dtype="float32"), transformed_param_263: R.Tensor((1280,), dtype="float32"), transformed_param_264: R.Tensor((1280, 2560, 3, 3), dtype="float32"), transformed_param_265: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_266: R.Tensor((1280, 2560, 1, 1), dtype="float32"), transformed_param_267: R.Tensor((2560,), dtype="float32"), transformed_param_268: R.Tensor((2560,), dtype="float32"), transformed_param_269: R.Tensor((1280,), dtype="float32"), transformed_param_270: R.Tensor((1280,), dtype="float32"), transformed_param_271: R.Tensor((1280,), dtype="float32"), transformed_param_272: R.Tensor((1280, 1920, 3, 3), dtype="float32"), transformed_param_273: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_274: R.Tensor((1280, 1920, 1, 1), dtype="float32"), transformed_param_275: R.Tensor((1920,), dtype="float32"), transformed_param_276: R.Tensor((1920,), dtype="float32"), transformed_param_277: R.Tensor((1280,), dtype="float32"), transformed_param_278: R.Tensor((1280,), dtype="float32"), transformed_param_279: R.Tensor((1280,), dtype="float32"), transformed_param_280: R.Tensor((1280, 1280, 3, 3), dtype="float32"), transformed_param_281: R.Tensor((640,), dtype="float32"), transformed_param_282: R.Tensor((640,), dtype="float32"), transformed_param_283: R.Tensor((640, 640, 1, 1), dtype="float32"), transformed_param_284: R.Tensor((640, 640, 1, 1), dtype="float32"), transformed_param_285: R.Tensor((640,), dtype="float32"), transformed_param_286: R.Tensor((640,), dtype="float32"), transformed_param_287: R.Tensor((2560,), dtype="float32"), transformed_param_288: R.Tensor((2560,), dtype="float32"), transformed_param_289: R.Tensor((640,), dtype="float32"), transformed_param_290: R.Tensor((640,), dtype="float32"), transformed_param_291: R.Tensor((640,), dtype="float32"), transformed_param_292: R.Tensor((640,), dtype="float32"), transformed_param_293: R.Tensor((640,), dtype="float32"), transformed_param_294: R.Tensor((640,), dtype="float32"), transformed_param_295: R.Tensor((640,), dtype="float32"), transformed_param_296: R.Tensor((640,), dtype="float32"), transformed_param_297: R.Tensor((640,), dtype="float32"), transformed_param_298: R.Tensor((640, 640, 1, 1), dtype="float32"), transformed_param_299: R.Tensor((640, 640, 1, 1), dtype="float32"), transformed_param_300: R.Tensor((640,), dtype="float32"), transformed_param_301: R.Tensor((640,), dtype="float32"), transformed_param_302: R.Tensor((2560,), dtype="float32"), transformed_param_303: R.Tensor((2560,), dtype="float32"), transformed_param_304: R.Tensor((640,), dtype="float32"), transformed_param_305: R.Tensor((640,), dtype="float32"), transformed_param_306: R.Tensor((640,), dtype="float32"), transformed_param_307: R.Tensor((640,), dtype="float32"), transformed_param_308: R.Tensor((640,), dtype="float32"), transformed_param_309: R.Tensor((640,), dtype="float32"), transformed_param_310: R.Tensor((640,), dtype="float32"), transformed_param_311: R.Tensor((640,), dtype="float32"), transformed_param_312: R.Tensor((640,), dtype="float32"), transformed_param_313: R.Tensor((640, 640, 1, 1), dtype="float32"), transformed_param_314: R.Tensor((640, 640, 1, 1), dtype="float32"), transformed_param_315: R.Tensor((640,), dtype="float32"), transformed_param_316: R.Tensor((640,), dtype="float32"), transformed_param_317: R.Tensor((2560,), dtype="float32"), transformed_param_318: R.Tensor((2560,), dtype="float32"), transformed_param_319: R.Tensor((640,), dtype="float32"), transformed_param_320: R.Tensor((640,), dtype="float32"), transformed_param_321: R.Tensor((640,), dtype="float32"), transformed_param_322: R.Tensor((640,), dtype="float32"), transformed_param_323: R.Tensor((640,), dtype="float32"), transformed_param_324: R.Tensor((640,), dtype="float32"), transformed_param_325: R.Tensor((640,), dtype="float32"), transformed_param_326: R.Tensor((640, 1920, 3, 3), dtype="float32"), transformed_param_327: R.Tensor((640, 640, 3, 3), dtype="float32"), transformed_param_328: R.Tensor((640, 1920, 1, 1), dtype="float32"), transformed_param_329: R.Tensor((1920,), dtype="float32"), transformed_param_330: R.Tensor((1920,), dtype="float32"), transformed_param_331: R.Tensor((640,), dtype="float32"), transformed_param_332: R.Tensor((640,), dtype="float32"), transformed_param_333: R.Tensor((640,), dtype="float32"), transformed_param_334: R.Tensor((640, 1280, 3, 3), dtype="float32"), transformed_param_335: R.Tensor((640, 640, 3, 3), dtype="float32"), transformed_param_336: R.Tensor((640, 1280, 1, 1), dtype="float32"), transformed_param_337: R.Tensor((1280,), dtype="float32"), transformed_param_338: R.Tensor((1280,), dtype="float32"), transformed_param_339: R.Tensor((640,), dtype="float32"), transformed_param_340: R.Tensor((640,), dtype="float32"), transformed_param_341: R.Tensor((640,), dtype="float32"), transformed_param_342: R.Tensor((640, 960, 3, 3), dtype="float32"), transformed_param_343: R.Tensor((640, 640, 3, 3), dtype="float32"), transformed_param_344: R.Tensor((640, 960, 1, 1), dtype="float32"), transformed_param_345: R.Tensor((960,), dtype="float32"), transformed_param_346: R.Tensor((960,), dtype="float32"), transformed_param_347: R.Tensor((640,), dtype="float32"), transformed_param_348: R.Tensor((640,), dtype="float32"), transformed_param_349: R.Tensor((640,), dtype="float32"), transformed_param_350: R.Tensor((640, 640, 3, 3), dtype="float32"), transformed_param_351: R.Tensor((320,), dtype="float32"), transformed_param_352: R.Tensor((320,), dtype="float32"), transformed_param_353: R.Tensor((320, 320, 1, 1), dtype="float32"), transformed_param_354: R.Tensor((320, 320, 1, 1), dtype="float32"), transformed_param_355: R.Tensor((320,), dtype="float32"), transformed_param_356: R.Tensor((320,), dtype="float32"), transformed_param_357: R.Tensor((1280,), dtype="float32"), transformed_param_358: R.Tensor((1280,), dtype="float32"), transformed_param_359: R.Tensor((320,), dtype="float32"), transformed_param_360: R.Tensor((320,), dtype="float32"), transformed_param_361: R.Tensor((320,), dtype="float32"), transformed_param_362: R.Tensor((320,), dtype="float32"), transformed_param_363: R.Tensor((320,), dtype="float32"), transformed_param_364: R.Tensor((320,), dtype="float32"), transformed_param_365: R.Tensor((320,), dtype="float32"), transformed_param_366: R.Tensor((320,), dtype="float32"), transformed_param_367: R.Tensor((320,), dtype="float32"), transformed_param_368: R.Tensor((320, 320, 1, 1), dtype="float32"), transformed_param_369: R.Tensor((320, 320, 1, 1), dtype="float32"), transformed_param_370: R.Tensor((320,), dtype="float32"), transformed_param_371: R.Tensor((320,), dtype="float32"), transformed_param_372: R.Tensor((1280,), dtype="float32"), transformed_param_373: R.Tensor((1280,), dtype="float32"), transformed_param_374: R.Tensor((320,), dtype="float32"), transformed_param_375: R.Tensor((320,), dtype="float32"), transformed_param_376: R.Tensor((320,), dtype="float32"), transformed_param_377: R.Tensor((320,), dtype="float32"), transformed_param_378: R.Tensor((320,), dtype="float32"), transformed_param_379: R.Tensor((320,), dtype="float32"), transformed_param_380: R.Tensor((320,), dtype="float32"), transformed_param_381: R.Tensor((320,), dtype="float32"), transformed_param_382: R.Tensor((320,), dtype="float32"), transformed_param_383: R.Tensor((320, 320, 1, 1), dtype="float32"), transformed_param_384: R.Tensor((320, 320, 1, 1), dtype="float32"), transformed_param_385: R.Tensor((320,), dtype="float32"), transformed_param_386: R.Tensor((320,), dtype="float32"), transformed_param_387: R.Tensor((1280,), dtype="float32"), transformed_param_388: R.Tensor((1280,), dtype="float32"), transformed_param_389: R.Tensor((320,), dtype="float32"), transformed_param_390: R.Tensor((320,), dtype="float32"), transformed_param_391: R.Tensor((320,), dtype="float32"), transformed_param_392: R.Tensor((320,), dtype="float32"), transformed_param_393: R.Tensor((320,), dtype="float32"), transformed_param_394: R.Tensor((320,), dtype="float32"), transformed_param_395: R.Tensor((320,), dtype="float32"), transformed_param_396: R.Tensor((320, 960, 3, 3), dtype="float32"), transformed_param_397: R.Tensor((320, 320, 3, 3), dtype="float32"), transformed_param_398: R.Tensor((320, 960, 1, 1), dtype="float32"), transformed_param_399: R.Tensor((960,), dtype="float32"), transformed_param_400: R.Tensor((960,), dtype="float32"), transformed_param_401: R.Tensor((320,), dtype="float32"), transformed_param_402: R.Tensor((320,), dtype="float32"), transformed_param_403: R.Tensor((320,), dtype="float32"), transformed_param_404: R.Tensor((320, 640, 3, 3), dtype="float32"), transformed_param_405: R.Tensor((320, 320, 3, 3), dtype="float32"), transformed_param_406: R.Tensor((320, 640, 1, 1), dtype="float32"), transformed_param_407: R.Tensor((640,), dtype="float32"), transformed_param_408: R.Tensor((640,), dtype="float32"), transformed_param_409: R.Tensor((320,), dtype="float32"), transformed_param_410: R.Tensor((320,), dtype="float32"), transformed_param_411: R.Tensor((320,), dtype="float32"), transformed_param_412: R.Tensor((320, 640, 3, 3), dtype="float32"), transformed_param_413: R.Tensor((320, 320, 3, 3), dtype="float32"), transformed_param_414: R.Tensor((320, 640, 1, 1), dtype="float32"), transformed_param_415: R.Tensor((640,), dtype="float32"), transformed_param_416: R.Tensor((640,), dtype="float32"), transformed_param_417: R.Tensor((320,), dtype="float32"), transformed_param_418: R.Tensor((320,), dtype="float32"), transformed_param_419: R.Tensor((320,), dtype="float32"), transformed_param_420: R.Tensor((320, 1280), dtype="float32"), transformed_param_421: R.Tensor((1280, 1280), dtype="float32"), transformed_param_422: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_423: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_424: R.Tensor((1280, 320), dtype="float32"), transformed_param_425: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_426: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_427: R.Tensor((320, 320), dtype="float32"), transformed_param_428: R.Tensor((320, 320), dtype="float32"), transformed_param_429: R.Tensor((320, 320), dtype="float32"), transformed_param_430: R.Tensor((320, 320), dtype="float32"), transformed_param_431: R.Tensor((320, 320), dtype="float32"), transformed_param_432: R.Tensor((768, 320), dtype="float32"), transformed_param_433: R.Tensor((768, 320), dtype="float32"), transformed_param_434: R.Tensor((320, 320), dtype="float32"), transformed_param_435: R.Tensor((320, 1280), dtype="float32"), transformed_param_436: R.Tensor((320, 1280), dtype="float32"), transformed_param_437: R.Tensor((1280, 320), dtype="float32"), transformed_param_438: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_439: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_440: R.Tensor((1280, 320), dtype="float32"), transformed_param_441: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_442: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_443: R.Tensor((320, 320), dtype="float32"), transformed_param_444: R.Tensor((320, 320), dtype="float32"), transformed_param_445: R.Tensor((320, 320), dtype="float32"), transformed_param_446: R.Tensor((320, 320), dtype="float32"), transformed_param_447: R.Tensor((320, 320), dtype="float32"), transformed_param_448: R.Tensor((768, 320), dtype="float32"), transformed_param_449: R.Tensor((768, 320), dtype="float32"), transformed_param_450: R.Tensor((320, 320), dtype="float32"), transformed_param_451: R.Tensor((320, 1280), dtype="float32"), transformed_param_452: R.Tensor((320, 1280), dtype="float32"), transformed_param_453: R.Tensor((1280, 320), dtype="float32"), transformed_param_454: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_455: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_456: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_457: R.Tensor((1280, 640), dtype="float32"), transformed_param_458: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_459: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_460: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_461: R.Tensor((640, 640), dtype="float32"), transformed_param_462: R.Tensor((640, 640), dtype="float32"), transformed_param_463: R.Tensor((640, 640), dtype="float32"), transformed_param_464: R.Tensor((640, 640), dtype="float32"), transformed_param_465: R.Tensor((640, 640), dtype="float32"), transformed_param_466: R.Tensor((768, 640), dtype="float32"), transformed_param_467: R.Tensor((768, 640), dtype="float32"), transformed_param_468: R.Tensor((640, 640), dtype="float32"), transformed_param_469: R.Tensor((640, 2560), dtype="float32"), transformed_param_470: R.Tensor((640, 2560), dtype="float32"), transformed_param_471: R.Tensor((2560, 640), dtype="float32"), transformed_param_472: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_473: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_474: R.Tensor((1280, 640), dtype="float32"), transformed_param_475: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_476: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_477: R.Tensor((640, 640), dtype="float32"), transformed_param_478: R.Tensor((640, 640), dtype="float32"), transformed_param_479: R.Tensor((640, 640), dtype="float32"), transformed_param_480: R.Tensor((640, 640), dtype="float32"), transformed_param_481: R.Tensor((640, 640), dtype="float32"), transformed_param_482: R.Tensor((768, 640), dtype="float32"), transformed_param_483: R.Tensor((768, 640), dtype="float32"), transformed_param_484: R.Tensor((640, 640), dtype="float32"), transformed_param_485: R.Tensor((640, 2560), dtype="float32"), transformed_param_486: R.Tensor((640, 2560), dtype="float32"), transformed_param_487: R.Tensor((2560, 640), dtype="float32"), transformed_param_488: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_489: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_490: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_491: R.Tensor((1280, 1280), dtype="float32"), transformed_param_492: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_493: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_494: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_495: R.Tensor((1280, 1280), dtype="float32"), transformed_param_496: R.Tensor((1280, 1280), dtype="float32"), transformed_param_497: R.Tensor((1280, 1280), dtype="float32"), transformed_param_498: R.Tensor((1280, 1280), dtype="float32"), transformed_param_499: R.Tensor((1280, 1280), dtype="float32"), transformed_param_500: R.Tensor((768, 1280), dtype="float32"), transformed_param_501: R.Tensor((768, 1280), dtype="float32"), transformed_param_502: R.Tensor((1280, 1280), dtype="float32"), transformed_param_503: R.Tensor((1280, 5120), dtype="float32"), transformed_param_504: R.Tensor((1280, 5120), dtype="float32"), transformed_param_505: R.Tensor((5120, 1280), dtype="float32"), transformed_param_506: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_507: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_508: R.Tensor((1280, 1280), dtype="float32"), transformed_param_509: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_510: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_511: R.Tensor((1280, 1280), dtype="float32"), transformed_param_512: R.Tensor((1280, 1280), dtype="float32"), transformed_param_513: R.Tensor((1280, 1280), dtype="float32"), transformed_param_514: R.Tensor((1280, 1280), dtype="float32"), transformed_param_515: R.Tensor((1280, 1280), dtype="float32"), transformed_param_516: R.Tensor((768, 1280), dtype="float32"), transformed_param_517: R.Tensor((768, 1280), dtype="float32"), transformed_param_518: R.Tensor((1280, 1280), dtype="float32"), transformed_param_519: R.Tensor((1280, 5120), dtype="float32"), transformed_param_520: R.Tensor((1280, 5120), dtype="float32"), transformed_param_521: R.Tensor((5120, 1280), dtype="float32"), transformed_param_522: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_523: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_524: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_525: R.Tensor((1280, 1280), dtype="float32"), transformed_param_526: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_527: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_528: R.Tensor((1280, 1280), dtype="float32"), transformed_param_529: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_530: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_531: R.Tensor((1280, 1280), dtype="float32"), transformed_param_532: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_533: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_534: R.Tensor((1280, 1280), dtype="float32"), transformed_param_535: R.Tensor((1280, 1280), dtype="float32"), transformed_param_536: R.Tensor((1280, 1280), dtype="float32"), transformed_param_537: R.Tensor((1280, 1280), dtype="float32"), transformed_param_538: R.Tensor((1280, 1280), dtype="float32"), transformed_param_539: R.Tensor((768, 1280), dtype="float32"), transformed_param_540: R.Tensor((768, 1280), dtype="float32"), transformed_param_541: R.Tensor((1280, 1280), dtype="float32"), transformed_param_542: R.Tensor((1280, 5120), dtype="float32"), transformed_param_543: R.Tensor((1280, 5120), dtype="float32"), transformed_param_544: R.Tensor((5120, 1280), dtype="float32"), transformed_param_545: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_546: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_547: R.Tensor((1280, 1280), dtype="float32"), transformed_param_548: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_549: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_550: R.Tensor((1280, 1280), dtype="float32"), transformed_param_551: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_552: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_553: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_554: R.Tensor((1280, 1280), dtype="float32"), transformed_param_555: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_556: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_557: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_558: R.Tensor((1280, 1280), dtype="float32"), transformed_param_559: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_560: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_561: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_562: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_563: R.Tensor((1280, 1280), dtype="float32"), transformed_param_564: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_565: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_566: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_567: R.Tensor((1280, 1280), dtype="float32"), transformed_param_568: R.Tensor((1280, 1280), dtype="float32"), transformed_param_569: R.Tensor((1280, 1280), dtype="float32"), transformed_param_570: R.Tensor((1280, 1280), dtype="float32"), transformed_param_571: R.Tensor((1280, 1280), dtype="float32"), transformed_param_572: R.Tensor((768, 1280), dtype="float32"), transformed_param_573: R.Tensor((768, 1280), dtype="float32"), transformed_param_574: R.Tensor((1280, 1280), dtype="float32"), transformed_param_575: R.Tensor((1280, 5120), dtype="float32"), transformed_param_576: R.Tensor((1280, 5120), dtype="float32"), transformed_param_577: R.Tensor((5120, 1280), dtype="float32"), transformed_param_578: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_579: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_580: R.Tensor((1280, 1280), dtype="float32"), transformed_param_581: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_582: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_583: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_584: R.Tensor((1280, 1280), dtype="float32"), transformed_param_585: R.Tensor((1280, 1280), dtype="float32"), transformed_param_586: R.Tensor((1280, 1280), dtype="float32"), transformed_param_587: R.Tensor((1280, 1280), dtype="float32"), transformed_param_588: R.Tensor((1280, 1280), dtype="float32"), transformed_param_589: R.Tensor((768, 1280), dtype="float32"), transformed_param_590: R.Tensor((768, 1280), dtype="float32"), transformed_param_591: R.Tensor((1280, 1280), dtype="float32"), transformed_param_592: R.Tensor((1280, 5120), dtype="float32"), transformed_param_593: R.Tensor((1280, 5120), dtype="float32"), transformed_param_594: R.Tensor((5120, 1280), dtype="float32"), transformed_param_595: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_596: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_597: R.Tensor((1280, 1280), dtype="float32"), transformed_param_598: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_599: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_600: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_601: R.Tensor((1280, 1280), dtype="float32"), transformed_param_602: R.Tensor((1280, 1280), dtype="float32"), transformed_param_603: R.Tensor((1280, 1280), dtype="float32"), transformed_param_604: R.Tensor((1280, 1280), dtype="float32"), transformed_param_605: R.Tensor((1280, 1280), dtype="float32"), transformed_param_606: R.Tensor((768, 1280), dtype="float32"), transformed_param_607: R.Tensor((768, 1280), dtype="float32"), transformed_param_608: R.Tensor((1280, 1280), dtype="float32"), transformed_param_609: R.Tensor((1280, 5120), dtype="float32"), transformed_param_610: R.Tensor((1280, 5120), dtype="float32"), transformed_param_611: R.Tensor((5120, 1280), dtype="float32"), transformed_param_612: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_613: R.Tensor((1, 1280, 1, 1), dtype="float32"), transformed_param_614: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_615: R.Tensor((1280, 640), dtype="float32"), transformed_param_616: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_617: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_618: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_619: R.Tensor((640, 640), dtype="float32"), transformed_param_620: R.Tensor((640, 640), dtype="float32"), transformed_param_621: R.Tensor((640, 640), dtype="float32"), transformed_param_622: R.Tensor((640, 640), dtype="float32"), transformed_param_623: R.Tensor((640, 640), dtype="float32"), transformed_param_624: R.Tensor((768, 640), dtype="float32"), transformed_param_625: R.Tensor((768, 640), dtype="float32"), transformed_param_626: R.Tensor((640, 640), dtype="float32"), transformed_param_627: R.Tensor((640, 2560), dtype="float32"), transformed_param_628: R.Tensor((640, 2560), dtype="float32"), transformed_param_629: R.Tensor((2560, 640), dtype="float32"), transformed_param_630: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_631: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_632: R.Tensor((1280, 640), dtype="float32"), transformed_param_633: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_634: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_635: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_636: R.Tensor((640, 640), dtype="float32"), transformed_param_637: R.Tensor((640, 640), dtype="float32"), transformed_param_638: R.Tensor((640, 640), dtype="float32"), transformed_param_639: R.Tensor((640, 640), dtype="float32"), transformed_param_640: R.Tensor((640, 640), dtype="float32"), transformed_param_641: R.Tensor((768, 640), dtype="float32"), transformed_param_642: R.Tensor((768, 640), dtype="float32"), transformed_param_643: R.Tensor((640, 640), dtype="float32"), transformed_param_644: R.Tensor((640, 2560), dtype="float32"), transformed_param_645: R.Tensor((640, 2560), dtype="float32"), transformed_param_646: R.Tensor((2560, 640), dtype="float32"), transformed_param_647: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_648: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_649: R.Tensor((1280, 640), dtype="float32"), transformed_param_650: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_651: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_652: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_653: R.Tensor((640, 640), dtype="float32"), transformed_param_654: R.Tensor((640, 640), dtype="float32"), transformed_param_655: R.Tensor((640, 640), dtype="float32"), transformed_param_656: R.Tensor((640, 640), dtype="float32"), transformed_param_657: R.Tensor((640, 640), dtype="float32"), transformed_param_658: R.Tensor((768, 640), dtype="float32"), transformed_param_659: R.Tensor((768, 640), dtype="float32"), transformed_param_660: R.Tensor((640, 640), dtype="float32"), transformed_param_661: R.Tensor((640, 2560), dtype="float32"), transformed_param_662: R.Tensor((640, 2560), dtype="float32"), transformed_param_663: R.Tensor((2560, 640), dtype="float32"), transformed_param_664: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_665: R.Tensor((1, 640, 1, 1), dtype="float32"), transformed_param_666: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_667: R.Tensor((1280, 320), dtype="float32"), transformed_param_668: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_669: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_670: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_671: R.Tensor((320, 320), dtype="float32"), transformed_param_672: R.Tensor((320, 320), dtype="float32"), transformed_param_673: R.Tensor((320, 320), dtype="float32"), transformed_param_674: R.Tensor((320, 320), dtype="float32"), transformed_param_675: R.Tensor((320, 320), dtype="float32"), transformed_param_676: R.Tensor((768, 320), dtype="float32"), transformed_param_677: R.Tensor((768, 320), dtype="float32"), transformed_param_678: R.Tensor((320, 320), dtype="float32"), transformed_param_679: R.Tensor((320, 1280), dtype="float32"), transformed_param_680: R.Tensor((320, 1280), dtype="float32"), transformed_param_681: R.Tensor((1280, 320), dtype="float32"), transformed_param_682: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_683: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_684: R.Tensor((1280, 320), dtype="float32"), transformed_param_685: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_686: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_687: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_688: R.Tensor((320, 320), dtype="float32"), transformed_param_689: R.Tensor((320, 320), dtype="float32"), transformed_param_690: R.Tensor((320, 320), dtype="float32"), transformed_param_691: R.Tensor((320, 320), dtype="float32"), transformed_param_692: R.Tensor((320, 320), dtype="float32"), transformed_param_693: R.Tensor((768, 320), dtype="float32"), transformed_param_694: R.Tensor((768, 320), dtype="float32"), transformed_param_695: R.Tensor((320, 320), dtype="float32"), transformed_param_696: R.Tensor((320, 1280), dtype="float32"), transformed_param_697: R.Tensor((320, 1280), dtype="float32"), transformed_param_698: R.Tensor((1280, 320), dtype="float32"), transformed_param_699: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_700: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_701: R.Tensor((1280, 320), dtype="float32"), transformed_param_702: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_703: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_704: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_705: R.Tensor((320, 320), dtype="float32"), transformed_param_706: R.Tensor((320, 320), dtype="float32"), transformed_param_707: R.Tensor((320, 320), dtype="float32"), transformed_param_708: R.Tensor((320, 320), dtype="float32"), transformed_param_709: R.Tensor((320, 320), dtype="float32"), transformed_param_710: R.Tensor((768, 320), dtype="float32"), transformed_param_711: R.Tensor((768, 320), dtype="float32"), transformed_param_712: R.Tensor((320, 320), dtype="float32"), transformed_param_713: R.Tensor((320, 1280), dtype="float32"), transformed_param_714: R.Tensor((320, 1280), dtype="float32"), transformed_param_715: R.Tensor((1280, 320), dtype="float32"), transformed_param_716: R.Tensor((1, 320, 1, 1), dtype="float32"), transformed_param_717: R.Tensor((1, 4, 1, 1), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        R.func_attr({"global_symbol": "main", "num_input": 3})
        cls = Module
        with R.dataflow():
            lv = R.call_tir(cls.concatenate1, (inp_0, inp_0), out_sinfo=R.Tensor((2, 4, 64, 64), dtype="float32"))
            lv77 = R.call_tir(cls.fused_broadcast_to1_strided_slice_reshape20_cast3_multiply12_multiply13_tir_sin_tir_cos_concatenate2_strided_slice1_reshape21_strided_slice2_reshape21_concatenate2, (inp_1, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv336: R.Tensor((320, 1280), dtype="float32") = transformed_param_420
            lv337: R.Tensor((1280,), dtype="float32") = transformed_param_184
            lv78 = R.call_tir(cls.fused_matmul8_add20_silu6, (lv77, lv336, lv337), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv338: R.Tensor((1280, 1280), dtype="float32") = transformed_param_421
            lv339: R.Tensor((1280,), dtype="float32") = transformed_param_185
            lv79 = R.call_tir(cls.fused_matmul9_add20, (lv78, lv338, lv339), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv340: R.Tensor((320, 4, 3, 3), dtype="float32") = transformed_param_0
            lv341: R.Tensor((1, 320, 1, 1), dtype="float32") = transformed_param_422
            lv80 = R.call_tir(cls.fused_conv2d13_add21, (lv, lv340, lv341), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv342: R.Tensor((320,), dtype="float32") = transformed_param_38
            lv343: R.Tensor((320,), dtype="float32") = transformed_param_37
            lv81 = R.call_tir(cls.fused_group_norm7_silu7, (lv80, lv342, lv343), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv30 = R.call_tir(cls.silu6, (lv79,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv344: R.Tensor((1280, 320), dtype="float32") = transformed_param_424
            lv345: R.Tensor((320,), dtype="float32") = transformed_param_41
            lv82 = R.call_tir(cls.fused_matmul10_add22_strided_slice3, (lv30, lv344, lv345), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv35 = R.call_tir(cls.reshape23, (lv82,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv346: R.Tensor((320, 320, 3, 3), dtype="float32") = transformed_param_35
            lv347: R.Tensor((1, 320, 1, 1), dtype="float32") = transformed_param_423
            lv83 = R.call_tir(cls.fused_conv2d14_add21_add23, (lv81, lv346, lv347, lv35), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv348: R.Tensor((320,), dtype="float32") = transformed_param_40
            lv349: R.Tensor((320,), dtype="float32") = transformed_param_39
            lv84 = R.call_tir(cls.fused_group_norm7_silu7, (lv83, lv348, lv349), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv350: R.Tensor((320, 320, 3, 3), dtype="float32") = transformed_param_36
            lv351: R.Tensor((1, 320, 1, 1), dtype="float32") = transformed_param_425
            lv85 = R.call_tir(cls.fused_conv2d14_add21_add24_divide7, (lv84, lv350, lv351, lv80), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            gv: R.Tensor((2, 320, 64, 64), dtype="float32") = lv85
            R.output(gv)
        return gv

    @R.function
    def vae(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), transformed_param_0: R.Tensor((512, 4, 3, 3), dtype="float32"), transformed_param_1: R.Tensor((128,), dtype="float32"), transformed_param_2: R.Tensor((128,), dtype="float32"), transformed_param_3: R.Tensor((3, 128, 3, 3), dtype="float32"), transformed_param_4: R.Tensor((512,), dtype="float32"), transformed_param_5: R.Tensor((512,), dtype="float32"), transformed_param_6: R.Tensor((512,), dtype="float32"), transformed_param_7: R.Tensor((512,), dtype="float32"), transformed_param_8: R.Tensor((512,), dtype="float32"), transformed_param_9: R.Tensor((512,), dtype="float32"), transformed_param_10: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_11: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_12: R.Tensor((512,), dtype="float32"), transformed_param_13: R.Tensor((512,), dtype="float32"), transformed_param_14: R.Tensor((512,), dtype="float32"), transformed_param_15: R.Tensor((512,), dtype="float32"), transformed_param_16: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_17: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_18: R.Tensor((512,), dtype="float32"), transformed_param_19: R.Tensor((512,), dtype="float32"), transformed_param_20: R.Tensor((512,), dtype="float32"), transformed_param_21: R.Tensor((512,), dtype="float32"), transformed_param_22: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_23: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_24: R.Tensor((512,), dtype="float32"), transformed_param_25: R.Tensor((512,), dtype="float32"), transformed_param_26: R.Tensor((512,), dtype="float32"), transformed_param_27: R.Tensor((512,), dtype="float32"), transformed_param_28: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_29: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_30: R.Tensor((512,), dtype="float32"), transformed_param_31: R.Tensor((512,), dtype="float32"), transformed_param_32: R.Tensor((512,), dtype="float32"), transformed_param_33: R.Tensor((512,), dtype="float32"), transformed_param_34: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_35: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_36: R.Tensor((512,), dtype="float32"), transformed_param_37: R.Tensor((512,), dtype="float32"), transformed_param_38: R.Tensor((512,), dtype="float32"), transformed_param_39: R.Tensor((512,), dtype="float32"), transformed_param_40: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_41: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_42: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_43: R.Tensor((512,), dtype="float32"), transformed_param_44: R.Tensor((512,), dtype="float32"), transformed_param_45: R.Tensor((512,), dtype="float32"), transformed_param_46: R.Tensor((512,), dtype="float32"), transformed_param_47: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_48: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_49: R.Tensor((512,), dtype="float32"), transformed_param_50: R.Tensor((512,), dtype="float32"), transformed_param_51: R.Tensor((512,), dtype="float32"), transformed_param_52: R.Tensor((512,), dtype="float32"), transformed_param_53: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_54: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_55: R.Tensor((512,), dtype="float32"), transformed_param_56: R.Tensor((512,), dtype="float32"), transformed_param_57: R.Tensor((512,), dtype="float32"), transformed_param_58: R.Tensor((512,), dtype="float32"), transformed_param_59: R.Tensor((512, 512, 3, 3), dtype="float32"), transformed_param_60: R.Tensor((256, 512, 3, 3), dtype="float32"), transformed_param_61: R.Tensor((256, 256, 3, 3), dtype="float32"), transformed_param_62: R.Tensor((256, 512, 1, 1), dtype="float32"), transformed_param_63: R.Tensor((512,), dtype="float32"), transformed_param_64: R.Tensor((512,), dtype="float32"), transformed_param_65: R.Tensor((256,), dtype="float32"), transformed_param_66: R.Tensor((256,), dtype="float32"), transformed_param_67: R.Tensor((256, 256, 3, 3), dtype="float32"), transformed_param_68: R.Tensor((256, 256, 3, 3), dtype="float32"), transformed_param_69: R.Tensor((256,), dtype="float32"), transformed_param_70: R.Tensor((256,), dtype="float32"), transformed_param_71: R.Tensor((256,), dtype="float32"), transformed_param_72: R.Tensor((256,), dtype="float32"), transformed_param_73: R.Tensor((256, 256, 3, 3), dtype="float32"), transformed_param_74: R.Tensor((256, 256, 3, 3), dtype="float32"), transformed_param_75: R.Tensor((256,), dtype="float32"), transformed_param_76: R.Tensor((256,), dtype="float32"), transformed_param_77: R.Tensor((256,), dtype="float32"), transformed_param_78: R.Tensor((256,), dtype="float32"), transformed_param_79: R.Tensor((256, 256, 3, 3), dtype="float32"), transformed_param_80: R.Tensor((128, 256, 3, 3), dtype="float32"), transformed_param_81: R.Tensor((128, 128, 3, 3), dtype="float32"), transformed_param_82: R.Tensor((128, 256, 1, 1), dtype="float32"), transformed_param_83: R.Tensor((256,), dtype="float32"), transformed_param_84: R.Tensor((256,), dtype="float32"), transformed_param_85: R.Tensor((128,), dtype="float32"), transformed_param_86: R.Tensor((128,), dtype="float32"), transformed_param_87: R.Tensor((128, 128, 3, 3), dtype="float32"), transformed_param_88: R.Tensor((128, 128, 3, 3), dtype="float32"), transformed_param_89: R.Tensor((128,), dtype="float32"), transformed_param_90: R.Tensor((128,), dtype="float32"), transformed_param_91: R.Tensor((128,), dtype="float32"), transformed_param_92: R.Tensor((128,), dtype="float32"), transformed_param_93: R.Tensor((128, 128, 3, 3), dtype="float32"), transformed_param_94: R.Tensor((128, 128, 3, 3), dtype="float32"), transformed_param_95: R.Tensor((128,), dtype="float32"), transformed_param_96: R.Tensor((128,), dtype="float32"), transformed_param_97: R.Tensor((128,), dtype="float32"), transformed_param_98: R.Tensor((128,), dtype="float32"), transformed_param_99: R.Tensor((4, 4, 1, 1), dtype="float32"), transformed_param_100: R.Tensor((1, 4, 1, 1), dtype="float32"), transformed_param_101: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_102: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_103: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_104: R.Tensor((512, 512), dtype="float32"), transformed_param_105: R.Tensor((512, 512), dtype="float32"), transformed_param_106: R.Tensor((512, 512), dtype="float32"), transformed_param_107: R.Tensor((512, 512), dtype="float32"), transformed_param_108: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_109: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_110: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_111: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_112: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_113: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_114: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_115: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_116: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_117: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_118: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_119: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_120: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_121: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_122: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_123: R.Tensor((1, 512, 1, 1), dtype="float32"), transformed_param_124: R.Tensor((1, 256, 1, 1), dtype="float32"), transformed_param_125: R.Tensor((1, 256, 1, 1), dtype="float32"), transformed_param_126: R.Tensor((1, 256, 1, 1), dtype="float32"), transformed_param_127: R.Tensor((1, 256, 1, 1), dtype="float32"), transformed_param_128: R.Tensor((1, 256, 1, 1), dtype="float32"), transformed_param_129: R.Tensor((1, 256, 1, 1), dtype="float32"), transformed_param_130: R.Tensor((1, 256, 1, 1), dtype="float32"), transformed_param_131: R.Tensor((1, 256, 1, 1), dtype="float32"), transformed_param_132: R.Tensor((1, 128, 1, 1), dtype="float32"), transformed_param_133: R.Tensor((1, 128, 1, 1), dtype="float32"), transformed_param_134: R.Tensor((1, 128, 1, 1), dtype="float32"), transformed_param_135: R.Tensor((1, 128, 1, 1), dtype="float32"), transformed_param_136: R.Tensor((1, 128, 1, 1), dtype="float32"), transformed_param_137: R.Tensor((1, 128, 1, 1), dtype="float32"), transformed_param_138: R.Tensor((1, 128, 1, 1), dtype="float32"), transformed_param_139: R.Tensor((1, 3, 1, 1), dtype="float32")) -> R.Tensor((1, 512, 512, 3), dtype="float32"):
        R.func_attr({"global_symbol": "main", "num_input": 1})
        cls = Module
        with R.dataflow():
            lv = R.call_tir(cls.multiply6, (inp_0,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            lv196: R.Tensor((4, 4, 1, 1), dtype="float32") = transformed_param_99
            lv197: R.Tensor((1, 4, 1, 1), dtype="float32") = transformed_param_100
            lv_1 = R.call_tir(cls.fused_conv2d_add1, (lv, lv196, lv197), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            lv198: R.Tensor((512, 4, 3, 3), dtype="float32") = transformed_param_0
            lv199: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_101
            lv1 = R.call_tir(cls.fused_conv2d1_add2, (lv_1, lv198, lv199), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv200: R.Tensor((512,), dtype="float32") = transformed_param_13
            lv201: R.Tensor((512,), dtype="float32") = transformed_param_12
            lv2 = R.call_tir(cls.fused_group_norm_silu, (lv1, lv200, lv201), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv202: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_10
            lv203: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_102
            lv3 = R.call_tir(cls.fused_conv2d2_add2, (lv2, lv202, lv203), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv204: R.Tensor((512,), dtype="float32") = transformed_param_15
            lv205: R.Tensor((512,), dtype="float32") = transformed_param_14
            lv4 = R.call_tir(cls.fused_group_norm_silu, (lv3, lv204, lv205), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv206: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_11
            lv207: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_103
            lv5 = R.call_tir(cls.fused_conv2d2_add2_add3_divide1, (lv4, lv206, lv207, lv1), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv6 = R.call_tir(cls.fused_reshape2_transpose_transpose1, (lv5,), out_sinfo=R.Tensor((1, 512, 4096), dtype="float32"))
            lv208: R.Tensor((512,), dtype="float32") = transformed_param_5
            lv209: R.Tensor((512,), dtype="float32") = transformed_param_4
            lv22 = R.call_tir(cls.group_norm1, (lv6, lv208, lv209), out_sinfo=R.Tensor((1, 512, 4096), dtype="float32"))
            lv23 = R.call_tir(cls.transpose, (lv22,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv210: R.Tensor((512, 512), dtype="float32") = transformed_param_104
            lv211: R.Tensor((512,), dtype="float32") = transformed_param_8
            lv7 = R.call_tir(cls.fused_matmul_add4, (lv23, lv210, lv211), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv212: R.Tensor((512, 512), dtype="float32") = transformed_param_105
            lv213: R.Tensor((512,), dtype="float32") = transformed_param_6
            lv8 = R.call_tir(cls.fused_matmul_add4, (lv23, lv212, lv213), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv214: R.Tensor((512, 512), dtype="float32") = transformed_param_106
            lv215: R.Tensor((512,), dtype="float32") = transformed_param_9
            lv9 = R.call_tir(cls.fused_matmul_add4, (lv23, lv214, lv215), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv10 = R.call_tir(cls.fused_reshape3_transpose3, (lv7,), out_sinfo=R.Tensor((1, 1, 4096, 512), dtype="float32"))
            lv11 = R.call_tir(cls.fused_reshape3_transpose3_transpose4, (lv8,), out_sinfo=R.Tensor((1, 1, 512, 4096), dtype="float32"))
            lv12 = R.call_tir(cls.fused_reshape3_transpose3, (lv9,), out_sinfo=R.Tensor((1, 1, 4096, 512), dtype="float32"))
            lv13 = R.call_tir(cls.fused_matmul1_multiply7, (lv10, lv11, R.const(0.044194173067808151, "float32")), out_sinfo=R.Tensor((1, 1, 4096, 4096), dtype="float32"))
            lv44 = R.call_tir(cls.softmax, (lv13,), out_sinfo=R.Tensor((1, 1, 4096, 4096), dtype="float32"))
            lv45 = R.call_tir(cls.matmul2, (lv44, lv12), out_sinfo=R.Tensor((1, 1, 4096, 512), dtype="float32"))
            lv14 = R.call_tir(cls.fused_transpose5_reshape4, (lv45,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv216: R.Tensor((512, 512), dtype="float32") = transformed_param_107
            lv217: R.Tensor((512,), dtype="float32") = transformed_param_7
            lv15 = R.call_tir(cls.fused_matmul_add4, (lv14, lv216, lv217), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv16 = R.call_tir(cls.fused_transpose1_reshape5_add3_divide1, (lv15, lv5), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv218: R.Tensor((512,), dtype="float32") = transformed_param_19
            lv219: R.Tensor((512,), dtype="float32") = transformed_param_18
            lv17 = R.call_tir(cls.fused_group_norm_silu, (lv16, lv218, lv219), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv220: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_16
            lv221: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_108
            lv18 = R.call_tir(cls.fused_conv2d2_add2, (lv17, lv220, lv221), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv222: R.Tensor((512,), dtype="float32") = transformed_param_21
            lv223: R.Tensor((512,), dtype="float32") = transformed_param_20
            lv19 = R.call_tir(cls.fused_group_norm_silu, (lv18, lv222, lv223), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv224: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_17
            lv225: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_109
            lv20 = R.call_tir(cls.fused_conv2d2_add2_add3_divide1_divide1, (lv19, lv224, lv225, lv16), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv226: R.Tensor((512,), dtype="float32") = transformed_param_25
            lv227: R.Tensor((512,), dtype="float32") = transformed_param_24
            lv21 = R.call_tir(cls.fused_group_norm_silu, (lv20, lv226, lv227), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv228: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_22
            lv229: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_110
            lv22_1 = R.call_tir(cls.fused_conv2d2_add2, (lv21, lv228, lv229), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv230: R.Tensor((512,), dtype="float32") = transformed_param_27
            lv231: R.Tensor((512,), dtype="float32") = transformed_param_26
            lv23_1 = R.call_tir(cls.fused_group_norm_silu, (lv22_1, lv230, lv231), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv232: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_23
            lv233: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_111
            lv24 = R.call_tir(cls.fused_conv2d2_add2_add3_divide1, (lv23_1, lv232, lv233, lv20), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv234: R.Tensor((512,), dtype="float32") = transformed_param_31
            lv235: R.Tensor((512,), dtype="float32") = transformed_param_30
            lv25 = R.call_tir(cls.fused_group_norm_silu, (lv24, lv234, lv235), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv236: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_28
            lv237: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_112
            lv26 = R.call_tir(cls.fused_conv2d2_add2, (lv25, lv236, lv237), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv238: R.Tensor((512,), dtype="float32") = transformed_param_33
            lv239: R.Tensor((512,), dtype="float32") = transformed_param_32
            lv27 = R.call_tir(cls.fused_group_norm_silu, (lv26, lv238, lv239), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv240: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_29
            lv241: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_113
            lv28 = R.call_tir(cls.fused_conv2d2_add2_add3_divide1, (lv27, lv240, lv241, lv24), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv242: R.Tensor((512,), dtype="float32") = transformed_param_37
            lv243: R.Tensor((512,), dtype="float32") = transformed_param_36
            lv29 = R.call_tir(cls.fused_group_norm_silu, (lv28, lv242, lv243), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv244: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_34
            lv245: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_114
            lv30 = R.call_tir(cls.fused_conv2d2_add2, (lv29, lv244, lv245), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv246: R.Tensor((512,), dtype="float32") = transformed_param_39
            lv247: R.Tensor((512,), dtype="float32") = transformed_param_38
            lv31 = R.call_tir(cls.fused_group_norm_silu, (lv30, lv246, lv247), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv248: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_35
            lv249: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_115
            lv32 = R.call_tir(cls.fused_conv2d2_add2_add3_divide1, (lv31, lv248, lv249, lv28), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv104 = R.call_tir(cls.resize2d, (lv32,), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv250: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_40
            lv251: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_116
            lv33 = R.call_tir(cls.fused_conv2d3_add5, (lv104, lv250, lv251), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv252: R.Tensor((512,), dtype="float32") = transformed_param_44
            lv253: R.Tensor((512,), dtype="float32") = transformed_param_43
            lv34 = R.call_tir(cls.fused_group_norm2_silu1, (lv33, lv252, lv253), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv254: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_41
            lv255: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_117
            lv35 = R.call_tir(cls.fused_conv2d3_add5, (lv34, lv254, lv255), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv256: R.Tensor((512,), dtype="float32") = transformed_param_46
            lv257: R.Tensor((512,), dtype="float32") = transformed_param_45
            lv36 = R.call_tir(cls.fused_group_norm2_silu1, (lv35, lv256, lv257), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv258: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_42
            lv259: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_118
            lv37 = R.call_tir(cls.fused_conv2d3_add5_add6_divide3, (lv36, lv258, lv259, lv33), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv260: R.Tensor((512,), dtype="float32") = transformed_param_50
            lv261: R.Tensor((512,), dtype="float32") = transformed_param_49
            lv38 = R.call_tir(cls.fused_group_norm2_silu1, (lv37, lv260, lv261), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv262: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_47
            lv263: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_119
            lv39 = R.call_tir(cls.fused_conv2d3_add5, (lv38, lv262, lv263), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv264: R.Tensor((512,), dtype="float32") = transformed_param_52
            lv265: R.Tensor((512,), dtype="float32") = transformed_param_51
            lv40 = R.call_tir(cls.fused_group_norm2_silu1, (lv39, lv264, lv265), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv266: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_48
            lv267: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_120
            lv41 = R.call_tir(cls.fused_conv2d3_add5_add6_divide3, (lv40, lv266, lv267, lv37), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv268: R.Tensor((512,), dtype="float32") = transformed_param_56
            lv269: R.Tensor((512,), dtype="float32") = transformed_param_55
            lv42 = R.call_tir(cls.fused_group_norm2_silu1, (lv41, lv268, lv269), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv270: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_53
            lv271: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_121
            lv43 = R.call_tir(cls.fused_conv2d3_add5, (lv42, lv270, lv271), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv272: R.Tensor((512,), dtype="float32") = transformed_param_58
            lv273: R.Tensor((512,), dtype="float32") = transformed_param_57
            lv44_1 = R.call_tir(cls.fused_group_norm2_silu1, (lv43, lv272, lv273), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv274: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_54
            lv275: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_122
            lv45_1 = R.call_tir(cls.fused_conv2d3_add5_add6_divide3, (lv44_1, lv274, lv275, lv41), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv144 = R.call_tir(cls.resize2d1, (lv45_1,), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv276: R.Tensor((512, 512, 3, 3), dtype="float32") = transformed_param_59
            lv277: R.Tensor((1, 512, 1, 1), dtype="float32") = transformed_param_123
            lv46 = R.call_tir(cls.fused_conv2d4_add7, (lv144, lv276, lv277), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv278: R.Tensor((512,), dtype="float32") = transformed_param_64
            lv279: R.Tensor((512,), dtype="float32") = transformed_param_63
            lv47 = R.call_tir(cls.fused_group_norm3_silu2, (lv46, lv278, lv279), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv280: R.Tensor((256, 512, 3, 3), dtype="float32") = transformed_param_60
            lv281: R.Tensor((1, 256, 1, 1), dtype="float32") = transformed_param_124
            lv48 = R.call_tir(cls.fused_conv2d5_add8, (lv47, lv280, lv281), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv282: R.Tensor((256,), dtype="float32") = transformed_param_66
            lv283: R.Tensor((256,), dtype="float32") = transformed_param_65
            lv49 = R.call_tir(cls.fused_group_norm4_silu3, (lv48, lv282, lv283), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv284: R.Tensor((256, 512, 1, 1), dtype="float32") = transformed_param_62
            lv285: R.Tensor((1, 256, 1, 1), dtype="float32") = transformed_param_126
            lv50 = R.call_tir(cls.fused_conv2d7_add8, (lv46, lv284, lv285), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv286: R.Tensor((256, 256, 3, 3), dtype="float32") = transformed_param_61
            lv287: R.Tensor((1, 256, 1, 1), dtype="float32") = transformed_param_125
            lv51 = R.call_tir(cls.fused_conv2d6_add8_add9_divide4, (lv49, lv286, lv287, lv50), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv288: R.Tensor((256,), dtype="float32") = transformed_param_70
            lv289: R.Tensor((256,), dtype="float32") = transformed_param_69
            lv52 = R.call_tir(cls.fused_group_norm4_silu3, (lv51, lv288, lv289), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv290: R.Tensor((256, 256, 3, 3), dtype="float32") = transformed_param_67
            lv291: R.Tensor((1, 256, 1, 1), dtype="float32") = transformed_param_127
            lv53 = R.call_tir(cls.fused_conv2d6_add8, (lv52, lv290, lv291), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv292: R.Tensor((256,), dtype="float32") = transformed_param_72
            lv293: R.Tensor((256,), dtype="float32") = transformed_param_71
            lv54 = R.call_tir(cls.fused_group_norm4_silu3, (lv53, lv292, lv293), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv294: R.Tensor((256, 256, 3, 3), dtype="float32") = transformed_param_68
            lv295: R.Tensor((1, 256, 1, 1), dtype="float32") = transformed_param_128
            lv55 = R.call_tir(cls.fused_conv2d6_add8_add9_divide4, (lv54, lv294, lv295, lv51), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv296: R.Tensor((256,), dtype="float32") = transformed_param_76
            lv297: R.Tensor((256,), dtype="float32") = transformed_param_75
            lv56 = R.call_tir(cls.fused_group_norm4_silu3, (lv55, lv296, lv297), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv298: R.Tensor((256, 256, 3, 3), dtype="float32") = transformed_param_73
            lv299: R.Tensor((1, 256, 1, 1), dtype="float32") = transformed_param_129
            lv57 = R.call_tir(cls.fused_conv2d6_add8, (lv56, lv298, lv299), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv300: R.Tensor((256,), dtype="float32") = transformed_param_78
            lv301: R.Tensor((256,), dtype="float32") = transformed_param_77
            lv58 = R.call_tir(cls.fused_group_norm4_silu3, (lv57, lv300, lv301), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv302: R.Tensor((256, 256, 3, 3), dtype="float32") = transformed_param_74
            lv303: R.Tensor((1, 256, 1, 1), dtype="float32") = transformed_param_130
            lv59 = R.call_tir(cls.fused_conv2d6_add8_add9_divide4, (lv58, lv302, lv303, lv55), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv187 = R.call_tir(cls.resize2d2, (lv59,), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv304: R.Tensor((256, 256, 3, 3), dtype="float32") = transformed_param_79
            lv305: R.Tensor((1, 256, 1, 1), dtype="float32") = transformed_param_131
            lv60 = R.call_tir(cls.fused_conv2d8_add10, (lv187, lv304, lv305), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv306: R.Tensor((256,), dtype="float32") = transformed_param_84
            lv307: R.Tensor((256,), dtype="float32") = transformed_param_83
            lv61 = R.call_tir(cls.fused_group_norm5_silu4, (lv60, lv306, lv307), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv308: R.Tensor((128, 256, 3, 3), dtype="float32") = transformed_param_80
            lv309: R.Tensor((1, 128, 1, 1), dtype="float32") = transformed_param_132
            lv62 = R.call_tir(cls.fused_conv2d9_add11, (lv61, lv308, lv309), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv310: R.Tensor((128,), dtype="float32") = transformed_param_86
            lv311: R.Tensor((128,), dtype="float32") = transformed_param_85
            lv63 = R.call_tir(cls.fused_group_norm6_silu5, (lv62, lv310, lv311), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv312: R.Tensor((128, 256, 1, 1), dtype="float32") = transformed_param_82
            lv313: R.Tensor((1, 128, 1, 1), dtype="float32") = transformed_param_134
            lv64 = R.call_tir(cls.fused_conv2d11_add11, (lv60, lv312, lv313), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv314: R.Tensor((128, 128, 3, 3), dtype="float32") = transformed_param_81
            lv315: R.Tensor((1, 128, 1, 1), dtype="float32") = transformed_param_133
            lv65 = R.call_tir(cls.fused_conv2d10_add11_add12_divide5, (lv63, lv314, lv315, lv64), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv316: R.Tensor((128,), dtype="float32") = transformed_param_90
            lv317: R.Tensor((128,), dtype="float32") = transformed_param_89
            lv66 = R.call_tir(cls.fused_group_norm6_silu5, (lv65, lv316, lv317), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv318: R.Tensor((128, 128, 3, 3), dtype="float32") = transformed_param_87
            lv319: R.Tensor((1, 128, 1, 1), dtype="float32") = transformed_param_135
            lv67 = R.call_tir(cls.fused_conv2d10_add11, (lv66, lv318, lv319), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv320: R.Tensor((128,), dtype="float32") = transformed_param_92
            lv321: R.Tensor((128,), dtype="float32") = transformed_param_91
            lv68 = R.call_tir(cls.fused_group_norm6_silu5, (lv67, lv320, lv321), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv322: R.Tensor((128, 128, 3, 3), dtype="float32") = transformed_param_88
            lv323: R.Tensor((1, 128, 1, 1), dtype="float32") = transformed_param_136
            lv69 = R.call_tir(cls.fused_conv2d10_add11_add12_divide5, (lv68, lv322, lv323, lv65), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv324: R.Tensor((128,), dtype="float32") = transformed_param_96
            lv325: R.Tensor((128,), dtype="float32") = transformed_param_95
            lv70 = R.call_tir(cls.fused_group_norm6_silu5, (lv69, lv324, lv325), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv326: R.Tensor((128, 128, 3, 3), dtype="float32") = transformed_param_93
            lv327: R.Tensor((1, 128, 1, 1), dtype="float32") = transformed_param_137
            lv71 = R.call_tir(cls.fused_conv2d10_add11, (lv70, lv326, lv327), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv328: R.Tensor((128,), dtype="float32") = transformed_param_98
            lv329: R.Tensor((128,), dtype="float32") = transformed_param_97
            lv72 = R.call_tir(cls.fused_group_norm6_silu5, (lv71, lv328, lv329), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv330: R.Tensor((128, 128, 3, 3), dtype="float32") = transformed_param_94
            lv331: R.Tensor((1, 128, 1, 1), dtype="float32") = transformed_param_138
            lv73 = R.call_tir(cls.fused_conv2d10_add11_add12_divide5, (lv72, lv330, lv331, lv69), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv332: R.Tensor((128,), dtype="float32") = transformed_param_2
            lv333: R.Tensor((128,), dtype="float32") = transformed_param_1
            lv74 = R.call_tir(cls.fused_group_norm6_silu5, (lv73, lv332, lv333), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv334: R.Tensor((3, 128, 3, 3), dtype="float32") = transformed_param_3
            lv335: R.Tensor((1, 3, 1, 1), dtype="float32") = transformed_param_139
            lv75 = R.call_tir(cls.fused_conv2d12_add13_divide6_add14_tir_clip, (lv74, lv334, lv335), out_sinfo=R.Tensor((1, 3, 512, 512), dtype="float32"))
            lv76 = R.call_tir(cls.fused_transpose6_multiply8_tir_round, (lv75,), out_sinfo=R.Tensor((1, 512, 512, 3), dtype="float32"))
            gv: R.Tensor((1, 512, 512, 3), dtype="float32") = lv76
            R.output(gv)
        return gv