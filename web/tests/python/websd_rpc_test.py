# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
"""Test relax vm through rpc."""

import tvm
import numpy as np
from tvm import rpc, relax
from tvm.contrib import utils, tvmjs
from tvm.script import relax as R

proxy_host = "127.0.0.1"
proxy_port = 9090


metadata = tvm.ir.load_json("""{
  \"root\": 1, 
  \"nodes\": [
    {
      \"type_key\": \"\"
    }, 
    {
      \"type_key\": \"Map\", 
      \"keys\": [
        \"relax.expr.Constant\"
      ], 
      \"data\": [2]
    }, 
    {
      \"type_key\": \"Array\", 
      \"data\": [3, 14]
    }, 
    {
      \"type_key\": \"relax.expr.Constant\", 
      \"attrs\": {
        \"_checked_type_\": \"13\", 
        \"data\": \"0\", 
        \"span\": \"0\", 
        \"struct_info_\": \"4\"
      }
    }, 
    {
      \"type_key\": \"relax.TensorStructInfo\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"4\", 
        \"shape\": \"5\", 
        \"span\": \"0\", 
        \"vdevice\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.expr.ShapeExpr\", 
      \"attrs\": {
        \"_checked_type_\": \"12\", 
        \"span\": \"0\", 
        \"struct_info_\": \"11\", 
        \"values\": \"6\"
      }
    }, 
    {
      \"type_key\": \"Array\", 
      \"data\": [7, 8, 9, 10]
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"1\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"1\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"77\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"77\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeStructInfo\", 
      \"attrs\": {
        \"ndim\": \"4\", 
        \"span\": \"0\", 
        \"values\": \"6\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeType\", 
      \"attrs\": {
        \"ndim\": \"4\", 
        \"span\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.DynTensorType\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"4\", 
        \"span\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.expr.Constant\", 
      \"attrs\": {
        \"_checked_type_\": \"22\", 
        \"data\": \"1\", 
        \"span\": \"0\", 
        \"struct_info_\": \"15\"
      }
    }, 
    {
      \"type_key\": \"relax.TensorStructInfo\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"2\", 
        \"shape\": \"16\", 
        \"span\": \"0\", 
        \"vdevice\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.expr.ShapeExpr\", 
      \"attrs\": {
        \"_checked_type_\": \"21\", 
        \"span\": \"0\", 
        \"struct_info_\": \"20\", 
        \"values\": \"17\"
      }
    }, 
    {
      \"type_key\": \"Array\", 
      \"data\": [18, 19]
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"1\"
      }
    }, 
    {
      \"type_key\": \"IntImm\", 
      \"attrs\": {
        \"dtype\": \"int64\", 
        \"span\": \"0\", 
        \"value\": \"160\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeStructInfo\", 
      \"attrs\": {
        \"ndim\": \"2\", 
        \"span\": \"0\", 
        \"values\": \"17\"
      }
    }, 
    {
      \"type_key\": \"relax.ShapeType\", 
      \"attrs\": {
        \"ndim\": \"2\", 
        \"span\": \"0\"
      }
    }, 
    {
      \"type_key\": \"relax.DynTensorType\", 
      \"attrs\": {
        \"dtype\": \"float32\", 
        \"ndim\": \"2\", 
        \"span\": \"0\"
      }
    }
  ], 
  \"b64ndarrays\": [
    \"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\", 
    \"P6G0lvBAXt0AAAAAAAAAAAEAAAAAAAAAAgAAAAIgAQABAAAAAAAAAKAAAAAAAAAAgAIAAAAAAAAAAIA/+a1xPwUpZD+rZVc/F1lLPxD5Pz/vOzU/lhgrP2uGIT9QfRg/mfUPPwroBz/OTQA/4EDyPrWz5D6a6Nc+s9TLPsFtwD4YqrU+loCrPprooT4B2pg+G02QPqc6iD7Mm4A+IdRyPrk+ZT7Xa1g+mlBMPrniQD6GGDY+1OgrPgVLIj7qNhk+0aQQPnaNCD746QA+v2fzPRLK5T1n79g9y8zMPfhXwT03h7Y9VFGsPaytoj0MlJk9v/yQPXfgiD1WOIE9svtzPcBVZj1Dc1k9R0lNPYDNQT0p9jY9FLosPZEQIz1o8Rk94FQRPaozCT3hhgE9A5D0PMHh5jxw99k8EsbNPFBDwjxhZbc8ESOtPKxzozz7Tpo8Oa2RPBKHiTyd1YE8piR1PB9uZzzwe1o8JkNOPGK5Qjze1Dc8UowtPAjXIzzGrBo8wgUSPKzaCTyKJAI8rLn1O8f65zvBANs7icDOO8EvwzuYRLg7zPWtO6I6pDvMCps7h16SO3Quijumc4I7DU92O82HaDvchVs7Mj5PO2imQzuctDg7jF8uO3KeJDsMaRs7gbcSO3SCCjvvwgI7v+T2OiYV6TpOC9w6M7zPOlgdxDrlJLk6h8muOoUCpTqBx5s6rRCTOqjWijptEoM63Xp3OteiaToSkVw6bjpQOomURDprlTk6yDMvOtRmJTo0Jhw6GGoTOg4rCzobYgM6PxH4Odcw6jkhF905BLnQOQkMxTk9Bro5T56vOWDLpTkghZw5sMOTOaN/izn0sYM5DKh4OS2/ajmOnV056zdROdODRTlSdzo5CwkwOSMwJjlB5Bw5fx0UOWzUCzkIAgQ5PD/5OOhN6zg/JN44\"
  ], 
  \"attrs\": {\"tvm_version\": \"0.14.dev0\"}
}""")
from tvm.script import ir as I
from tvm.script import tir as T
from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func
    def add32(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3], B[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_add[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] + B[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def concatenate(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_concat"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16384))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16384) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(B[v_ax0 - T.int64(1), v_ax1, v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1) <= v_ax0, B[v_ax0 - T.int64(1), v_ax1, v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate10(A: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), B: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(59136))
                        v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(59136) // T.int64(768))
                        v_ax2 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(768))
                        T.where((ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) * T.int64(256) + ax0_ax1_ax2_fused_2 < T.int64(118272))
                        T.reads(B[v_ax0 - T.int64(1), v_ax1, v_ax2], A[v_ax0, v_ax1, v_ax2])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2])
                        T_concat[v_ax0, v_ax1, v_ax2] = T.if_then_else(T.int64(1) <= v_ax0, B[v_ax0 - T.int64(1), v_ax1, v_ax2], A[v_ax0, v_ax1, v_ax2])

    @T.prim_func
    def concatenate2(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), B: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(163840))
                        v_ax1 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(163840) // T.int64(64))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                        v_ax3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                        T.reads(B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate3(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), B: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate4(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), B: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(15)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(491520))
                        v_ax1 = T.axis.spatial(T.int64(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(491520) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate5(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), B: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(60)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1966080))
                        v_ax1 = T.axis.spatial(T.int64(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1966080) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1280) <= v_ax1, B[v_ax0, v_ax1 - T.int64(1280), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate6(A: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), B: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(640) <= v_ax1, B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate7(A: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), B: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(30)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(983040))
                        v_ax1 = T.axis.spatial(T.int64(960), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(983040) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(640) <= v_ax1, B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate8(A: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(120)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3932160))
                        v_ax1 = T.axis.spatial(T.int64(960), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3932160) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(640) <= v_ax1, B[v_ax0, v_ax1 - T.int64(640), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def concatenate9(A: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), T_concat: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(80)):
                    with T.block("T_concat"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2621440))
                        v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2621440) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(B[v_ax0, v_ax1 - T.int64(320), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(T_concat[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_concat[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(320) <= v_ax1, B[v_ax0, v_ax1 - T.int64(320), v_ax2, v_ax3], A[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def divide(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3], B[()])
                    T.writes(T_divide[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_divide[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] / B[()]

    @T.prim_func
    def divide5(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_divide[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_divide[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] * T.float32(0.5)

    @T.prim_func
    def divide6(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_divide[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_divide[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] * T.float32(0.083333333333333329)

    @T.prim_func
    def fused_broadcast_to_strided_slice_reshape_cast_multiply1_multiply2_tir_sin_tir_cos_concatenate1_strided_slice1_reshape1_strided_slice2_reshape1_concatenate1(inp_1: T.Buffer((), "int32"), param_0: T.Buffer((T.int64(1), T.int64(160)), "float32"), var_T_concat_intermediate: T.Buffer((T.int64(2), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_concat_1"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) // T.int64(320))
                    v_ax1 = T.axis.spatial(T.int64(320), (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) % T.int64(320))
                    T.where(ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 < T.int64(640))
                    T.reads(inp_1[()], param_0[T.int64(0), v_ax1 % T.int64(160) - T.int64(160):v_ax1 % T.int64(160) - T.int64(160) + T.int64(321)])
                    T.writes(var_T_concat_intermediate[v_ax0, v_ax1])
                    var_T_concat_intermediate[v_ax0, v_ax1] = T.if_then_else(T.int64(160) <= v_ax1, T.if_then_else(T.int64(160) <= (v_ax1 - T.int64(160)) % T.int64(160), T.cos(T.Cast("float32", inp_1[()]) * param_0[T.int64(0), (v_ax1 - T.int64(160)) % T.int64(160) - T.int64(160)]), T.sin(T.Cast("float32", inp_1[()]) * param_0[T.int64(0), (v_ax1 - T.int64(160)) % T.int64(160)])), T.if_then_else(T.int64(160) <= v_ax1 % T.int64(160) + T.int64(160), T.cos(T.Cast("float32", inp_1[()]) * param_0[T.int64(0), v_ax1 % T.int64(160) + T.int64(160) - T.int64(160)]), T.sin(T.Cast("float32", inp_1[()]) * param_0[T.int64(0), v_ax1 % T.int64(160) + T.int64(160)])))

    @T.prim_func
    def fused_conv2d10_add17(lv866: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_0_upsamplers_0_conv_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv868: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(18), T.int64(18)), scope="shared")
        unet_up_blocks_0_upsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(15)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(900))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(5) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(900) // T.int64(180))
                                    v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(180) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(18), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1800))
                                    T.reads(lv866[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv866[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.up_blocks.0.upsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(45))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(5) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(45) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(720))
                                        T.reads(unet_up_blocks_0_upsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_0_upsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_0_upsamplers_0_conv_weight_shared[v0, v1, v2, v3] = unet_up_blocks_0_upsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(5), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(5) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_0_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_0_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv868[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv868[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d10_add17_add18(lv538: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_1_conv1_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv540: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv547: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(18), T.int64(18)), scope="shared")
        unet_down_blocks_2_resnets_1_conv1_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40) // T.int64(4))
                                        v3 = T.axis.spatial(T.int64(18), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        T.reads(lv538[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv538[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.down_blocks.2.resnets.1.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(24) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(unet_down_blocks_2_resnets_1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_2_resnets_1_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_2_resnets_1_conv1_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_resnets_1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(8) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) // T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv540[T.int64(0), v1, T.int64(0), T.int64(0)], lv547[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv540[T.int64(0), v1, T.int64(0), T.int64(0)] + lv547[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d10_add17_add19_divide3(lv447: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_conv2_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv449: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv453: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(18), T.int64(18)), scope="shared")
        unet_down_blocks_2_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(32) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(500))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(5) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(500) // T.int64(100))
                                        v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(100) // T.int64(10))
                                        v3 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1000))
                                        T.reads(lv447[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv447[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.down_blocks.2.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(45))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(5) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(45) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(360))
                                        T.reads(unet_down_blocks_2_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_2_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_2_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(3), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(32) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(5) + rc_1 * T.int64(5) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(32) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(lv453[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv449[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv453[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv449[T.int64(0), v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d11_add17(lv433: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(1280), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv452: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), scope="shared")
        unet_down_blocks_2_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(640), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(40) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(5) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(40) // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(640))
                                        v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.reads(lv433[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv433[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("unet.down_blocks.2.resnets.0.conv_shortcut.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) // T.int64(40))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) % T.int64(40))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(unet_down_blocks_2_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                    T.writes(unet_down_blocks_2_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                    unet_down_blocks_2_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(40), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(40) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(5) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(40) // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(40) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(40) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(5) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(40) // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv452[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv452[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d12_add17(lv456: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_attentions_0_proj_in_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv458: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="shared")
        unet_down_blocks_2_attentions_0_proj_in_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(80), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(10) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(160) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(320))
                                    T.reads(lv456[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv456[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.down_blocks.2.attentions.0.proj_in.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(256) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(10))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(10) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_down_blocks_2_attentions_0_proj_in_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_2_attentions_0_proj_in_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_2_attentions_0_proj_in_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_attentions_0_proj_in_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(5), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(10) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv458[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv458[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d12_add17_add19(lv532: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_attentions_0_proj_out_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv534: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv455: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="shared")
        unet_down_blocks_2_attentions_0_proj_out_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(10), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(40), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2048))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2048) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(lv532[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv532[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("unet.down_blocks.2.attentions.0.proj_out.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(unet_down_blocks_2_attentions_0_proj_out_weight[v0, v1, v2, v3])
                                    T.writes(unet_down_blocks_2_attentions_0_proj_out_weight_shared[v0, v1, v2, v3])
                                    unet_down_blocks_2_attentions_0_proj_out_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_attentions_0_proj_out_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(32) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv534[T.int64(0), v1, T.int64(0), T.int64(0)], lv455[v0, v1, v2, v3])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv534[T.int64(0), v1, T.int64(0), T.int64(0)] + lv455[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d13_add23(lv636: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_downsamplers_0_conv_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv638: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(18), T.int64(18)), scope="shared")
        unet_down_blocks_2_downsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1500))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1500) // T.int64(75))
                                        v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(75) // T.int64(15))
                                        v3 = T.axis.spatial(T.int64(18), rx_0 + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(15))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(3000))
                                        T.reads(lv636[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv636[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(15)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("unet.down_blocks.2.downsamplers.0.conv.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(60))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(60) // T.int64(3))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), rx_0)
                                    T.reads(unet_down_blocks_2_downsamplers_0_conv_weight[v0, v1, v2, v3])
                                    T.writes(unet_down_blocks_2_downsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                    unet_down_blocks_2_downsamplers_0_conv_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_downsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(20), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(20) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx], unet_down_blocks_2_downsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx] * unet_down_blocks_2_downsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv638[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv638[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d14_add23_add24(lv641: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), unet_down_blocks_3_resnets_0_conv1_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv643: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv650: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(10), T.int64(10)), scope="shared")
        unet_down_blocks_3_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(2) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(10), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(10), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 < T.int64(128))
                                    T.reads(lv641[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(9) and T.int64(1) <= v3 and v3 < T.int64(9), lv641[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.down_blocks.3.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_down_blocks_3_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_3_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_3_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_down_blocks_3_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(2) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_3_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_3_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv643[T.int64(0), v1, T.int64(0), T.int64(0)], lv650[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv643[T.int64(0), v1, T.int64(0), T.int64(0)] + lv650[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d14_add23_add25_divide4(lv653: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), unet_down_blocks_3_resnets_0_conv2_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv655: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv639: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(10), T.int64(10)), scope="shared")
        unet_down_blocks_3_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(60))
                                        v2 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(60) // T.int64(6))
                                        v3 = T.axis.spatial(T.int64(10), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(6))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(240))
                                        T.reads(lv653[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(9) and T.int64(1) <= v3 and v3 < T.int64(9), lv653[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("unet.down_blocks.3.resnets.0.conv2.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(36))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(36) // T.int64(9))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(unet_down_blocks_3_resnets_0_conv2_weight[v0, v1, v2, v3])
                                    T.writes(unet_down_blocks_3_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                    unet_down_blocks_3_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = unet_down_blocks_3_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_3_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_3_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(lv639[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv655[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv639[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv655[T.int64(0), v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d15_add23(lv697: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), unet_mid_block_attentions_0_proj_in_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv699: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="shared")
        unet_mid_block_attentions_0_proj_in_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(160))
                                        T.reads(lv697[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv697[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.mid_block.attentions.0.proj_in.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(160) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(20))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_mid_block_attentions_0_proj_in_weight[v0, v1, v2, v3])
                                        T.writes(unet_mid_block_attentions_0_proj_in_weight_shared[v0, v1, v2, v3])
                                        unet_mid_block_attentions_0_proj_in_weight_shared[v0, v1, v2, v3] = unet_mid_block_attentions_0_proj_in_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(20), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(20) + rc_1 * T.int64(20) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_mid_block_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_mid_block_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv699[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv699[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d15_add23_add25(lv773: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), unet_mid_block_attentions_0_proj_out_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv775: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv696: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="shared")
        unet_mid_block_attentions_0_proj_out_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.reads(lv773[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv773[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.mid_block.attentions.0.proj_out.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_mid_block_attentions_0_proj_out_weight[v0, v1, v2, v3])
                                        T.writes(unet_mid_block_attentions_0_proj_out_weight_shared[v0, v1, v2, v3])
                                        unet_mid_block_attentions_0_proj_out_weight_shared[v0, v1, v2, v3] = unet_mid_block_attentions_0_proj_out_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(16) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_mid_block_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_mid_block_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv775[T.int64(0), v1, T.int64(0), T.int64(0)], lv696[v0, v1, v2, v3])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv775[T.int64(0), v1, T.int64(0), T.int64(0)] + lv696[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d16_add23_add24(lv799: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32"), unet_up_blocks_0_resnets_0_conv1_weight: T.Buffer((T.int64(1280), T.int64(2560), T.int64(3), T.int64(3)), "float32"), lv801: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv808: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(2560), T.int64(10), T.int64(10)), scope="shared")
        unet_up_blocks_0_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(2560), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(4) + yy_3_init * T.int64(4) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(100))
                                    v2 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(100) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.reads(lv799[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(9) and T.int64(1) <= v3 and v3 < T.int64(9), lv799[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.0.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(72))
                                        v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(72) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_up_blocks_0_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_0_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_0_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_0_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(4) + yy_3 * T.int64(4) + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(2560), rc_0 * T.int64(8) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_0_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_0_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv801[T.int64(0), v1, T.int64(0), T.int64(0)], lv808[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv801[T.int64(0), v1, T.int64(0), T.int64(0)] + lv808[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d17_add23(lv797: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32"), unet_up_blocks_0_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(1280), T.int64(2560), T.int64(1), T.int64(1)), "float32"), lv816: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), scope="shared")
        unet_up_blocks_0_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(2560), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) % T.int64(160) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8))
                                    T.reads(lv797[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv797[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("unet.up_blocks.0.resnets.0.conv_shortcut.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) // T.int64(20))
                                    v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) % T.int64(20))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(unet_up_blocks_0_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                    T.writes(unet_up_blocks_0_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                    unet_up_blocks_0_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_0_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(10), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(2560), rc_0 * T.int64(20) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_0_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_0_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(8), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(8), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv816[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv816[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d18_add17_add18(lv872: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_1_resnets_0_conv1_weight: T.Buffer((T.int64(1280), T.int64(2560), T.int64(3), T.int64(3)), "float32"), lv874: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv881: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(2560), T.int64(18), T.int64(18)), scope="shared")
        unet_up_blocks_1_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(2560), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(8) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160))
                                        v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(160))
                                        v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(18), rx_0 + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1280))
                                        T.reads(lv872[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv872[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.up_blocks.1.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(24) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(384))
                                        T.reads(unet_up_blocks_1_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_1_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_1_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_1_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(8) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(2560), rc_0 * T.int64(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(160) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(160) // T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv874[T.int64(0), v1, T.int64(0), T.int64(0)], lv881[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv874[T.int64(0), v1, T.int64(0), T.int64(0)] + lv881[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d19_add17(lv870: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_1_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(1280), T.int64(2560), T.int64(1), T.int64(1)), "float32"), lv889: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), scope="shared")
        unet_up_blocks_1_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(2560), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(40), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512))
                                        v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1024))
                                        T.reads(lv870[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv870[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.up_blocks.1.resnets.0.conv_shortcut.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(2560), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_up_blocks_1_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_1_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_1_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_1_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(2560), rc_0 * T.int64(64) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv889[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv889[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d1_add1_add3(lv26: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_resnets_0_conv1_weight: T.Buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv28: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv35: T.Buffer((T.int64(2), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(66), T.int64(66)), scope="shared")
        unet_down_blocks_0_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(720))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(720) // T.int64(180))
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(180) // T.int64(10))
                                        v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1440))
                                        T.reads(lv26[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv26[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.down_blocks.0.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(288))
                                        T.reads(unet_down_blocks_0_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_0_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_0_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_down_blocks_0_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(3), T.int64(1), T.int64(4), T.int64(2), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_0_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_0_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv28[T.int64(0), v1, T.int64(0), T.int64(0)], lv35[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv28[T.int64(0), v1, T.int64(0), T.int64(0)] + lv35[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d1_add1_add4_divide1(lv38: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_resnets_0_conv2_weight: T.Buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv40: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv24: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(66), T.int64(66)), scope="shared")
        unet_down_blocks_0_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2560), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(128) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(72))
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(72) // T.int64(18))
                                        v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(288))
                                        T.reads(lv38[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv38[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.down_blocks.0.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(128) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_down_blocks_0_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_0_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_0_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = unet_down_blocks_0_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(128) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_0_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_0_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(128) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(lv24[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv40[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv24[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv40[T.int64(0), v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d20_add17_add18(lv1080: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_1_resnets_2_conv1_weight: T.Buffer((T.int64(1280), T.int64(1920), T.int64(3), T.int64(3)), "float32"), lv1082: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv1089: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1920), T.int64(18), T.int64(18)), scope="shared")
        unet_up_blocks_1_resnets_2_conv1_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1920), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(480), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(18), rx_0 + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(lv1080[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv1080[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.1.resnets.2.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(12))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(12) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(unet_up_blocks_1_resnets_2_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_1_resnets_2_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_1_resnets_2_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_1_resnets_2_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1920), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_2_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_2_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1082[T.int64(0), v1, T.int64(0), T.int64(0)], lv1089[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1082[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1089[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d21_add17(lv1078: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_1_resnets_2_conv_shortcut_weight: T.Buffer((T.int64(1280), T.int64(1920), T.int64(1), T.int64(1)), "float32"), lv1097: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), scope="shared")
        unet_up_blocks_1_resnets_2_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1920), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(10) + ff_3_init * T.int64(5) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(12)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(24) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(128))
                                    v2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(lv1078[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1078[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.up_blocks.1.resnets.2.conv_shortcut.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(24) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(24))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1920))
                                        T.reads(unet_up_blocks_1_resnets_2_conv_shortcut_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_1_resnets_2_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_1_resnets_2_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_1_resnets_2_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(6), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(10) + ff_3 * T.int64(5) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1920), rc_0 * T.int64(24) + rc_1 * T.int64(6) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_resnets_2_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_resnets_2_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(10), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(10) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1097[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1097[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d22_add29(lv1182: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_1_upsamplers_0_conv_weight: T.Buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv1184: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(34), T.int64(34)), scope="shared")
        unet_up_blocks_1_upsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(34), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv1182[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv1182[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.1.upsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(12))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(12) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(unet_up_blocks_1_upsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_1_upsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_1_upsamplers_0_conv_weight_shared[v0, v1, v2, v3] = unet_up_blocks_1_upsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(3), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(4) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_1_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_1_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1184[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1184[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d23_add9_add11(lv1188: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_0_conv1_weight: T.Buffer((T.int64(640), T.int64(1920), T.int64(3), T.int64(3)), "float32"), lv1190: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv1197: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1920), T.int64(34), T.int64(34)), scope="shared")
        unet_up_blocks_2_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(640), T.int64(1920), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(720))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(12) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(720) // T.int64(60))
                                        v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(60) // T.int64(10))
                                        v3 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        T.reads(lv1188[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv1188[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.up_blocks.2.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(108))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(12) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(108) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2160))
                                        T.reads(unet_up_blocks_2_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_2_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_2_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(12), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1920), rc_0 * T.int64(12) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(80) // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1190[T.int64(0), v1, T.int64(0), T.int64(0)], lv1197[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1190[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1197[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d24_add9(lv1186: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(640), T.int64(1920), T.int64(1), T.int64(1)), "float32"), lv1205: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), scope="shared")
        unet_up_blocks_2_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(640), T.int64(1920), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(12) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(96) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(96) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(96) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(lv1186[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv1186[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.up_blocks.2.resnets.0.conv_shortcut.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(12))
                                        v1 = T.axis.spatial(T.int64(1920), rc_0 * T.int64(12) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(12))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_up_blocks_2_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1920), rc_0 * T.int64(12) + rc_1 * T.int64(3) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(640) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(640) // T.int64(32) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1205[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1205[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d25_add9_add11(lv1292: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_1_conv1_weight: T.Buffer((T.int64(640), T.int64(1280), T.int64(3), T.int64(3)), "float32"), lv1294: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv1301: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(34), T.int64(34)), scope="shared")
        unet_up_blocks_2_resnets_1_conv1_weight_shared = T.alloc_buffer((T.int64(640), T.int64(1280), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1024) // T.int64(128))
                                    v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(34), rx_0 + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(lv1292[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv1292[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.2.resnets.1.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(24) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(unet_up_blocks_2_resnets_1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_2_resnets_1_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_2_resnets_1_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_resnets_1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(8) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(128) // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1294[T.int64(0), v1, T.int64(0), T.int64(0)], lv1301[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1294[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1301[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d26_add9(lv1290: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_1_conv_shortcut_weight: T.Buffer((T.int64(640), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv1309: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), scope="shared")
        unet_up_blocks_2_resnets_1_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(640), T.int64(1280), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(lv1290[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv1290[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.up_blocks.2.resnets.1.conv_shortcut.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1280), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_up_blocks_2_resnets_1_conv_shortcut_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_2_resnets_1_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_2_resnets_1_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_resnets_1_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(1280), rc_0 * T.int64(16) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_1_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_1_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1309[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1309[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d27_add9_add11(lv1396: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_2_conv1_weight: T.Buffer((T.int64(640), T.int64(960), T.int64(3), T.int64(3)), "float32"), lv1398: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv1405: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(960), T.int64(34), T.int64(34)), scope="shared")
        unet_up_blocks_2_resnets_2_conv1_weight_shared = T.alloc_buffer((T.int64(640), T.int64(960), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(8) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(408))
                                        v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(3) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(408) // T.int64(136))
                                        v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(136) // T.int64(34))
                                        v3 = T.axis.spatial(T.int64(34), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(34))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(816))
                                        T.reads(lv1396[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv1396[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.up_blocks.2.resnets.2.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(27))
                                        v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(3) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(27) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(432))
                                        T.reads(unet_up_blocks_2_resnets_2_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_2_resnets_2_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_2_resnets_2_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_resnets_2_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(3), T.int64(1), T.int64(3), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(8) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(960), rc_0 * T.int64(3) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_2_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_2_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1398[T.int64(0), v1, T.int64(0), T.int64(0)], lv1405[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1398[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1405[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d28_add9(lv1394: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_2_conv_shortcut_weight: T.Buffer((T.int64(640), T.int64(960), T.int64(1), T.int64(1)), "float32"), lv1413: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), scope="shared")
        unet_up_blocks_2_resnets_2_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(640), T.int64(960), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(5), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(24), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128) // T.int64(4))
                                        v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        T.reads(lv1394[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv1394[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("unet.up_blocks.2.resnets.2.conv_shortcut.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(40))
                                    v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(40))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1600))
                                    T.reads(unet_up_blocks_2_resnets_2_conv_shortcut_weight[v0, v1, v2, v3])
                                    T.writes(unet_up_blocks_2_resnets_2_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                    unet_up_blocks_2_resnets_2_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_resnets_2_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(40), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(960), rc_0 * T.int64(40) + rc_1 * T.int64(40) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_resnets_2_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_resnets_2_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(40) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1413[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1413[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d29_add30(lv1498: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_2_upsamplers_0_conv_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv1500: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(66), T.int64(66)), scope="shared")
        unet_up_blocks_2_upsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(320) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(32) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(24))
                                    v1 = T.axis.spatial(T.int64(640), rc_0)
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(32) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(24) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 < T.int64(48))
                                    T.reads(lv1498[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv1498[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.2.upsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(320) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_up_blocks_2_upsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_2_upsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_2_upsamplers_0_conv_weight_shared[v0, v1, v2, v3] = unet_up_blocks_2_upsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(320) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(32) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_2_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_2_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(320) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(32) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1500[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1500[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d2_add1(lv44: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_attentions_0_proj_in_weight: T.Buffer((T.int64(320), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv46: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="shared")
        unet_down_blocks_0_attentions_0_proj_in_weight_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 < T.int64(64))
                                    T.reads(lv44[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv44[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.down_blocks.0.attentions.0.proj_in.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(640) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(640) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(320))
                                        T.reads(unet_down_blocks_0_attentions_0_proj_in_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_0_attentions_0_proj_in_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_0_attentions_0_proj_in_weight_shared[v0, v1, v2, v3] = unet_down_blocks_0_attentions_0_proj_in_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_0_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_0_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv46[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv46[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d2_add1_add4(lv120: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_attentions_0_proj_out_weight: T.Buffer((T.int64(320), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv122: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv43: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="shared")
        unet_down_blocks_0_attentions_0_proj_out_weight_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2560), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv120[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv120[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.down_blocks.0.attentions.0.proj_out.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(32))
                                        T.reads(unet_down_blocks_0_attentions_0_proj_out_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_0_attentions_0_proj_out_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_0_attentions_0_proj_out_weight_shared[v0, v1, v2, v3] = unet_down_blocks_0_attentions_0_proj_out_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_0_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_0_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv122[T.int64(0), v1, T.int64(0), T.int64(0)], lv43[v0, v1, v2, v3])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv122[T.int64(0), v1, T.int64(0), T.int64(0)] + lv43[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d30_add1_add3(lv1504: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_3_resnets_0_conv1_weight: T.Buffer((T.int64(320), T.int64(960), T.int64(3), T.int64(3)), "float32"), lv1506: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv1513: T.Buffer((T.int64(2), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(960), T.int64(66), T.int64(66)), scope="shared")
        unet_up_blocks_3_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(320), T.int64(960), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(960), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(960), rc_0)
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(34))
                                        v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(34))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(102))
                                        T.reads(lv1504[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv1504[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.3.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(960), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_up_blocks_3_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_3_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_3_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_3_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(960), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_3_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_3_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(256) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(256) // T.int64(128) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1506[T.int64(0), v1, T.int64(0), T.int64(0)], lv1513[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1506[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1513[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d31_add1(lv1502: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_3_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(320), T.int64(960), T.int64(1), T.int64(1)), "float32"), lv1521: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), scope="shared")
        unet_up_blocks_3_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(320), T.int64(960), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(60), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(40) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(lv1502[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv1502[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("unet.up_blocks.3.resnets.0.conv_shortcut.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(40) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(960), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(40) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(unet_up_blocks_3_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                    T.writes(unet_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                    unet_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_3_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(960), rc_0 * T.int64(16) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1521[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1521[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d32_add1_add3(lv1608: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_3_resnets_1_conv1_weight: T.Buffer((T.int64(320), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv1610: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv1617: T.Buffer((T.int64(2), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(66), T.int64(66)), scope="shared")
        unet_up_blocks_3_resnets_1_conv1_weight_shared = T.alloc_buffer((T.int64(320), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(16) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(60))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(16) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(60) // T.int64(6))
                                        v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(6))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(120))
                                        T.reads(lv1608[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv1608[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.up_blocks.3.resnets.1.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(40) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(360))
                                        T.reads(unet_up_blocks_3_resnets_1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_up_blocks_3_resnets_1_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_up_blocks_3_resnets_1_conv1_weight_shared[v0, v1, v2, v3] = unet_up_blocks_3_resnets_1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(16) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_3_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_3_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(5), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(5) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(16) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1610[T.int64(0), v1, T.int64(0), T.int64(0)], lv1617[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1610[T.int64(0), v1, T.int64(0), T.int64(0)] + lv1617[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d33_add1(lv1606: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_3_resnets_1_conv_shortcut_weight: T.Buffer((T.int64(320), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv1625: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), scope="shared")
        unet_up_blocks_3_resnets_1_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(320), T.int64(640), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(8), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(lv1606[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv1606[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("unet.up_blocks.3.resnets.1.conv_shortcut.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(unet_up_blocks_3_resnets_1_conv_shortcut_weight[v0, v1, v2, v3])
                                    T.writes(unet_up_blocks_3_resnets_1_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                    unet_up_blocks_3_resnets_1_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_up_blocks_3_resnets_1_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_up_blocks_3_resnets_1_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_up_blocks_3_resnets_1_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(8), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused // T.int64(128) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1625[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1625[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d34_add31(lv1815: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_conv_out_weight: T.Buffer((T.int64(4), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv1817: T.Buffer((T.int64(1), T.int64(4), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(66), T.int64(66)), scope="shared")
        unet_conv_out_weight_shared = T.alloc_buffer((T.int64(4), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(4), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(20), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(33)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(396))
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(396) // T.int64(66))
                                        v3 = T.axis.spatial(T.int64(66), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(66))
                                        T.reads(lv1815[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv1815[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.conv_out.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(144))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(144) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_conv_out_weight[v0, v1, v2, v3])
                                        T.writes(unet_conv_out_weight_shared[v0, v1, v2, v3])
                                        unet_conv_out_weight_shared[v0, v1, v2, v3] = unet_conv_out_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(4), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(16) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_conv_out_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_conv_out_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(4), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1817[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1817[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d35_add38(lv: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), vae_post_quant_conv_weight: T.Buffer((T.int64(4), T.int64(4), T.int64(1), T.int64(1)), "float32"), lv2: T.Buffer((T.int64(1), T.int64(4), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), scope="shared")
        vae_post_quant_conv_weight_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(4), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(256))
                                        v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.reads(lv[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("vae.post_quant_conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(4), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(8))
                                        T.reads(vae_post_quant_conv_weight[v0, v1, v2, v3])
                                        T.writes(vae_post_quant_conv_weight_shared[v0, v1, v2, v3])
                                        vae_post_quant_conv_weight_shared[v0, v1, v2, v3] = vae_post_quant_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(4), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(4), rc_0 * T.int64(4) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_post_quant_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_post_quant_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(8) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv2[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv2[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d36_add39(lv3: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), vae_decoder_conv_in_weight: T.Buffer((T.int64(512), T.int64(4), T.int64(3), T.int64(3)), "float32"), lv5: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(4), T.int64(66), T.int64(66)), scope="shared")
        vae_decoder_conv_in_weight_shared = T.alloc_buffer((T.int64(512), T.int64(4), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(204))
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(204) // T.int64(34))
                                        v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(34))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(816))
                                        T.reads(lv3[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv3[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("vae.decoder.conv_in.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(576))
                                        T.reads(vae_decoder_conv_in_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_conv_in_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_conv_in_weight_shared[v0, v1, v2, v3] = vae_decoder_conv_in_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(4), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_conv_in_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_conv_in_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv5[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv5[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d37_add39(lv8: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), vae_decoder_mid_block_resnets_0_conv1_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv10: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(66), T.int64(66)), scope="shared")
        vae_decoder_mid_block_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(324))
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(324) // T.int64(18))
                                        v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(648))
                                        T.reads(lv8[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv8[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("vae.decoder.mid_block.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(18))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(288))
                                        T.reads(vae_decoder_mid_block_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_mid_block_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_mid_block_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = vae_decoder_mid_block_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_mid_block_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_mid_block_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(32) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(32) // T.int64(16) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv10[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv10[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d37_add39_add40_divide7(lv13: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), vae_decoder_mid_block_resnets_0_conv2_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv15: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv6: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(66), T.int64(66)), scope="shared")
        vae_decoder_mid_block_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + yy_3_init * T.int64(4) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(108))
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(108) // T.int64(18))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(18))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(216))
                                    T.reads(lv13[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv13[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.mid_block.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(18))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(288))
                                        T.reads(vae_decoder_mid_block_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_mid_block_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_mid_block_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = vae_decoder_mid_block_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + yy_3 * T.int64(4) + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_mid_block_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_mid_block_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(lv6[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv15[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv6[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv15[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d37_add39_add40_divide7_divide7(lv61: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), vae_decoder_mid_block_resnets_1_conv2_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv63: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv54: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(66), T.int64(66)), scope="shared")
        vae_decoder_mid_block_resnets_1_conv2_weight_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0)
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v3 = T.axis.spatial(T.int64(66), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(72))
                                        T.reads(lv61[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv61[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("vae.decoder.mid_block.resnets.1.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3))
                                        v1 = T.axis.spatial(T.int64(512), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(384))
                                        T.reads(vae_decoder_mid_block_resnets_1_conv2_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_mid_block_resnets_1_conv2_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_mid_block_resnets_1_conv2_weight_shared[v0, v1, v2, v3] = vae_decoder_mid_block_resnets_1_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_mid_block_resnets_1_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_mid_block_resnets_1_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + ax3)
                            T.reads(lv54[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv63[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv54[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv63[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d38_add42(lv104: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), vae_decoder_up_blocks_0_upsamplers_0_conv_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv106: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(130), T.int64(130)), scope="shared")
        vae_decoder_up_blocks_0_upsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(16384), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(60))
                                    v2 = T.axis.spatial(T.int64(130), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(60) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(130), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(240))
                                    T.reads(lv104[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(129) and T.int64(1) <= v3 and v3 < T.int64(129), lv104[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.0.upsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(vae_decoder_up_blocks_0_upsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_0_upsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_0_upsamplers_0_conv_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_0_upsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_0_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_0_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv106[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv106[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d38_add42_add43_divide9(lv114: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), vae_decoder_up_blocks_1_resnets_0_conv2_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv116: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv107: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(130), T.int64(130)), scope="shared")
        vae_decoder_up_blocks_1_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(8) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(324))
                                        v2 = T.axis.spatial(T.int64(130), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(324) // T.int64(18))
                                        v3 = T.axis.spatial(T.int64(130), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2592))
                                        T.reads(lv114[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(129) and T.int64(1) <= v3 and v3 < T.int64(129), lv114[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.1.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(72))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(72) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(768) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(vae_decoder_up_blocks_1_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_1_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_1_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_1_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(3), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(8) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(8) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_1_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_1_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(64) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(lv107[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv116[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv107[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv116[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d39_add44(lv144: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_1_upsamplers_0_conv_weight: T.Buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv146: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(258), T.int64(258)), scope="shared")
        vae_decoder_up_blocks_1_upsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(512), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(102))
                                    v2 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(102) // T.int64(34))
                                    v3 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(34))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(204))
                                    T.reads(lv144[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(257) and T.int64(1) <= v3 and v3 < T.int64(257), lv144[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.1.upsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(18))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(vae_decoder_up_blocks_1_upsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_1_upsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_1_upsamplers_0_conv_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_1_upsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_1_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_1_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv146[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv146[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d3_add8(lv224: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_downsamplers_0_conv_weight: T.Buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv226: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(66), T.int64(66)), scope="shared")
        unet_down_blocks_0_downsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(320), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(5120), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2560) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2560) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2560))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(5) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(81))
                                    v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(81) // T.int64(9))
                                    v3 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(405))
                                    T.reads(lv224[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv224[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.down_blocks.0.downsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2560) // T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(45))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(5) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(45) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(360))
                                        T.reads(unet_down_blocks_0_downsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_0_downsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_0_downsamplers_0_conv_weight_shared[v0, v1, v2, v3] = unet_down_blocks_0_downsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(5), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2560) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2560) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(5) + rc_1 * T.int64(5) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx], unet_down_blocks_0_downsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx] * unet_down_blocks_0_downsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2560) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2560) // T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) // T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv226[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv226[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d40_add45(lv149: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_conv1_weight: T.Buffer((T.int64(256), T.int64(512), T.int64(3), T.int64(3)), "float32"), lv151: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(258), T.int64(258)), scope="shared")
        vae_decoder_up_blocks_2_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(256), T.int64(512), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(96))
                                        v2 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(96) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(258), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(768))
                                        T.reads(lv149[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(257) and T.int64(1) <= v3 and v3 < T.int64(257), lv149[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(12)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("vae.decoder.up_blocks.2.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(24) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(vae_decoder_up_blocks_2_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_2_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_2_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_2_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv151[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv151[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d41_add45(lv164: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_1_conv1_weight: T.Buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), "float32"), lv166: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(258), T.int64(258)), scope="shared")
        vae_decoder_up_blocks_2_resnets_1_conv1_weight_shared = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(180))
                                        v2 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(180) // T.int64(18))
                                        v3 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(720))
                                        T.reads(lv164[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(257) and T.int64(1) <= v3 and v3 < T.int64(257), lv164[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.2.resnets.1.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(576))
                                        T.reads(vae_decoder_up_blocks_2_resnets_1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_2_resnets_1_conv1_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_2_resnets_1_conv1_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_2_resnets_1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(16) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv166[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv166[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d41_add45_add46_divide10(lv154: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_conv2_weight: T.Buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), "float32"), lv156: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), lv160: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(258), T.int64(258)), scope="shared")
        vae_decoder_up_blocks_2_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(256), rc_0)
                                    v2 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(34))
                                    v3 = T.axis.spatial(T.int64(258), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(34))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(102))
                                    T.reads(lv154[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(257) and T.int64(1) <= v3 and v3 < T.int64(257), lv154[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.2.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(256), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(vae_decoder_up_blocks_2_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_2_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_2_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_2_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(8) + ax3)
                            T.reads(lv160[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv156[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv160[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv156[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d42_add45(lv147: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(256), T.int64(512), T.int64(1), T.int64(1)), "float32"), lv159: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), scope="shared")
        vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(256), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(8) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(32) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(128))
                                        T.reads(lv147[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv147[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("vae.decoder.up_blocks.2.resnets.0.conv_shortcut.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(128))
                                        T.reads(vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(8) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv159[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv159[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d43_add47(lv187: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_2_upsamplers_0_conv_weight: T.Buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), "float32"), lv189: T.Buffer((T.int64(1), T.int64(256), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(514), T.int64(514)), scope="shared")
        vae_decoder_up_blocks_2_upsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32768), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8192) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8192) // T.int64(64) * T.int64(4) + yy_3_init * T.int64(4) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(256), rc_0)
                                    v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8192) // T.int64(64) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(10))
                                    v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(10))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(60))
                                    T.reads(lv187[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv187[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.2.upsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8192) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(256), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(vae_decoder_up_blocks_2_upsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_2_upsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_2_upsamplers_0_conv_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_2_upsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8192) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8192) // T.int64(64) * T.int64(4) + yy_3 * T.int64(4) + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_2_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_2_upsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(256), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8192) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8192) // T.int64(64) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv189[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv189[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d44_add48(lv192: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_conv1_weight: T.Buffer((T.int64(128), T.int64(256), T.int64(3), T.int64(3)), "float32"), lv194: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(514), T.int64(514)), scope="shared")
        vae_decoder_up_blocks_3_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(128), T.int64(256), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + xx_3_init * T.int64(8) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(204))
                                    v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(16) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(204) // T.int64(34))
                                    v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(34))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(816))
                                    T.reads(lv192[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv192[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.3.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(36))
                                        v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(36) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(vae_decoder_up_blocks_3_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_3_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_3_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_3_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + xx_3 * T.int64(8) + xx_4)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_3_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_3_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2048) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2048) // T.int64(16) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv194[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv194[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d45_add48(lv207: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_1_conv1_weight: T.Buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), "float32"), lv209: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(514), T.int64(514)), scope="shared")
        vae_decoder_up_blocks_3_resnets_1_conv1_weight_shared = T.alloc_buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32768), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(64) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(128), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(100))
                                        v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(100) // T.int64(10))
                                        v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(200))
                                        T.reads(lv207[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv207[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.3.resnets.1.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(18))
                                        v1 = T.axis.spatial(T.int64(128), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(288))
                                        T.reads(vae_decoder_up_blocks_3_resnets_1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_3_resnets_1_conv1_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_3_resnets_1_conv1_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_3_resnets_1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(64) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(128), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_3_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_3_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(64) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(64) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv209[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv209[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d45_add48_add49_divide11(lv197: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_conv2_weight: T.Buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), "float32"), lv199: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), lv203: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(514), T.int64(514)), scope="shared")
        vae_decoder_up_blocks_3_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(128), T.int64(128), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(128), rc_0)
                                        v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(6))
                                        v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(6))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(396))
                                        T.reads(lv197[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv197[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("vae.decoder.up_blocks.3.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(128), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(144))
                                        T.reads(vae_decoder_up_blocks_3_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(vae_decoder_up_blocks_3_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        vae_decoder_up_blocks_3_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_3_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(128), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_3_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_3_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1024) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1024) // T.int64(128) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) * T.int64(4) + ax3)
                            T.reads(lv203[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv199[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv203[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv199[v0, v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d46_add48(lv190: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(128), T.int64(256), T.int64(1), T.int64(1)), "float32"), lv202: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), scope="shared")
        vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(128), T.int64(256), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32768), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.reads(lv190[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = lv190[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("vae.decoder.up_blocks.3.resnets.0.conv_shortcut.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(256), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                    T.writes(vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                    vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(256), rc_0 * T.int64(16) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_up_blocks_3_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(128), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4096) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4096) // T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv202[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv202[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d47_add50_divide12_add51_tir_clip(lv231: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), vae_decoder_conv_out_weight: T.Buffer((T.int64(3), T.int64(128), T.int64(3), T.int64(3)), "float32"), lv233: T.Buffer((T.int64(1), T.int64(3), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(514), T.int64(514)), scope="shared")
        vae_decoder_conv_out_weight_shared = T.alloc_buffer((T.int64(3), T.int64(128), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(3), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(3), nn_1_ff_1_yy_1_xx_1_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(128), rc_0)
                                        v2 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(18))
                                        v3 = T.axis.spatial(T.int64(514), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(180))
                                        T.reads(lv231[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(513) and T.int64(1) <= v3 and v3 < T.int64(513), lv231[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("vae.decoder.conv_out.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(9))
                                    v1 = T.axis.spatial(T.int64(128), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(27))
                                    T.reads(vae_decoder_conv_out_weight[v0, v1, v2, v3])
                                    T.writes(vae_decoder_conv_out_weight_shared[v0, v1, v2, v3])
                                    vae_decoder_conv_out_weight_shared[v0, v1, v2, v3] = vae_decoder_conv_out_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(3), nn_1_ff_1_yy_1_xx_1_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(128), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], vae_decoder_conv_out_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * vae_decoder_conv_out_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(3), nn_1_ff_1_yy_1_xx_1_fused + ax1)
                            v2 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(512), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv233[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_compute_intermediate[v0, v1, v2, v3])
                            var_compute_intermediate[v0, v1, v2, v3] = T.max(T.min((var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv233[v0, v1, T.int64(0), T.int64(0)]) * T.float32(0.5) + T.float32(0.5), T.float32(1)), T.float32(0))

    @T.prim_func
    def fused_conv2d4_add9_add11(lv229: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_conv1_weight: T.Buffer((T.int64(640), T.int64(320), T.int64(3), T.int64(3)), "float32"), lv231: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv238: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(34), T.int64(34)), scope="shared")
        unet_down_blocks_1_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(640), T.int64(320), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(20), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1024) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(34), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(lv229[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv229[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(12)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.down_blocks.1.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(48))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(48) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(unet_down_blocks_1_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_1_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_1_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(16), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(16) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv231[T.int64(0), v1, T.int64(0), T.int64(0)], lv238[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv231[T.int64(0), v1, T.int64(0), T.int64(0)] + lv238[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d5_add9_add11(lv332: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_1_conv1_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv334: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv341: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(34), T.int64(34)), scope="shared")
        unet_down_blocks_1_resnets_1_conv1_weight_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2560), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(80), T.int64(3), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280))
                                        v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(48))
                                        v2 = T.axis.spatial(T.int64(34), ry_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(48) // T.int64(6))
                                        v3 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(6))
                                        T.reads(lv332[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv332[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.down_blocks.1.resnets.1.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(24) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), ry_0)
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_down_blocks_1_resnets_1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_1_resnets_1_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_1_resnets_1_conv1_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_resnets_1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(8) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_resnets_1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(1280) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused % T.int64(1280) // T.int64(32) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(8) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv334[T.int64(0), v1, T.int64(0), T.int64(0)], lv341[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv334[T.int64(0), v1, T.int64(0), T.int64(0)] + lv341[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d5_add9_add12_divide2(lv241: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_conv2_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv243: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv247: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(34), T.int64(34)), scope="shared")
        unet_down_blocks_1_resnets_0_conv2_weight_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(100))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(100) // T.int64(10))
                                        v3 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(200))
                                        T.reads(lv241[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv241[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.down_blocks.1.resnets.0.conv2.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(144))
                                        T.reads(unet_down_blocks_1_resnets_0_conv2_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_1_resnets_0_conv2_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_1_resnets_0_conv2_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_resnets_0_conv2_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(2), T.int64(2), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_resnets_0_conv2_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_2_ff_2_yy_2_xx_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(lv247[v0, v1, v2, v3], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv243[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_divide_intermediate[v0, v1, v2, v3])
                            var_T_divide_intermediate[v0, v1, v2, v3] = lv247[v0, v1, v2, v3] + (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv243[T.int64(0), v1, T.int64(0), T.int64(0)])

    @T.prim_func
    def fused_conv2d6_add9(lv227: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_conv_shortcut_weight: T.Buffer((T.int64(640), T.int64(320), T.int64(1), T.int64(1)), "float32"), lv246: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), scope="shared")
        unet_down_blocks_1_resnets_0_conv_shortcut_weight_shared = T.alloc_buffer((T.int64(640), T.int64(320), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(20), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(2))
                                    v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2))
                                    T.reads(lv227[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv227[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("unet.down_blocks.1.resnets.0.conv_shortcut.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(320), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(unet_down_blocks_1_resnets_0_conv_shortcut_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_1_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_1_resnets_0_conv_shortcut_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_resnets_0_conv_shortcut_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(320), rc_0 * T.int64(16) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_resnets_0_conv_shortcut_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(16) * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv246[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv246[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d7_add9(lv250: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_attentions_0_proj_in_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv252: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="shared")
        unet_down_blocks_1_attentions_0_proj_in_weight_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(160), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(lv250[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv250[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("unet.down_blocks.1.attentions.0.proj_in.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(unet_down_blocks_1_attentions_0_proj_in_weight[v0, v1, v2, v3])
                                    T.writes(unet_down_blocks_1_attentions_0_proj_in_weight_shared[v0, v1, v2, v3])
                                    unet_down_blocks_1_attentions_0_proj_in_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_attentions_0_proj_in_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_attentions_0_proj_in_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(32) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(32) // T.int64(4) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv252[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv252[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d7_add9_add12(lv326: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_attentions_0_proj_out_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv328: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), lv249: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), scope="shared")
        unet_down_blocks_1_attentions_0_proj_out_weight_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1024))
                                    T.reads(lv326[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = lv326[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.down_blocks.1.attentions.0.proj_out.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(640) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(640) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(40))
                                        T.reads(unet_down_blocks_1_attentions_0_proj_out_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_1_attentions_0_proj_out_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_1_attentions_0_proj_out_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_attentions_0_proj_out_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_1_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_1_attentions_0_proj_out_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(32), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv328[T.int64(0), v1, T.int64(0), T.int64(0)], lv249[v0, v1, v2, v3])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv328[T.int64(0), v1, T.int64(0), T.int64(0)] + lv249[v0, v1, v2, v3]

    @T.prim_func
    def fused_conv2d8_add16(lv430: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_downsamplers_0_conv_weight: T.Buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv432: T.Buffer((T.int64(1), T.int64(640), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(34), T.int64(34)), scope="shared")
        unet_down_blocks_1_downsamplers_0_conv_weight_shared = T.alloc_buffer((T.int64(640), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + yy_3_init * T.int64(4) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(320), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(198))
                                        v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(198) // T.int64(99))
                                        v2 = T.axis.spatial(T.int64(34), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(99) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(34), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(396))
                                        T.reads(lv430[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(33) and T.int64(1) <= v3 and v3 < T.int64(33), lv430[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.down_blocks.1.downsamplers.0.conv.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(18))
                                        v1 = T.axis.spatial(T.int64(640), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(192) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(unet_down_blocks_1_downsamplers_0_conv_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_1_downsamplers_0_conv_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_1_downsamplers_0_conv_weight_shared[v0, v1, v2, v3] = unet_down_blocks_1_downsamplers_0_conv_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + yy_3 * T.int64(4) + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx], unet_down_blocks_1_downsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx] * unet_down_blocks_1_downsamplers_0_conv_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv432[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv432[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d9_add17_add18(lv435: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_conv1_weight: T.Buffer((T.int64(1280), T.int64(640), T.int64(3), T.int64(3)), "float32"), lv437: T.Buffer((T.int64(1), T.int64(1280), T.int64(1), T.int64(1)), "float32"), lv444: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(640), T.int64(18), T.int64(18)), scope="shared")
        unet_down_blocks_2_resnets_0_conv1_weight_shared = T.alloc_buffer((T.int64(1280), T.int64(640), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_3_init * T.int64(2) + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(16), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(640), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(240) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(72))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(18), (ax0_ax1_ax2_ax3_fused_0 * T.int64(240) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(72) // T.int64(4))
                                        v3 = T.axis.spatial(T.int64(18), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(240) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(144))
                                        T.reads(lv435[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(17) and T.int64(1) <= v3 and v3 < T.int64(17), lv435[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("unet.down_blocks.2.resnets.0.conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(640), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(720))
                                        T.reads(unet_down_blocks_2_resnets_0_conv1_weight[v0, v1, v2, v3])
                                        T.writes(unet_down_blocks_2_resnets_0_conv1_weight_shared[v0, v1, v2, v3])
                                        unet_down_blocks_2_resnets_0_conv1_weight_shared[v0, v1, v2, v3] = unet_down_blocks_2_resnets_0_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(3), T.int64(2), T.int64(4), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_3 * T.int64(2) + nn_4)
                                v_ff = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(16), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(640), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_down_blocks_2_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_down_blocks_2_resnets_0_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), nn_0_ff_0_yy_0_xx_0_fused // T.int64(8) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(16), nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(16), nn_0_ff_0_yy_0_xx_0_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv437[T.int64(0), v1, T.int64(0), T.int64(0)], lv444[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv437[T.int64(0), v1, T.int64(0), T.int64(0)] + lv444[v0, v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_conv2d_add1(lv: T.Buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), "float32"), unet_conv_in_weight: T.Buffer((T.int64(320), T.int64(4), T.int64(3), T.int64(3)), "float32"), lv23: T.Buffer((T.int64(1), T.int64(320), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(2), T.int64(4), T.int64(66), T.int64(66)), scope="shared")
        unet_conv_in_weight_shared = T.alloc_buffer((T.int64(320), T.int64(4), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(128) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(2), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512))
                                        v1 = T.axis.spatial(T.int64(4), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(66), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(66), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(320) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(128))
                                        T.reads(lv[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(65) and T.int64(1) <= v3 and v3 < T.int64(65), lv[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("unet.conv_in.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(128) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(6))
                                        v1 = T.axis.spatial(T.int64(4), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(6) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(480) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.reads(unet_conv_in_weight[v0, v1, v2, v3])
                                        T.writes(unet_conv_in_weight_shared[v0, v1, v2, v3])
                                        unet_conv_in_weight_shared[v0, v1, v2, v3] = unet_conv_in_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(128) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(4), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], unet_conv_in_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * unet_conv_in_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), nn_0_ff_0_yy_0_xx_0_fused // T.int64(512) + ax0)
                            v1 = T.axis.spatial(T.int64(320), nn_0_ff_0_yy_0_xx_0_fused % T.int64(512) // T.int64(128) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(128) // T.int64(4) * T.int64(2) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(64), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv23[T.int64(0), v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv23[T.int64(0), v1, T.int64(0), T.int64(0)]

    @T.prim_func
    def fused_group_norm10_silu7(lv797: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32"), unet_up_blocks_0_resnets_0_norm1_weight: T.Buffer((T.int64(2560),), "float32"), unet_up_blocks_0_resnets_0_norm1_bias: T.Buffer((T.int64(2560),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(2560), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(80)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(80), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) // T.int64(64))
                        v_k3 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(64) // T.int64(8))
                        v_k4 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(8))
                        T.reads(lv797[((v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(2560), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv797[((v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(2560), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv797[((v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(2560), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)] * lv797[((v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(2560), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(163840))
                        v_ax1 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(163840) // T.int64(64))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                        v_ax3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                        T.reads(lv797[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560), (v_ax3 // T.int64(8) + v_ax2) % T.int64(8), v_ax3 % T.int64(8)], A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)], A_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)], unet_up_blocks_0_resnets_0_norm1_weight[((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560)], unet_up_blocks_0_resnets_0_norm1_bias[((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv797[((((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) // T.int64(2560) + (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560), (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) % T.int64(8), v_ax3 % T.int64(8) % T.int64(8)] - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001) - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001) * (A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_0_resnets_0_norm1_weight[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560)] + unet_up_blocks_0_resnets_0_norm1_bias[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560)]) * T.sigmoid((lv797[((((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) // T.int64(2560) + (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560), (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) % T.int64(8), v_ax3 % T.int64(8) % T.int64(8)] - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001) - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001) * (A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_0_resnets_0_norm1_weight[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560)] + unet_up_blocks_0_resnets_0_norm1_bias[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(80)) % T.int64(2560)])

    @T.prim_func
    def fused_group_norm11_silu8(lv870: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_1_resnets_0_norm1_weight: T.Buffer((T.int64(2560),), "float32"), unet_up_blocks_1_resnets_0_norm1_bias: T.Buffer((T.int64(2560),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(2560), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(80), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(lv870[((v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(2560), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv870[((v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(2560), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv870[((v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(2560), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * lv870[((v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(2560) + v_ax0) % T.int64(2), (v_ax1 * T.int64(80) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(2560), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(lv870[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)], A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)], A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)], unet_up_blocks_1_resnets_0_norm1_weight[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560)], unet_up_blocks_1_resnets_0_norm1_bias[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv870[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) // T.int64(2560) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_1_resnets_0_norm1_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560)] + unet_up_blocks_1_resnets_0_norm1_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560)]) * T.sigmoid((lv870[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) // T.int64(2560) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(2560) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_1_resnets_0_norm1_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560)] + unet_up_blocks_1_resnets_0_norm1_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(2560) // T.int64(80) * T.int64(80) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(80)) % T.int64(2560)])

    @T.prim_func
    def fused_group_norm12_silu9(lv1078: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32"), unet_up_blocks_1_resnets_2_norm1_weight: T.Buffer((T.int64(1920),), "float32"), unet_up_blocks_1_resnets_2_norm1_bias: T.Buffer((T.int64(1920),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1920), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(60)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(60), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(lv1078[((v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1920), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv1078[((v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1920), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv1078[((v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1920), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * lv1078[((v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1920), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(15)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(491520))
                        v_ax1 = T.axis.spatial(T.int64(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(491520) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(lv1078[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)], A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)], A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)], unet_up_blocks_1_resnets_2_norm1_weight[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920)], unet_up_blocks_1_resnets_2_norm1_bias[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1078[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) // T.int64(1920) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_1_resnets_2_norm1_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920)] + unet_up_blocks_1_resnets_2_norm1_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920)]) * T.sigmoid((lv1078[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) // T.int64(1920) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(6.5104166666666666e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_1_resnets_2_norm1_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920)] + unet_up_blocks_1_resnets_2_norm1_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(60)) % T.int64(1920)])

    @T.prim_func
    def fused_group_norm13_silu10(lv1186: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_0_norm1_weight: T.Buffer((T.int64(1920),), "float32"), unet_up_blocks_2_resnets_0_norm1_bias: T.Buffer((T.int64(1920),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1920), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(240)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(60), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv1186[((v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1920), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv1186[((v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1920), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv1186[((v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1920), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv1186[((v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1920) + v_ax0) % T.int64(2), (v_ax1 * T.int64(60) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1920), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(60)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1966080))
                        v_ax1 = T.axis.spatial(T.int64(1920), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1966080) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv1186[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)], A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)], unet_up_blocks_2_resnets_0_norm1_weight[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920)], unet_up_blocks_2_resnets_0_norm1_bias[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1186[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) // T.int64(1920) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_2_resnets_0_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920)] + unet_up_blocks_2_resnets_0_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920)]) * T.sigmoid((lv1186[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) // T.int64(1920) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1920) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60)] * T.float32(1.6276041666666666e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_2_resnets_0_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920)] + unet_up_blocks_2_resnets_0_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1920) // T.int64(60) * T.int64(60) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(60)) % T.int64(1920)])

    @T.prim_func
    def fused_group_norm14_silu11(lv1290: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_1_norm1_weight: T.Buffer((T.int64(1280),), "float32"), unet_up_blocks_2_resnets_1_norm1_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv1290[((v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1280), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv1290[((v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1280), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv1290[((v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1280), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv1290[((v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(1280), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv1290[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)], A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)], unet_up_blocks_2_resnets_1_norm1_weight[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280)], unet_up_blocks_2_resnets_1_norm1_bias[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1290[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_2_resnets_1_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280)] + unet_up_blocks_2_resnets_1_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280)]) * T.sigmoid((lv1290[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_2_resnets_1_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280)] + unet_up_blocks_2_resnets_1_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(40)) % T.int64(1280)])

    @T.prim_func
    def fused_group_norm15_silu12(lv1394: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32"), unet_up_blocks_2_resnets_2_norm1_weight: T.Buffer((T.int64(960),), "float32"), unet_up_blocks_2_resnets_2_norm1_bias: T.Buffer((T.int64(960),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(960), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(120)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(30), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv1394[((v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(960), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv1394[((v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(960), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv1394[((v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(960), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv1394[((v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(960), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(30)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(983040))
                        v_ax1 = T.axis.spatial(T.int64(960), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(983040) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv1394[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)], A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)], unet_up_blocks_2_resnets_2_norm1_weight[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960)], unet_up_blocks_2_resnets_2_norm1_bias[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1394[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) // T.int64(960) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_2_resnets_2_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960)] + unet_up_blocks_2_resnets_2_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960)]) * T.sigmoid((lv1394[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) // T.int64(960) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(3.2552083333333333e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_2_resnets_2_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960)] + unet_up_blocks_2_resnets_2_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(30)) % T.int64(960)])

    @T.prim_func
    def fused_group_norm16_silu13(lv1502: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_3_resnets_0_norm1_weight: T.Buffer((T.int64(960),), "float32"), unet_up_blocks_3_resnets_0_norm1_bias: T.Buffer((T.int64(960),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(960), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(480)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(30), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(lv1502[((v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(960), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv1502[((v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(960), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv1502[((v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(960), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * lv1502[((v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(960) + v_ax0) % T.int64(2), (v_ax1 * T.int64(30) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(960), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(120)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3932160))
                        v_ax1 = T.axis.spatial(T.int64(960), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3932160) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(lv1502[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)], A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)], A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)], unet_up_blocks_3_resnets_0_norm1_weight[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960)], unet_up_blocks_3_resnets_0_norm1_bias[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1502[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) // T.int64(960) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06) - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06) * (A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_3_resnets_0_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960)] + unet_up_blocks_3_resnets_0_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960)]) * T.sigmoid((lv1502[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) // T.int64(960) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06) - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06) * (A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(960) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30)] * T.float32(8.1380208333333332e-06)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_3_resnets_0_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960)] + unet_up_blocks_3_resnets_0_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(960) // T.int64(30) * T.int64(30) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(30)) % T.int64(960)])

    @T.prim_func
    def fused_group_norm17_silu14(lv1606: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32"), unet_up_blocks_3_resnets_1_norm1_weight: T.Buffer((T.int64(640),), "float32"), unet_up_blocks_3_resnets_1_norm1_bias: T.Buffer((T.int64(640),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(320)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(20), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(lv1606[((v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(640), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv1606[((v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(640), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv1606[((v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(640), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * lv1606[((v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(640), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(80)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2621440))
                        v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2621440) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(lv1606[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)], A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)], A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)], unet_up_blocks_3_resnets_1_norm1_weight[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640)], unet_up_blocks_3_resnets_1_norm1_bias[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv1606[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05) - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_3_resnets_1_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640)] + unet_up_blocks_3_resnets_1_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640)]) * T.sigmoid((lv1606[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05) - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(1.2207031250000001e-05)) + T.float32(1.0000000000000001e-05)) * unet_up_blocks_3_resnets_1_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640)] + unet_up_blocks_3_resnets_1_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(20)) % T.int64(640)])

    @T.prim_func
    def fused_group_norm18_silu15(lv6: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), vae_decoder_mid_block_resnets_0_norm1_weight: T.Buffer((T.int64(512),), "float32"), vae_decoder_mid_block_resnets_0_norm1_bias: T.Buffer((T.int64(512),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(512)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(lv6[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv6[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv6[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * lv6[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(512), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(lv6[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)], A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)], A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)], vae_decoder_mid_block_resnets_0_norm1_weight[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512)], vae_decoder_mid_block_resnets_0_norm1_bias[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv6[T.int64(0), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * vae_decoder_mid_block_resnets_0_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512)] + vae_decoder_mid_block_resnets_0_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512)]) * T.sigmoid((lv6[T.int64(0), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * vae_decoder_mid_block_resnets_0_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512)] + vae_decoder_mid_block_resnets_0_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16)) % T.int64(512)])

    @T.prim_func
    def fused_group_norm20_silu16(lv107: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), vae_decoder_up_blocks_1_resnets_0_norm1_weight: T.Buffer((T.int64(512),), "float32"), vae_decoder_up_blocks_1_resnets_0_norm1_bias: T.Buffer((T.int64(512),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(2048)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(16384))
                        v_k3 = T.axis.reduce(T.int64(128), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(16384) // T.int64(128))
                        v_k4 = T.axis.reduce(T.int64(128), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(128))
                        T.reads(lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(128) + v_k3) // T.int64(128) + v_k2) % T.int64(512), (v_k4 // T.int64(128) + v_k3) % T.int64(128), v_k4 % T.int64(128)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(128) + v_k3) // T.int64(128) + v_k2) % T.int64(512), (v_k4 // T.int64(128) + v_k3) % T.int64(128), v_k4 % T.int64(128)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(128) + v_k3) // T.int64(128) + v_k2) % T.int64(512), (v_k4 // T.int64(128) + v_k3) % T.int64(128), v_k4 % T.int64(128)] * lv107[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(128) + v_k3) // T.int64(128) + v_k2) % T.int64(512), (v_k4 // T.int64(128) + v_k3) % T.int64(128), v_k4 % T.int64(128)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(128)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16384))
                        v_ax2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16384) // T.int64(128))
                        v_ax3 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                        T.reads(lv107[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512), (v_ax3 // T.int64(128) + v_ax2) % T.int64(128), v_ax3 % T.int64(128)], A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)], A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)], vae_decoder_up_blocks_1_resnets_0_norm1_weight[((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512)], vae_decoder_up_blocks_1_resnets_0_norm1_bias[((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv107[T.int64(0), (((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(128) // T.int64(128) + (v_ax3 // T.int64(128) + v_ax2) % T.int64(128)) // T.int64(128) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(128) // T.int64(128) + (v_ax3 // T.int64(128) + v_ax2) % T.int64(128)) % T.int64(128), v_ax3 % T.int64(128) % T.int64(128)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_1_resnets_0_norm1_weight[(((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512)] + vae_decoder_up_blocks_1_resnets_0_norm1_bias[(((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512)]) * T.sigmoid((lv107[T.int64(0), (((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(128) // T.int64(128) + (v_ax3 // T.int64(128) + v_ax2) % T.int64(128)) // T.int64(128) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(128) // T.int64(128) + (v_ax3 // T.int64(128) + v_ax2) % T.int64(128)) % T.int64(128), v_ax3 % T.int64(128) % T.int64(128)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(3.814697265625e-06)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_1_resnets_0_norm1_weight[(((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512)] + vae_decoder_up_blocks_1_resnets_0_norm1_bias[(((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(128) + v_ax2) // T.int64(128) + v_ax1) % T.int64(16)) % T.int64(512)])

    @T.prim_func
    def fused_group_norm21_silu17(lv147: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_norm1_weight: T.Buffer((T.int64(512),), "float32"), vae_decoder_up_blocks_2_resnets_0_norm1_bias: T.Buffer((T.int64(512),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x"):
            for k2_k3_k4_fused_0 in range(T.int64(8192)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(65536))
                        v_k3 = T.axis.reduce(T.int64(256), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(65536) // T.int64(256))
                        v_k4 = T.axis.reduce(T.int64(256), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(256))
                        T.reads(lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(512), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(512), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(512), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)] * lv147[T.int64(0), (v_ax1 * T.int64(16) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(512), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(512)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(65536))
                        v_ax2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(65536) // T.int64(256))
                        v_ax3 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256))
                        T.reads(lv147[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512), (v_ax3 // T.int64(256) + v_ax2) % T.int64(256), v_ax3 % T.int64(256)], A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)], A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)], vae_decoder_up_blocks_2_resnets_0_norm1_weight[((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512)], vae_decoder_up_blocks_2_resnets_0_norm1_bias[((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv147[T.int64(0), (((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) // T.int64(256) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) % T.int64(256), v_ax3 % T.int64(256) % T.int64(256)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_2_resnets_0_norm1_weight[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512)] + vae_decoder_up_blocks_2_resnets_0_norm1_bias[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512)]) * T.sigmoid((lv147[T.int64(0), (((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) // T.int64(256) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512), (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) % T.int64(256), v_ax3 % T.int64(256) % T.int64(256)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_2_resnets_0_norm1_weight[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512)] + vae_decoder_up_blocks_2_resnets_0_norm1_bias[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(512) // T.int64(16) * T.int64(16) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16)) % T.int64(512)])

    @T.prim_func
    def fused_group_norm22_silu18(lv152: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), vae_decoder_up_blocks_2_resnets_0_norm2_weight: T.Buffer((T.int64(256),), "float32"), vae_decoder_up_blocks_2_resnets_0_norm2_bias: T.Buffer((T.int64(256),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(4096)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(65536))
                        v_k3 = T.axis.reduce(T.int64(256), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(65536) // T.int64(256))
                        v_k4 = T.axis.reduce(T.int64(256), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(256))
                        T.reads(lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(256), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(256), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(256), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)] * lv152[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(256) + v_k3) // T.int64(256) + v_k2) % T.int64(256), (v_k4 // T.int64(256) + v_k3) % T.int64(256), v_k4 % T.int64(256)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(256)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(65536))
                        v_ax2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(65536) // T.int64(256))
                        v_ax3 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256))
                        T.reads(lv152[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256), (v_ax3 // T.int64(256) + v_ax2) % T.int64(256), v_ax3 % T.int64(256)], A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)], A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)], vae_decoder_up_blocks_2_resnets_0_norm2_weight[((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256)], vae_decoder_up_blocks_2_resnets_0_norm2_bias[((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv152[T.int64(0), (((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) // T.int64(256) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256), (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) % T.int64(256), v_ax3 % T.int64(256) % T.int64(256)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_2_resnets_0_norm2_weight[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256)] + vae_decoder_up_blocks_2_resnets_0_norm2_bias[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256)]) * T.sigmoid((lv152[T.int64(0), (((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) // T.int64(256) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256), (v_ax3 % T.int64(256) // T.int64(256) + (v_ax3 // T.int64(256) + v_ax2) % T.int64(256)) % T.int64(256), v_ax3 % T.int64(256) % T.int64(256)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(1.9073486328125e-06)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_2_resnets_0_norm2_weight[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256)] + vae_decoder_up_blocks_2_resnets_0_norm2_bias[(((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(256) + v_ax2) // T.int64(256) + v_ax1) % T.int64(8)) % T.int64(256)])

    @T.prim_func
    def fused_group_norm23_silu19(lv190: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_norm1_weight: T.Buffer((T.int64(256),), "float32"), vae_decoder_up_blocks_3_resnets_0_norm1_bias: T.Buffer((T.int64(256),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(16384)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(262144))
                        v_k3 = T.axis.reduce(T.int64(512), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(262144) // T.int64(512))
                        v_k4 = T.axis.reduce(T.int64(512), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(512))
                        T.reads(lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(256), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(256), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(256), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)] * lv190[T.int64(0), (v_ax1 * T.int64(8) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(256), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1024)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(262144))
                        v_ax2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(262144) // T.int64(512))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv190[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256), (v_ax3 // T.int64(512) + v_ax2) % T.int64(512), v_ax3 % T.int64(512)], A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)], A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)], vae_decoder_up_blocks_3_resnets_0_norm1_weight[((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256)], vae_decoder_up_blocks_3_resnets_0_norm1_bias[((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv190[T.int64(0), (((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) // T.int64(512) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256), (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) % T.int64(512), v_ax3 % T.int64(512) % T.int64(512)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_3_resnets_0_norm1_weight[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256)] + vae_decoder_up_blocks_3_resnets_0_norm1_bias[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256)]) * T.sigmoid((lv190[T.int64(0), (((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) // T.int64(512) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256), (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) % T.int64(512), v_ax3 % T.int64(512) % T.int64(512)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8)] * T.float32(4.76837158203125e-07)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_3_resnets_0_norm1_weight[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256)] + vae_decoder_up_blocks_3_resnets_0_norm1_bias[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(256) // T.int64(8) * T.int64(8) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(8)) % T.int64(256)])

    @T.prim_func
    def fused_group_norm24_silu20(lv195: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32"), vae_decoder_up_blocks_3_resnets_0_norm2_weight: T.Buffer((T.int64(128),), "float32"), vae_decoder_up_blocks_3_resnets_0_norm2_bias: T.Buffer((T.int64(128),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(128), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(8192)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(4), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(262144))
                        v_k3 = T.axis.reduce(T.int64(512), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(262144) // T.int64(512))
                        v_k4 = T.axis.reduce(T.int64(512), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(512))
                        T.reads(lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(128), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(128), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(128), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)] * lv195[T.int64(0), (v_ax1 * T.int64(4) + (v_k4 // T.int64(512) + v_k3) // T.int64(512) + v_k2) % T.int64(128), (v_k4 // T.int64(512) + v_k3) % T.int64(512), v_k4 % T.int64(512)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(512)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(262144))
                        v_ax2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(262144) // T.int64(512))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv195[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128), (v_ax3 // T.int64(512) + v_ax2) % T.int64(512), v_ax3 % T.int64(512)], A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)], A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)], vae_decoder_up_blocks_3_resnets_0_norm2_weight[((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128)], vae_decoder_up_blocks_3_resnets_0_norm2_bias[((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv195[T.int64(0), (((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) // T.int64(512) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128), (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) % T.int64(512), v_ax3 % T.int64(512) % T.int64(512)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_3_resnets_0_norm2_weight[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128)] + vae_decoder_up_blocks_3_resnets_0_norm2_bias[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128)]) * T.sigmoid((lv195[T.int64(0), (((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) // T.int64(512) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128), (v_ax3 % T.int64(512) // T.int64(512) + (v_ax3 // T.int64(512) + v_ax2) % T.int64(512)) % T.int64(512), v_ax3 % T.int64(512) % T.int64(512)] - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07)) * T.rsqrt(A_red_temp_v1[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07) - A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07) * (A_red_temp_v0[T.int64(0), ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4)] * T.float32(9.5367431640625e-07)) + T.float32(9.9999999999999995e-07)) * vae_decoder_up_blocks_3_resnets_0_norm2_weight[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128)] + vae_decoder_up_blocks_3_resnets_0_norm2_bias[(((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(128) // T.int64(4) * T.int64(4) + ((v_ax3 // T.int64(512) + v_ax2) // T.int64(512) + v_ax1) % T.int64(4)) % T.int64(128)])

    @T.prim_func
    def fused_group_norm2_silu2(lv227: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_norm1_weight: T.Buffer((T.int64(320),), "float32"), unet_down_blocks_1_resnets_0_norm1_bias: T.Buffer((T.int64(320),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(40)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(10), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv227[((v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(320), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv227[((v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(320), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv227[((v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(320), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv227[((v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(320), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv227[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)], A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)], unet_down_blocks_1_resnets_0_norm1_weight[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320)], unet_down_blocks_1_resnets_0_norm1_bias[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv227[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) // T.int64(320) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_1_resnets_0_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320)] + unet_down_blocks_1_resnets_0_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320)]) * T.sigmoid((lv227[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) // T.int64(320) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_1_resnets_0_norm1_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320)] + unet_down_blocks_1_resnets_0_norm1_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(10)) % T.int64(320)])

    @T.prim_func
    def fused_group_norm3_silu3(lv239: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), unet_down_blocks_1_resnets_0_norm2_weight: T.Buffer((T.int64(640),), "float32"), unet_down_blocks_1_resnets_0_norm2_bias: T.Buffer((T.int64(640),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(20), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(lv239[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv239[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv239[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * lv239[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(1024))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                        T.reads(lv239[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640), (v_ax3 // T.int64(32) + v_ax2) % T.int64(32), v_ax3 % T.int64(32)], A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)], A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)], unet_down_blocks_1_resnets_0_norm2_weight[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640)], unet_down_blocks_1_resnets_0_norm2_bias[((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv239[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_1_resnets_0_norm2_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640)] + unet_down_blocks_1_resnets_0_norm2_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640)]) * T.sigmoid((lv239[((((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) // T.int64(32) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(32) // T.int64(32) + (v_ax3 // T.int64(32) + v_ax2) % T.int64(32)) % T.int64(32), v_ax3 % T.int64(32) % T.int64(32)] - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05) - A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(4.8828125000000003e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_1_resnets_0_norm2_weight[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640)] + unet_down_blocks_1_resnets_0_norm2_bias[(((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(32) + v_ax2) // T.int64(32) + v_ax1) % T.int64(20)) % T.int64(640)])

    @T.prim_func
    def fused_group_norm5_silu4(lv433: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_norm1_weight: T.Buffer((T.int64(640),), "float32"), unet_down_blocks_2_resnets_0_norm1_bias: T.Buffer((T.int64(640),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(80)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(20), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(64) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(lv433[((v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(640), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv433[((v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(640), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv433[((v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(640), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * lv433[((v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(640), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(163840))
                        v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(163840) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(lv433[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)], A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)], A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)], unet_down_blocks_2_resnets_0_norm1_weight[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640)], unet_down_blocks_2_resnets_0_norm1_bias[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv433[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_2_resnets_0_norm1_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640)] + unet_down_blocks_2_resnets_0_norm1_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640)]) * T.sigmoid((lv433[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) // T.int64(640) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(640) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20)] * T.float32(0.00019531250000000001)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_2_resnets_0_norm1_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640)] + unet_down_blocks_2_resnets_0_norm1_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(640) // T.int64(20) * T.int64(20) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(20)) % T.int64(640)])

    @T.prim_func
    def fused_group_norm6_silu5(lv445: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), unet_down_blocks_2_resnets_0_norm2_weight: T.Buffer((T.int64(1280),), "float32"), unet_down_blocks_2_resnets_0_norm2_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(320)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(32) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(lv445[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv445[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv445[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * lv445[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(256))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                        T.reads(lv445[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280), (v_ax3 // T.int64(16) + v_ax2) % T.int64(16), v_ax3 % T.int64(16)], A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)], A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)], unet_down_blocks_2_resnets_0_norm2_weight[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280)], unet_down_blocks_2_resnets_0_norm2_bias[((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv445[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_2_resnets_0_norm2_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280)] + unet_down_blocks_2_resnets_0_norm2_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280)]) * T.sigmoid((lv445[((((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) // T.int64(16) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(16) // T.int64(16) + (v_ax3 // T.int64(16) + v_ax2) % T.int64(16)) % T.int64(16), v_ax3 % T.int64(16) % T.int64(16)] - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05) - A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(9.7656250000000005e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_2_resnets_0_norm2_weight[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280)] + unet_down_blocks_2_resnets_0_norm2_bias[(((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(16) + v_ax2) // T.int64(16) + v_ax1) % T.int64(40)) % T.int64(1280)])

    @T.prim_func
    def fused_group_norm8_silu6(lv639: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), unet_down_blocks_3_resnets_0_norm1_weight: T.Buffer((T.int64(1280),), "float32"), unet_down_blocks_3_resnets_0_norm1_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) // T.int64(64))
                        v_k3 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) % T.int64(64) // T.int64(8))
                        v_k4 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) % T.int64(8))
                        T.reads(lv639[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv639[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv639[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)] * lv639[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(64))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64) // T.int64(8))
                        v_ax3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv639[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280), (v_ax3 // T.int64(8) + v_ax2) % T.int64(8), v_ax3 % T.int64(8)], A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)], A_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)], unet_down_blocks_3_resnets_0_norm1_weight[((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280)], unet_down_blocks_3_resnets_0_norm1_bias[((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv639[((((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) % T.int64(8), v_ax3 % T.int64(8) % T.int64(8)] - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002) - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002) * (A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_3_resnets_0_norm1_weight[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280)] + unet_down_blocks_3_resnets_0_norm1_bias[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280)]) * T.sigmoid((lv639[((((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) // T.int64(1280) + (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) // T.int64(8) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280), (v_ax3 % T.int64(8) // T.int64(8) + (v_ax3 // T.int64(8) + v_ax2) % T.int64(8)) % T.int64(8), v_ax3 % T.int64(8) % T.int64(8)] - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002) - A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002) * (A_red_temp_v0[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) // T.int64(1280) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40)] * T.float32(0.00039062500000000002)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_3_resnets_0_norm1_weight[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280)] + unet_down_blocks_3_resnets_0_norm1_bias[(((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(1280) // T.int64(40) * T.int64(40) + ((v_ax3 // T.int64(8) + v_ax2) // T.int64(8) + v_ax1) % T.int64(40)) % T.int64(1280)])

    @T.prim_func
    def fused_group_norm_silu1(lv24: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), unet_down_blocks_0_resnets_0_norm1_weight: T.Buffer((T.int64(320),), "float32"), unet_down_blocks_0_resnets_0_norm1_bias: T.Buffer((T.int64(320),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(10), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(lv24[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + lv24[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + lv24[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * lv24[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_multiply"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                        T.reads(lv24[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320), (v_ax3 // T.int64(64) + v_ax2) % T.int64(64), v_ax3 % T.int64(64)], A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)], A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)], unet_down_blocks_0_resnets_0_norm1_weight[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320)], unet_down_blocks_0_resnets_0_norm1_bias[((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320)])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = ((lv24[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) // T.int64(320) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05) - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_0_resnets_0_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320)] + unet_down_blocks_0_resnets_0_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320)]) * T.sigmoid((lv24[((((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) // T.int64(320) + (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2)) % T.int64(2), (((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) // T.int64(64) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320), (v_ax3 % T.int64(64) // T.int64(64) + (v_ax3 // T.int64(64) + v_ax2) % T.int64(64)) % T.int64(64), v_ax3 % T.int64(64) % T.int64(64)] - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05)) * T.rsqrt(A_red_temp_v1[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05) - A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05) * (A_red_temp_v0[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) // T.int64(320) + v_ax0) % T.int64(2), ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10)] * T.float32(2.4414062500000001e-05)) + T.float32(1.0000000000000001e-05)) * unet_down_blocks_0_resnets_0_norm1_weight[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320)] + unet_down_blocks_0_resnets_0_norm1_bias[(((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(320) // T.int64(10) * T.int64(10) + ((v_ax3 // T.int64(64) + v_ax2) // T.int64(64) + v_ax1) % T.int64(10)) % T.int64(320)])

    @T.prim_func
    def fused_matmul10_add5_add6(lv114: T.Buffer((T.int64(2), T.int64(4096), T.int64(1280)), "float32"), lv115: T.Buffer((T.int64(1280), T.int64(320)), "float32"), unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias: T.Buffer((T.int64(320),), "float32"), lv105: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="local")
        lv114_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(1280)), scope="shared")
        lv115_shared = T.alloc_buffer((T.int64(1280), T.int64(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(80) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(80) // T.int64(20) * T.int64(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv114_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(640))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(640) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.reads(lv114[v0, v1, v2])
                                        T.writes(lv114_shared[v0, v1, v2])
                                        lv114_shared[v0, v1, v2] = lv114[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv115_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(640) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(40))
                                        v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(40) + (ax0_ax1_fused_0 * T.int64(640) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(40))
                                        T.where((ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(1600))
                                        T.reads(lv115[v0, v1])
                                        T.writes(lv115_shared[v0, v1])
                                        lv115_shared[v0, v1] = lv115[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(40), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(80) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(80) // T.int64(20) * T.int64(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(40) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv114_shared[v_i0, v_i1, v_k], lv115_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv114_shared[v_i0, v_i1, v_k] * lv115_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(80) // T.int64(20) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias[v2], lv105[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias[v2] + lv105[v0, v1, v2]

    @T.prim_func
    def fused_matmul11_add10_strided_slice4(lv233: T.Buffer((T.int64(2), T.int64(1280)), "float32"), lv234: T.Buffer((T.int64(1280), T.int64(640)), "float32"), unet_down_blocks_1_resnets_0_time_emb_proj_bias: T.Buffer((T.int64(640),), "float32"), var_T_strided_slice_with_axes_intermediate: T.Buffer((T.int64(2), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(640)), scope="local")
        lv233_shared = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="shared")
        lv234_shared = T.alloc_buffer((T.int64(1280), T.int64(640)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(4) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(640), i0_0_i1_0_fused % T.int64(4) * T.int64(160) + i0_1_i1_1_fused * T.int64(80) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(40)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv233_shared"):
                                        v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(1280), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(32))
                                        T.reads(lv233[v0, v1])
                                        T.writes(lv233_shared[v0, v1])
                                        lv233_shared[v0, v1] = lv233[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv234_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(320) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(160))
                                        v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_fused % T.int64(4) * T.int64(160) + (ax0_ax1_fused_0 * T.int64(320) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(160))
                                        T.reads(lv234[v0, v1])
                                        T.writes(lv234_shared[v0, v1])
                                        lv234_shared[v0, v1] = lv234[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(4) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(640), i0_0_i1_0_fused % T.int64(4) * T.int64(160) + i0_1_i1_1_fused * T.int64(80) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(32) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1], lv233_shared[v_i0, v_k], lv234_shared[v_k, v_i1])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1] = var_matmul_intermediate_local[v_i0, v_i1] + lv233_shared[v_i0, v_k] * lv234_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(4) + ax0)
                            v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_fused % T.int64(4) * T.int64(160) + i0_1_i1_1_fused * T.int64(80) + i0_2_i1_2_fused + ax1)
                            T.reads(var_matmul_intermediate_local[v0, v1], unet_down_blocks_1_resnets_0_time_emb_proj_bias[v1])
                            T.writes(var_T_strided_slice_with_axes_intermediate[v0, v1])
                            var_T_strided_slice_with_axes_intermediate[v0, v1] = var_matmul_intermediate_local[v0, v1] + unet_down_blocks_1_resnets_0_time_emb_proj_bias[v1]

    @T.prim_func
    def fused_matmul12_add13_add14(lv279: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), lv280: T.Buffer((T.int64(640), T.int64(640)), "float32"), unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: T.Buffer((T.int64(640),), "float32"), lv255: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="local")
        lv279_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="shared")
        lv280_shared = T.alloc_buffer((T.int64(640), T.int64(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(5), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv279_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(512))
                                        v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(512) // T.int64(4))
                                        v2 = T.axis.spatial(T.int64(640), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(4))
                                        T.reads(lv279[v0, v1, v2])
                                        T.writes(lv279_shared[v0, v1, v2])
                                        lv279_shared[v0, v1, v2] = lv279[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv280_shared"):
                                        v0 = T.axis.spatial(T.int64(640), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(40))
                                        v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(40))
                                        T.where((ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(160))
                                        T.reads(lv280[v0, v1])
                                        T.writes(lv280_shared[v0, v1])
                                        lv280_shared[v0, v1] = lv280[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(5), T.int64(4), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(640), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv279_shared[v_i0, v_i1, v_k], lv280_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv279_shared[v_i0, v_i1, v_k] * lv280_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(2), T.int64(5)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2], lv255[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2] + lv255[v0, v1, v2]

    @T.prim_func
    def fused_matmul13_multiply6(lv265: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32"), lv272: T.Buffer((T.int64(16), T.int64(80), T.int64(1024)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(1024), T.int64(1024)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(1024)), scope="local")
        lv265_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="shared")
        lv272_shared = T.alloc_buffer((T.int64(16), T.int64(80), T.int64(1024)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2048) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(64) * T.int64(2) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(2048) // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(4) + i2_3_init * T.int64(4) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(5)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv265_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2048) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(2048) // T.int64(32) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(256) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(80), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(16))
                                        T.reads(lv265[v0, v1, v2])
                                        T.writes(lv265_shared[v0, v1, v2])
                                        lv265_shared[v0, v1, v2] = lv265[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("lv272_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2048) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(80), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(512) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.reads(lv272[v0, v1, v2])
                                    T.writes(lv272_shared[v0, v1, v2])
                                    lv272_shared[v0, v1, v2] = lv272[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2048) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(64) * T.int64(2) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(2048) // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(4) + i2_3 * T.int64(4) + i2_4)
                                v_k = T.axis.reduce(T.int64(80), k_0 * T.int64(16) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv265_shared[v_i0, v_i1, v_k], lv272_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv265_shared[v_i0, v_i1, v_k] * lv272_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(1), T.int64(4)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(2048) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(64) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(2048) // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(4) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.11180339753627777)

    @T.prim_func
    def fused_matmul16_multiply7(lv293: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32"), lv300: T.Buffer((T.int64(16), T.int64(80), T.int64(77)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(1024), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(77)), scope="local")
        lv293_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="shared")
        lv300_shared = T.alloc_buffer((T.int64(16), T.int64(80), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1408), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(88) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(88) // T.int64(11) * T.int64(128) + i0_1_i1_1_i2_1_fused // T.int64(7) * T.int64(64) + i0_2_i1_2_i2_2_fused + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_1_i1_1_i2_1_fused % T.int64(7) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(10)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv293_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(88))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(88) // T.int64(11) * T.int64(128) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(80), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(lv293[v0, v1, v2])
                                    T.writes(lv293_shared[v0, v1, v2])
                                    lv293_shared[v0, v1, v2] = lv293[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv300_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(88))
                                    v1 = T.axis.spatial(T.int64(80), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(7))
                                    v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 < T.int64(56))
                                    T.reads(lv300[v0, v1, v2])
                                    T.writes(lv300_shared[v0, v1, v2])
                                    lv300_shared[v0, v1, v2] = lv300[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(88) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(88) // T.int64(11) * T.int64(128) + i0_1_i1_1_i2_1_fused // T.int64(7) * T.int64(64) + i0_2_i1_2_i2_2_fused + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_1_i1_1_i2_1_fused % T.int64(7) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(80), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv293_shared[v_i0, v_i1, v_k], lv300_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv293_shared[v_i0, v_i1, v_k] * lv300_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(88) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(88) // T.int64(11) * T.int64(128) + i0_1_i1_1_i2_1_fused // T.int64(7) * T.int64(64) + i0_2_i1_2_i2_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_1_i1_1_i2_1_fused % T.int64(7) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.11180339753627777)

    @T.prim_func
    def fused_matmul18_add15_gelu1(lv312: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), lv316: T.Buffer((T.int64(640), T.int64(2560)), "float32"), unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias: T.Buffer((T.int64(2560),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1024), T.int64(2560)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(2560)))
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(2560)), scope="local")
        lv312_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="shared")
        lv316_shared = T.alloc_buffer((T.int64(640), T.int64(2560)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1024) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(1024) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(10) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(20)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv312_shared"):
                                        v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(1024) // T.int64(32) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(640) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(640), k_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(640) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(1024))
                                        T.reads(lv312[v0, v1, v2])
                                        T.writes(lv312_shared[v0, v1, v2])
                                        lv312_shared[v0, v1, v2] = lv312[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv316_shared"):
                                        v0 = T.axis.spatial(T.int64(640), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(320) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(80))
                                        v1 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(80) + (ax0_ax1_fused_0 * T.int64(320) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(80))
                                        T.reads(lv316[v0, v1])
                                        T.writes(lv316_shared[v0, v1])
                                        lv316_shared[v0, v1] = lv316[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1024) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(1024) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(10) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(640), k_0 * T.int64(32) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv312_shared[v_i0, v_i1, v_k], lv316_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv312_shared[v_i0, v_i1, v_k] * lv316_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(1024) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(10) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_matmul_intermediate[v0, v1, v2])
                            var_matmul_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(80)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(2621440))
                        v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(2621440) // T.int64(2560))
                        v_ax2 = T.axis.spatial(T.int64(2560), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(2560))
                        T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = (var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * (T.float32(0.5) + T.erf((var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul18_add15_multiply8(lv312: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), lv313: T.Buffer((T.int64(640), T.int64(2560)), "float32"), unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias: T.Buffer((T.int64(2560),), "float32"), lv319: T.Buffer((T.int64(2), T.int64(1024), T.int64(2560)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1024), T.int64(2560)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(2560)), scope="local")
        lv312_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="shared")
        lv313_shared = T.alloc_buffer((T.int64(640), T.int64(2560)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(20) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(160) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(8) + i2_3_init * T.int64(8) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("lv312_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(320) // T.int64(20))
                                    v2 = T.axis.spatial(T.int64(640), k_0 * T.int64(20) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(20))
                                    T.reads(lv312[v0, v1, v2])
                                    T.writes(lv312_shared[v0, v1, v2])
                                    lv312_shared[v0, v1, v2] = lv312[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(20)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("lv313_shared"):
                                    v0 = T.axis.spatial(T.int64(640), k_0 * T.int64(20) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(160) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) % T.int64(160))
                                    T.reads(lv313[v0, v1])
                                    T.writes(lv313_shared[v0, v1])
                                    lv313_shared[v0, v1] = lv313[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(20), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(20) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(160) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(8) + i2_3 * T.int64(8) + i2_4)
                                v_k = T.axis.reduce(T.int64(640), k_0 * T.int64(20) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv312_shared[v_i0, v_i1, v_k], lv313_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv312_shared[v_i0, v_i1, v_k] * lv313_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(1), T.int64(8)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(20) + ax1)
                            v2 = T.axis.spatial(T.int64(2560), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(160) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(8) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2], lv319[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = (var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2]) * lv319[v0, v1, v2]

    @T.prim_func
    def fused_matmul19_add13_add14(lv320: T.Buffer((T.int64(2), T.int64(1024), T.int64(2560)), "float32"), lv321: T.Buffer((T.int64(2560), T.int64(640)), "float32"), unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias: T.Buffer((T.int64(640),), "float32"), lv311: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="local")
        lv320_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(2560)), scope="shared")
        lv321_shared = T.alloc_buffer((T.int64(2560), T.int64(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(8) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused % T.int64(8) // T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(128) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(1280)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("lv320_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(64) // T.int64(2))
                                    v2 = T.axis.spatial(T.int64(2560), k_0 * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(2))
                                    T.reads(lv320[v0, v1, v2])
                                    T.writes(lv320_shared[v0, v1, v2])
                                    lv320_shared[v0, v1, v2] = lv320[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("lv321_shared"):
                                    v0 = T.axis.spatial(T.int64(2560), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(lv321[v0, v1])
                                    T.writes(lv321_shared[v0, v1])
                                    lv321_shared[v0, v1] = lv321[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(8) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused % T.int64(8) // T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(128) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(2560), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv320_shared[v_i0, v_i1, v_k], lv321_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv320_shared[v_i0, v_i1, v_k] * lv321_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(8) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused % T.int64(8) // T.int64(4) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(128) + i0_1_i1_1_i2_1_fused % T.int64(4) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias[v2], lv311[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias[v2] + lv311[v0, v1, v2]

    @T.prim_func
    def fused_matmul1_add(lv18: T.Buffer((T.int64(2), T.int64(1280)), "float32"), lv19: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), unet_time_embedding_linear_2_bias: T.Buffer((T.int64(1280),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="local")
        lv18_shared = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="shared")
        lv19_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("lv18_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1280), k_0 * T.int64(80) + ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1)
                                    T.reads(lv18[v0, v1])
                                    T.writes(lv18_shared[v0, v1])
                                    lv18_shared[v0, v1] = lv18[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(40)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv19_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(80) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(80))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(80))
                                        T.reads(lv19[v0, v1])
                                        T.writes(lv19_shared[v0, v1])
                                        lv19_shared[v0, v1] = lv19[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(80), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(80) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1], lv18_shared[v_i0, v_k], lv19_shared[v_k, v_i1])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1] = var_matmul_intermediate_local[v_i0, v_i1] + lv18_shared[v_i0, v_k] * lv19_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + i0_2_i1_2_fused + ax1)
                            T.reads(var_matmul_intermediate_local[v0, v1], unet_time_embedding_linear_2_bias[v1])
                            T.writes(var_T_add_intermediate[v0, v1])
                            var_T_add_intermediate[v0, v1] = var_matmul_intermediate_local[v0, v1] + unet_time_embedding_linear_2_bias[v1]

    @T.prim_func
    def fused_matmul1_add_strided_slice5(lv439: T.Buffer((T.int64(2), T.int64(1280)), "float32"), lv440: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), unet_down_blocks_2_resnets_0_time_emb_proj_bias: T.Buffer((T.int64(1280),), "float32"), var_T_strided_slice_with_axes_intermediate: T.Buffer((T.int64(2), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="local")
        lv439_shared = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="shared")
        lv440_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("lv439_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1280), k_0 * T.int64(40) + ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1)
                                    T.reads(lv439[v0, v1])
                                    T.writes(lv439_shared[v0, v1])
                                    lv439_shared[v0, v1] = lv439[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(20)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv440_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(80))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(80))
                                        T.reads(lv440[v0, v1])
                                        T.writes(lv440_shared[v0, v1])
                                        lv440_shared[v0, v1] = lv440[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(5), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(40) + k_1 * T.int64(5) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1], lv439_shared[v_i0, v_k], lv440_shared[v_k, v_i1])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1] = var_matmul_intermediate_local[v_i0, v_i1] + lv439_shared[v_i0, v_k] * lv440_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused % T.int64(16) * T.int64(80) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(var_matmul_intermediate_local[v0, v1], unet_down_blocks_2_resnets_0_time_emb_proj_bias[v1])
                            T.writes(var_T_strided_slice_with_axes_intermediate[v0, v1])
                            var_T_strided_slice_with_axes_intermediate[v0, v1] = var_matmul_intermediate_local[v0, v1] + unet_down_blocks_2_resnets_0_time_emb_proj_bias[v1]

    @T.prim_func
    def fused_matmul20_add20_add21(lv485: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), lv486: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: T.Buffer((T.int64(1280),), "float32"), lv461: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="local")
        lv485_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="shared")
        lv486_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv485_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(1280))
                                        v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(128) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(1280) // T.int64(10))
                                        v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(10) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(10))
                                        T.reads(lv485[v0, v1, v2])
                                        T.writes(lv485_shared[v0, v1, v2])
                                        lv485_shared[v0, v1, v2] = lv485[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv486_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(10) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(16))
                                        T.where((ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(160))
                                        T.reads(lv486[v0, v1])
                                        T.writes(lv486_shared[v0, v1])
                                        lv486_shared[v0, v1] = lv486[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(5), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(10) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv485_shared[v_i0, v_i1, v_k], lv486_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv485_shared[v_i0, v_i1, v_k] * lv486_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2], lv461[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2] + lv461[v0, v1, v2]

    @T.prim_func
    def fused_matmul21_multiply9(lv471: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32"), lv478: T.Buffer((T.int64(16), T.int64(160), T.int64(256)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(256)), scope="local")
        lv471_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="shared")
        lv478_shared = T.alloc_buffer((T.int64(16), T.int64(160), T.int64(256)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(8) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(10)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv471_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(8) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(1024) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(160), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(16))
                                        T.reads(lv471[v0, v1, v2])
                                        T.writes(lv471_shared[v0, v1, v2])
                                        lv471_shared[v0, v1, v2] = lv471[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("lv478_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(160), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(512) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.reads(lv478[v0, v1, v2])
                                    T.writes(lv478_shared[v0, v1, v2])
                                    lv478_shared[v0, v1, v2] = lv478[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(16), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(8) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(160), k_0 * T.int64(16) + k_1 * T.int64(16) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv471_shared[v_i0, v_i1, v_k], lv478_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv471_shared[v_i0, v_i1, v_k] * lv478_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(4)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(8) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul24_multiply10(lv499: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32"), lv506: T.Buffer((T.int64(16), T.int64(160), T.int64(77)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(256), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(77)), scope="local")
        lv499_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="shared")
        lv506_shared = T.alloc_buffer((T.int64(16), T.int64(160), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(112) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(44) * T.int64(4) + i0_3_init * T.int64(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(112) // T.int64(7) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(44) // T.int64(11) * T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(40)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv499_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(112) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(352) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(112) // T.int64(7) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(352) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(64) // T.int64(4))
                                        v2 = T.axis.spatial(T.int64(160), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(352) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(4))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(512))
                                        T.reads(lv499[v0, v1, v2])
                                        T.writes(lv499_shared[v0, v1, v2])
                                        lv499_shared[v0, v1, v2] = lv499[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv506_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(112) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(44))
                                        v1 = T.axis.spatial(T.int64(160), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(44) // T.int64(11))
                                        v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(11))
                                        T.reads(lv506[v0, v1, v2])
                                        T.writes(lv506_shared[v0, v1, v2])
                                        lv506_shared[v0, v1, v2] = lv506[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(2), T.int64(4), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(112) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(44) * T.int64(4) + i0_3 * T.int64(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(112) // T.int64(7) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(44) // T.int64(11) * T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(160), k_0 * T.int64(4) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv499_shared[v_i0, v_i1, v_k], lv506_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv499_shared[v_i0, v_i1, v_k] * lv506_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(4), T.int64(4), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(112) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(44) * T.int64(4) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(112) // T.int64(7) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(44) // T.int64(11) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul26_add22_gelu2(lv518: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), lv522: T.Buffer((T.int64(1280), T.int64(5120)), "float32"), unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias: T.Buffer((T.int64(5120),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(256), T.int64(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(5120)))
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(5120)), scope="local")
        lv518_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="shared")
        lv522_shared = T.alloc_buffer((T.int64(1280), T.int64(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init * T.int64(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv518_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(1024) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(16))
                                        T.reads(lv518[v0, v1, v2])
                                        T.writes(lv518_shared[v0, v1, v2])
                                        lv518_shared[v0, v1, v2] = lv518[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv522_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(lv522[v0, v1])
                                        T.writes(lv522_shared[v0, v1])
                                        lv522_shared[v0, v1] = lv522[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(4), T.int64(2), T.int64(4), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 * T.int64(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(16) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv518_shared[v_i0, v_i1, v_k], lv522_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv518_shared[v_i0, v_i1, v_k] * lv522_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(8), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_matmul_intermediate[v0, v1, v2])
                            var_matmul_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(40)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1310720) // T.int64(5120))
                        v_ax2 = T.axis.spatial(T.int64(5120), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(5120))
                        T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = (var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * (T.float32(0.5) + T.erf((var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul26_add22_multiply11(lv518: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), lv519: T.Buffer((T.int64(1280), T.int64(5120)), "float32"), unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias: T.Buffer((T.int64(5120),), "float32"), lv525: T.Buffer((T.int64(2), T.int64(256), T.int64(5120)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(256), T.int64(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(5120)), scope="local")
        lv518_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="shared")
        lv519_shared = T.alloc_buffer((T.int64(1280), T.int64(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2560), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(32) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(8) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(8) + i2_3_init * T.int64(8) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv518_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(128) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(lv518[v0, v1, v2])
                                    T.writes(lv518_shared[v0, v1, v2])
                                    lv518_shared[v0, v1, v2] = lv518[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv519_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(lv519[v0, v1])
                                        T.writes(lv519_shared[v0, v1])
                                        lv519_shared[v0, v1] = lv519[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(32) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(8) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(8) + i2_3 * T.int64(8) + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(16) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv518_shared[v_i0, v_i1, v_k], lv519_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv518_shared[v_i0, v_i1, v_k] * lv519_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(8)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(8) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2], lv525[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = (var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2]) * lv525[v0, v1, v2]

    @T.prim_func
    def fused_matmul27_add20_add21(lv526: T.Buffer((T.int64(2), T.int64(256), T.int64(5120)), "float32"), lv527: T.Buffer((T.int64(5120), T.int64(1280)), "float32"), unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias: T.Buffer((T.int64(1280),), "float32"), lv517: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="local")
        lv526_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(5120)), scope="shared")
        lv527_shared = T.alloc_buffer((T.int64(5120), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(320) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(8) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(1280)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv526_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(40) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(5120), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.reads(lv526[v0, v1, v2])
                                    T.writes(lv526_shared[v0, v1, v2])
                                    lv526_shared[v0, v1, v2] = lv526[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv527_shared"):
                                        v0 = T.axis.spatial(T.int64(5120), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(128))
                                        T.reads(lv527[v0, v1])
                                        T.writes(lv527_shared[v0, v1])
                                        lv527_shared[v0, v1] = lv527[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(320) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(8) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(5120), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv526_shared[v_i0, v_i1, v_k], lv527_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv526_shared[v_i0, v_i1, v_k] * lv527_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(320) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(320) // T.int64(40) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(40) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(8) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias[v2], lv517[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias[v2] + lv517[v0, v1, v2]

    @T.prim_func
    def fused_matmul28_add26_add27(lv726: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), lv727: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: T.Buffer((T.int64(1280),), "float32"), lv702: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="local")
        lv726_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="shared")
        lv727_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(128) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(5) * T.int64(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(5) * T.int64(8) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv726_shared"):
                                        v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(32) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(40) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(128))
                                        T.reads(lv726[v0, v1, v2])
                                        T.writes(lv726_shared[v0, v1, v2])
                                        lv726_shared[v0, v1, v2] = lv726[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv727_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(40))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(40))
                                        T.reads(lv727[v0, v1])
                                        T.writes(lv727_shared[v0, v1])
                                        lv727_shared[v0, v1] = lv727[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(2), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(128) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(5) * T.int64(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(5) * T.int64(8) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(8) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv726_shared[v_i0, v_i1, v_k], lv727_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv726_shared[v_i0, v_i1, v_k] * lv727_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(8)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(5) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(5) * T.int64(8) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2], lv702[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2] + lv702[v0, v1, v2]

    @T.prim_func
    def fused_matmul29_multiply12(lv712: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32"), lv719: T.Buffer((T.int64(16), T.int64(160), T.int64(64)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(64)), scope="local")
        lv712_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="shared")
        lv719_shared = T.alloc_buffer((T.int64(16), T.int64(160), T.int64(64)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(32) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) // T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(20)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv712_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(640))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) // T.int64(2) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(640) // T.int64(40))
                                    v2 = T.axis.spatial(T.int64(160), k_0 * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(40))
                                    T.reads(lv712[v0, v1, v2])
                                    T.writes(lv712_shared[v0, v1, v2])
                                    lv712_shared[v0, v1, v2] = lv712[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(40)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv719_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(1280))
                                    v1 = T.axis.spatial(T.int64(160), k_0 * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(1280) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.reads(lv719[v0, v1, v2])
                                    T.writes(lv719_shared[v0, v1, v2])
                                    lv719_shared[v0, v1, v2] = lv719[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(40), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(32) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) // T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(160), k_0 * T.int64(40) + k_1 * T.int64(40) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv712_shared[v_i0, v_i1, v_k], lv719_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv712_shared[v_i0, v_i1, v_k] * lv719_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) // T.int64(2) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul2_add2_strided_slice3(lv30: T.Buffer((T.int64(2), T.int64(1280)), "float32"), lv31: T.Buffer((T.int64(1280), T.int64(320)), "float32"), unet_down_blocks_0_resnets_0_time_emb_proj_bias: T.Buffer((T.int64(320),), "float32"), var_T_strided_slice_with_axes_intermediate: T.Buffer((T.int64(2), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(320)), scope="local")
        lv30_shared = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="shared")
        lv31_shared = T.alloc_buffer((T.int64(1280), T.int64(320)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(20) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(320), i0_0_i1_0_fused * T.int64(20) + i0_2_i1_2_fused % T.int64(20) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(8)):
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("lv30_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(1280), k_0 * T.int64(160) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) % T.int64(160))
                                    T.reads(lv30[v0, v1])
                                    T.writes(lv30_shared[v0, v1])
                                    lv30_shared[v0, v1] = lv30[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(20)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv31_shared"):
                                        v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(160) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(20))
                                        v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_fused * T.int64(20) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(20))
                                        T.reads(lv31[v0, v1])
                                        T.writes(lv31_shared[v0, v1])
                                        lv31_shared[v0, v1] = lv31[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(10), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(20) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(320), i0_0_i1_0_fused * T.int64(20) + i0_2_i1_2_fused % T.int64(20) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(160) + k_1 * T.int64(16) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1], lv30_shared[v_i0, v_k], lv31_shared[v_k, v_i1])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1] = var_matmul_intermediate_local[v_i0, v_i1] + lv30_shared[v_i0, v_k] * lv31_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_fused // T.int64(20) + ax0)
                            v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_fused * T.int64(20) + i0_2_i1_2_fused % T.int64(20) + ax1)
                            T.reads(var_matmul_intermediate_local[v0, v1], unet_down_blocks_0_resnets_0_time_emb_proj_bias[v1])
                            T.writes(var_T_strided_slice_with_axes_intermediate[v0, v1])
                            var_T_strided_slice_with_axes_intermediate[v0, v1] = var_matmul_intermediate_local[v0, v1] + unet_down_blocks_0_resnets_0_time_emb_proj_bias[v1]

    @T.prim_func
    def fused_matmul31_multiply13(lv740: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32"), lv747: T.Buffer((T.int64(16), T.int64(160), T.int64(77)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(64), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(77)), scope="local")
        lv740_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="shared")
        lv747_shared = T.alloc_buffer((T.int64(16), T.int64(160), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(112), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(44), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(7) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(11) * T.int64(16) + i1_3_init * T.int64(4) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(20)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(44), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv740_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(7))
                                        v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(160), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(44) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(512))
                                        T.reads(lv740[v0, v1, v2])
                                        T.writes(lv740_shared[v0, v1, v2])
                                        lv740_shared[v0, v1, v2] = lv740[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(44), thread="threadIdx.x"):
                                with T.block("lv747_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(7))
                                    v1 = T.axis.spatial(T.int64(160), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(44) + ax0_ax1_ax2_fused_1) // T.int64(11))
                                    v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(44) + ax0_ax1_ax2_fused_1) % T.int64(11))
                                    T.reads(lv747[v0, v1, v2])
                                    T.writes(lv747_shared[v0, v1, v2])
                                    lv747_shared[v0, v1, v2] = lv747[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(7) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(11) * T.int64(16) + i1_3 * T.int64(4) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(160), k_0 * T.int64(8) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv740_shared[v_i0, v_i1, v_k], lv747_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv740_shared[v_i0, v_i1, v_k] * lv747_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(16), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(7) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(11) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(7) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(11) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.079056940972805023)

    @T.prim_func
    def fused_matmul33_add28_gelu3(lv759: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), lv763: T.Buffer((T.int64(1280), T.int64(5120)), "float32"), unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias: T.Buffer((T.int64(5120),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(64), T.int64(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(5120)))
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(5120)), scope="local")
        lv759_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="shared")
        lv763_shared = T.alloc_buffer((T.int64(1280), T.int64(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(16) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(160)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv759_shared"):
                                        v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(512))
                                        T.reads(lv759[v0, v1, v2])
                                        T.writes(lv759_shared[v0, v1, v2])
                                        lv759_shared[v0, v1, v2] = lv759[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("lv763_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1) // T.int64(80))
                                    v1 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(80) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1) % T.int64(80))
                                    T.reads(lv763[v0, v1])
                                    T.writes(lv763_shared[v0, v1])
                                    lv763_shared[v0, v1] = lv763[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(16), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(16) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(8) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv759_shared[v_i0, v_i1, v_k], lv763_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv759_shared[v_i0, v_i1, v_k] * lv763_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(16), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(64) * T.int64(80) + i0_1_i1_1_i2_1_fused * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_matmul_intermediate[v0, v1, v2])
                            var_matmul_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(10)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(327680) // T.int64(5120))
                        v_ax2 = T.axis.spatial(T.int64(5120), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(5120))
                        T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = (var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * (T.float32(0.5) + T.erf((var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul33_add28_multiply14(lv759: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), lv760: T.Buffer((T.int64(1280), T.int64(5120)), "float32"), unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias: T.Buffer((T.int64(5120),), "float32"), lv766: T.Buffer((T.int64(2), T.int64(64), T.int64(5120)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(64), T.int64(5120)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(5120)), scope="local")
        lv759_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="shared")
        lv760_shared = T.alloc_buffer((T.int64(1280), T.int64(5120)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(4) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(320) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(4) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(640)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("lv759_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(64) // T.int64(2))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(2))
                                    T.reads(lv759[v0, v1, v2])
                                    T.writes(lv759_shared[v0, v1, v2])
                                    lv759_shared[v0, v1, v2] = lv759[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("lv760_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(320) * T.int64(16) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(16))
                                    T.reads(lv760[v0, v1])
                                    T.writes(lv760_shared[v0, v1])
                                    lv760_shared[v0, v1] = lv760[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(4) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(320) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(4) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv759_shared[v_i0, v_i1, v_k], lv760_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv759_shared[v_i0, v_i1, v_k] * lv760_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(320) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(5120), i0_0_i1_0_i2_0_fused % T.int64(320) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(4) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2], lv766[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = (var_matmul_intermediate_local[v0, v1, v2] + unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2]) * lv766[v0, v1, v2]

    @T.prim_func
    def fused_matmul34_add26_add27(lv767: T.Buffer((T.int64(2), T.int64(64), T.int64(5120)), "float32"), lv768: T.Buffer((T.int64(5120), T.int64(1280)), "float32"), unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias: T.Buffer((T.int64(1280),), "float32"), lv758: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="local")
        lv767_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(5120)), scope="shared")
        lv768_shared = T.alloc_buffer((T.int64(5120), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv767_shared"):
                                        v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(32) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(640) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(5120), k_0 * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(640) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.reads(lv767[v0, v1, v2])
                                        T.writes(lv767_shared[v0, v1, v2])
                                        lv767_shared[v0, v1, v2] = lv767[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv768_shared"):
                                        v0 = T.axis.spatial(T.int64(5120), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(640) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(40))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + (ax0_ax1_fused_0 * T.int64(640) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(40))
                                        T.where((ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(1600))
                                        T.reads(lv768[v0, v1])
                                        T.writes(lv768_shared[v0, v1])
                                        lv768_shared[v0, v1] = lv768[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(40), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(5120), k_0 * T.int64(40) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv767_shared[v_i0, v_i1, v_k], lv768_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv767_shared[v_i0, v_i1, v_k] * lv768_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(32) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(32) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(20) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias[v2], lv758[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias[v2] + lv758[v0, v1, v2]

    @T.prim_func
    def fused_matmul35_add35(lv21: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), lv26: T.Buffer((T.int64(768), T.int64(768)), "float32"), self_clip_text_model_encoder_layers_0_self_attn_k_proj_bias: T.Buffer((T.int64(768),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="local")
        lv21_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="shared")
        lv26_shared = T.alloc_buffer((T.int64(768), T.int64(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(231), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(3) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(24) + i0_1_i1_1_i2_1_fused * T.int64(3) + i0_2_i1_2_i2_2_fused % T.int64(3) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(48)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(231), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv21_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(462) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(462) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(16))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(231) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(1232))
                                        T.reads(lv21[v0, v1, v2])
                                        T.writes(lv21_shared[v0, v1, v2])
                                        lv21_shared[v0, v1, v2] = lv21[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(231), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv26_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(924) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(24) + (ax0_ax1_fused_0 * T.int64(924) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(24))
                                        T.where((ax0_ax1_fused_0 * T.int64(231) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(384))
                                        T.reads(lv26[v0, v1])
                                        T.writes(lv26_shared[v0, v1])
                                        lv26_shared[v0, v1] = lv26[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(3) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(24) + i0_1_i1_1_i2_1_fused * T.int64(3) + i0_2_i1_2_i2_2_fused % T.int64(3) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(16) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv21_shared[v_i0, v_i1, v_k], lv26_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv21_shared[v_i0, v_i1, v_k] * lv26_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused // T.int64(3) + ax1)
                            v2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused * T.int64(24) + i0_1_i1_1_i2_1_fused * T.int64(3) + i0_2_i1_2_i2_2_fused % T.int64(3) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], self_clip_text_model_encoder_layers_0_self_attn_k_proj_bias[v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + self_clip_text_model_encoder_layers_0_self_attn_k_proj_bias[v2]

    @T.prim_func
    def fused_matmul35_add35_add33(lv50: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), lv51: T.Buffer((T.int64(768), T.int64(768)), "float32"), self_clip_text_model_encoder_layers_0_self_attn_out_proj_bias: T.Buffer((T.int64(768),), "float32"), lv9: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="local")
        lv50_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="shared")
        lv51_shared = T.alloc_buffer((T.int64(768), T.int64(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(264), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(24) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(24) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv50_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(24) * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(24))
                                        v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(24) + (ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(24))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(56) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(168))
                                        T.reads(lv50[v0, v1, v2])
                                        T.writes(lv50_shared[v0, v1, v2])
                                        lv50_shared[v0, v1, v2] = lv50[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(5)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("lv51_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(24) + (ax0_ax1_fused_0 * T.int64(168) + ax0_ax1_fused_1 * T.int64(3) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(24) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(168) + ax0_ax1_fused_1 * T.int64(3) + ax0_ax1_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_fused_0 * T.int64(56) + ax0_ax1_fused_1) * T.int64(3) + ax0_ax1_fused_2 < T.int64(768))
                                        T.reads(lv51[v0, v1])
                                        T.writes(lv51_shared[v0, v1])
                                        lv51_shared[v0, v1] = lv51[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(24) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(24) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(24) + k_1 * T.int64(8) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv50_shared[v_i0, v_i1, v_k], lv51_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv50_shared[v_i0, v_i1, v_k] * lv51_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(24) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(24) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + ax2)
                            T.reads(lv9[v0, v1, v2], var_matmul_intermediate_local[v0, v1, v2], self_clip_text_model_encoder_layers_0_self_attn_out_proj_bias[v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = lv9[v0, v1, v2] + (var_matmul_intermediate_local[v0, v1, v2] + self_clip_text_model_encoder_layers_0_self_attn_out_proj_bias[v2])

    @T.prim_func
    def fused_matmul35_add35_multiply21(lv21: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), lv22: T.Buffer((T.int64(768), T.int64(768)), "float32"), self_clip_text_model_encoder_layers_0_self_attn_q_proj_bias: T.Buffer((T.int64(768),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="local")
        lv21_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="shared")
        lv22_shared = T.alloc_buffer((T.int64(768), T.int64(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(84), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(12) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(16) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(12) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                                with T.block("lv21_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(12) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) // T.int64(12))
                                    v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(12) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1) % T.int64(12))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 < T.int64(132))
                                    T.reads(lv21[v0, v1, v2])
                                    T.writes(lv21_shared[v0, v1, v2])
                                    lv21_shared[v0, v1, v2] = lv21[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(176), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv22_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(12) + (ax0_ax1_fused_0 * T.int64(704) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(12) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(704) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.where((ax0_ax1_fused_0 * T.int64(176) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(768))
                                        T.reads(lv22[v0, v1])
                                        T.writes(lv22_shared[v0, v1])
                                        lv22_shared[v0, v1] = lv22[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(12), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(12) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(16) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(12) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(12) + k_1 * T.int64(12) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv21_shared[v_i0, v_i1, v_k], lv22_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv21_shared[v_i0, v_i1, v_k] * lv22_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(12) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(12) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], self_clip_text_model_encoder_layers_0_self_attn_q_proj_bias[v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = (var_matmul_intermediate_local[v0, v1, v2] + self_clip_text_model_encoder_layers_0_self_attn_q_proj_bias[v2]) * T.float32(0.125)

    @T.prim_func
    def fused_matmul38_add37_multiply22_tir_sigmoid_multiply23(lv55: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), lv56: T.Buffer((T.int64(768), T.int64(3072)), "float32"), self_clip_text_model_encoder_layers_0_mlp_fc1_bias: T.Buffer((T.int64(3072),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(3072)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(3072)))
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(3072)), scope="local")
        lv55_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="shared")
        lv56_shared = T.alloc_buffer((T.int64(768), T.int64(3072)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(528), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(7), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(48) * T.int64(7) + i1_3_init * T.int64(7) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(3072), i0_0_i1_0_i2_0_fused % T.int64(48) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv55_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(48) * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(12))
                                        v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(12) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(12))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(84))
                                        T.reads(lv55[v0, v1, v2])
                                        T.writes(lv55_shared[v0, v1, v2])
                                        lv55_shared[v0, v1, v2] = lv55[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(6)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv56_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(12) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(3072), i0_0_i1_0_i2_0_fused % T.int64(48) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(lv56[v0, v1])
                                        T.writes(lv56_shared[v0, v1])
                                        lv56_shared[v0, v1] = lv56[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(3), T.int64(1), T.int64(7), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(48) * T.int64(7) + i1_3 * T.int64(7) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(3072), i0_0_i1_0_i2_0_fused % T.int64(48) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(12) + k_1 * T.int64(3) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv55_shared[v_i0, v_i1, v_k], lv56_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv55_shared[v_i0, v_i1, v_k] * lv56_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(7), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(48) * T.int64(7) + ax1)
                            v2 = T.axis.spatial(T.int64(3072), i0_0_i1_0_i2_0_fused % T.int64(48) * T.int64(64) + i0_2_i1_2_i2_2_fused * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_matmul_intermediate[v0, v1, v2])
                            var_matmul_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                    with T.block("T_multiply_1"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(3072))
                        v_ax2 = T.axis.spatial(T.int64(3072), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(3072))
                        T.where((ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) * T.int64(256) + ax0_ax1_ax2_fused_2 < T.int64(236544))
                        T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], self_clip_text_model_encoder_layers_0_mlp_fc1_bias[v_ax2])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = (var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_mlp_fc1_bias[v_ax2]) * T.sigmoid(T.float32(1.7020000219345093) * (var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + self_clip_text_model_encoder_layers_0_mlp_fc1_bias[v_ax2]))

    @T.prim_func
    def fused_matmul39_add35_add33(lv61: T.Buffer((T.int64(1), T.int64(77), T.int64(3072)), "float32"), lv62: T.Buffer((T.int64(3072), T.int64(768)), "float32"), self_clip_text_model_encoder_layers_0_mlp_fc2_bias: T.Buffer((T.int64(768),), "float32"), lv54: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(768)), scope="local")
        lv61_shared = T.alloc_buffer((T.int64(1), T.int64(77), T.int64(3072)), scope="shared")
        lv62_shared = T.alloc_buffer((T.int64(3072), T.int64(768)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(462), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(6) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(6) * T.int64(128) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(384)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv61_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(6))
                                        v2 = T.axis.spatial(T.int64(3072), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(8))
                                        T.reads(lv61[v0, v1, v2])
                                        T.writes(lv61_shared[v0, v1, v2])
                                        lv61_shared[v0, v1, v2] = lv61[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv62_shared"):
                                    v0 = T.axis.spatial(T.int64(3072), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(6) * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(lv62[v0, v1])
                                    T.writes(lv62_shared[v0, v1])
                                    lv62_shared[v0, v1] = lv62[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(6) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(6) * T.int64(128) + i0_2_i1_2_i2_2_fused * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(3072), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv61_shared[v_i0, v_i1, v_k], lv62_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv61_shared[v_i0, v_i1, v_k] * lv62_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(6) + ax1)
                            v2 = T.axis.spatial(T.int64(768), i0_0_i1_0_i2_0_fused % T.int64(6) * T.int64(128) + i0_2_i1_2_i2_2_fused * T.int64(2) + ax2)
                            T.reads(lv54[v0, v1, v2], var_matmul_intermediate_local[v0, v1, v2], self_clip_text_model_encoder_layers_0_mlp_fc2_bias[v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = lv54[v0, v1, v2] + (var_matmul_intermediate_local[v0, v1, v2] + self_clip_text_model_encoder_layers_0_mlp_fc2_bias[v2])

    @T.prim_func
    def fused_matmul3_add5_add6(lv73: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), lv74: T.Buffer((T.int64(320), T.int64(320)), "float32"), unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias: T.Buffer((T.int64(320),), "float32"), lv49: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="local")
        lv73_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="shared")
        lv74_shared = T.alloc_buffer((T.int64(320), T.int64(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2560), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1280) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(1280) // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3_init * T.int64(4) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(20)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("lv73_shared"):
                                    v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1280))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(1280) // T.int64(10) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(320), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(lv73[v0, v1, v2])
                                    T.writes(lv73_shared[v0, v1, v2])
                                    lv73_shared[v0, v1, v2] = lv73[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv74_shared"):
                                        v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(lv74[v0, v1])
                                        T.writes(lv74_shared[v0, v1])
                                        lv74_shared[v0, v1] = lv74[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(8), T.int64(1), T.int64(4), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1280) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(1280) // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + i1_3 * T.int64(4) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(16) + k_1 * T.int64(8) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv73_shared[v_i0, v_i1, v_k], lv74_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv73_shared[v_i0, v_i1, v_k] * lv74_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_0_i1_0_i2_0_fused // T.int64(1280) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(1280) // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2], lv49[v0, v1, v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias[v2] + lv49[v0, v1, v2]

    @T.prim_func
    def fused_matmul40_add41(lv23: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), lv24: T.Buffer((T.int64(512), T.int64(512)), "float32"), vae_decoder_mid_block_attentions_0_to_q_bias: T.Buffer((T.int64(512),), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(512)), scope="local")
        lv23_shared = T.alloc_buffer((T.int64(1), T.int64(4096), T.int64(512)), scope="shared")
        lv24_shared = T.alloc_buffer((T.int64(512), T.int64(512)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + i1_3_init * T.int64(4) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv23_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(512), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.reads(lv23[v0, v1, v2])
                                        T.writes(lv23_shared[v0, v1, v2])
                                        lv23_shared[v0, v1, v2] = lv23[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv24_shared"):
                                        v0 = T.axis.spatial(T.int64(512), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(lv24[v0, v1])
                                        T.writes(lv24_shared[v0, v1])
                                        lv24_shared[v0, v1] = lv24[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(2), T.int64(4), T.int64(2), T.int64(1), T.int64(4), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + i1_3 * T.int64(4) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(8) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv23_shared[v_i0, v_i1, v_k], lv24_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv23_shared[v_i0, v_i1, v_k] * lv24_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(4)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_fused * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(4) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], vae_decoder_mid_block_attentions_0_to_q_bias[v2])
                            T.writes(var_T_add_intermediate[v0, v1, v2])
                            var_T_add_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] + vae_decoder_mid_block_attentions_0_to_q_bias[v2]

    @T.prim_func
    def fused_matmul41_multiply29(lv34: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32"), lv41: T.Buffer((T.int64(1), T.int64(1), T.int64(512), T.int64(4096)), "float32"), param_0: T.Buffer((), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), scope="local")
        lv34_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), scope="shared")
        lv41_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(512), T.int64(4096)), scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i3_3_init, i0_4_init, i1_4_init, i2_4_init, i3_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1), i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_i3_2_fused // T.int64(8) * T.int64(4) + i2_3_init + i2_4_init)
                            v_i3 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused % T.int64(32) * T.int64(128) + i0_1_i1_1_i2_1_i3_1_fused * T.int64(64) + i0_2_i1_2_i2_2_i3_2_fused % T.int64(8) * T.int64(8) + i3_3_init * T.int64(4) + i3_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2, v_i3])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv34_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(32))
                                        T.reads(lv34[v0, v1, v2, v3])
                                        T.writes(lv34_shared[v0, v1, v2, v3])
                                        lv34_shared[v0, v1, v2, v3] = lv34[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv41_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v3 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused % T.int64(32) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        T.reads(lv41[v0, v1, v2, v3])
                                        T.writes(lv41_shared[v0, v1, v2, v3])
                                        lv41_shared[v0, v1, v2, v3] = lv41[v0, v1, v2, v3]
                        for k_1, i0_3, i1_3, i2_3, i3_3, k_2, i0_4, i1_4, i2_4, i3_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1), i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_i3_2_fused // T.int64(8) * T.int64(4) + i2_3 + i2_4)
                                v_i3 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused % T.int64(32) * T.int64(128) + i0_1_i1_1_i2_1_i3_1_fused * T.int64(64) + i0_2_i1_2_i2_2_i3_2_fused % T.int64(8) * T.int64(8) + i3_3 * T.int64(4) + i3_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2, v_i3], lv34_shared[v_i0, v_i1, v_i2, v_k], lv41_shared[v_i0, v_i1, v_k, v_i3])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2, v_i3])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2, v_i3] = var_matmul_intermediate_local[v_i0, v_i1, v_i2, v_i3] + lv34_shared[v_i0, v_i1, v_i2, v_k] * lv41_shared[v_i0, v_i1, v_k, v_i3]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(8)):
                        with T.block("var_matmul_intermediate_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(32) * T.int64(16) + i0_2_i1_2_i2_2_i3_2_fused // T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused % T.int64(32) * T.int64(128) + i0_1_i1_1_i2_1_i3_1_fused * T.int64(64) + i0_2_i1_2_i2_2_i3_2_fused % T.int64(8) * T.int64(8) + ax3)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2, v3], param_0[()])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2, v3])
                            var_T_multiply_intermediate[v0, v1, v2, v3] = var_matmul_intermediate_local[v0, v1, v2, v3] * param_0[()]

    @T.prim_func
    def fused_matmul4_multiply3(lv59: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32"), lv66: T.Buffer((T.int64(16), T.int64(40), T.int64(4096)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(4096)), scope="local")
        lv59_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="shared")
        lv66_shared = T.alloc_buffer((T.int64(16), T.int64(40), T.int64(4096)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(262144), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(16384) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(16384) // T.int64(256) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(5)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv59_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(16384))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(16384) // T.int64(256) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(40), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.reads(lv59[v0, v1, v2])
                                        T.writes(lv59_shared[v0, v1, v2])
                                        lv59_shared[v0, v1, v2] = lv59[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv66_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(16384))
                                        v1 = T.axis.spatial(T.int64(40), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(64) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(16))
                                        T.reads(lv66[v0, v1, v2])
                                        T.writes(lv66_shared[v0, v1, v2])
                                        lv66_shared[v0, v1, v2] = lv66[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(8), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(16384) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(16384) // T.int64(256) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(40), k_0 * T.int64(8) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv59_shared[v_i0, v_i1, v_k], lv66_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv59_shared[v_i0, v_i1, v_k] * lv66_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(16384) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(16384) // T.int64(256) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.15811388194561005)

    @T.prim_func
    def fused_matmul7_multiply4(lv87: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32"), lv94: T.Buffer((T.int64(16), T.int64(40), T.int64(77)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(16), T.int64(4096), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(77)), scope="local")
        lv87_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="shared")
        lv94_shared = T.alloc_buffer((T.int64(16), T.int64(40), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(11) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(11) * T.int64(7) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(5)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv87_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(352) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(40), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(352) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(128))
                                        T.reads(lv87[v0, v1, v2])
                                        T.writes(lv87_shared[v0, v1, v2])
                                        lv87_shared[v0, v1, v2] = lv87[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(88), thread="threadIdx.x"):
                                with T.block("lv94_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(40), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) // T.int64(77))
                                    v2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(88) + ax0_ax1_ax2_fused_1) % T.int64(77))
                                    T.reads(lv94[v0, v1, v2])
                                    T.writes(lv94_shared[v0, v1, v2])
                                    lv94_shared[v0, v1, v2] = lv94[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(11) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(11) * T.int64(7) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(40), k_0 * T.int64(8) + k_1 + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv87_shared[v_i0, v_i1, v_k], lv94_shared[v_i0, v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv87_shared[v_i0, v_i1, v_k] * lv94_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(7)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(256) * T.int64(16) + i0_2_i1_2_i2_2_fused // T.int64(11) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(11) * T.int64(7) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2] * T.float32(0.15811388194561005)

    @T.prim_func
    def fused_matmul9_add7_gelu(lv106: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), lv110: T.Buffer((T.int64(320), T.int64(1280)), "float32"), unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(4096), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(1280)))
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(1280)), scope="local")
        lv106_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="shared")
        lv110_shared = T.alloc_buffer((T.int64(320), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(10240), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(16) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(8) + i1_3_init * T.int64(4) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(20)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("lv106_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(128) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(320), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(lv106[v0, v1, v2])
                                    T.writes(lv106_shared[v0, v1, v2])
                                    lv106_shared[v0, v1, v2] = lv106[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("lv110_shared"):
                                    v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(lv110[v0, v1])
                                    T.writes(lv110_shared[v0, v1])
                                    lv110_shared[v0, v1] = lv110[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(16) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(8) + i1_3 * T.int64(4) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(16) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv106_shared[v_i0, v_i1, v_k], lv110_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv106_shared[v_i0, v_i1, v_k] * lv110_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(20) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(20) * T.int64(64) + i0_1_i1_1_i2_1_fused * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(16) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2])
                            T.writes(var_matmul_intermediate[v0, v1, v2])
                            var_matmul_intermediate[v0, v1, v2] = var_matmul_intermediate_local[v0, v1, v2]
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(160)):
                    with T.block("T_multiply_2"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(5242880))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(5242880) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1280))
                        T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2], unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2])
                        T.writes(var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                        var_T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = (var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * (T.float32(0.5) + T.erf((var_matmul_intermediate[v_ax0, v_ax1, v_ax2] + unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj2_bias[v_ax2]) * T.float32(0.70710678118654757)) * T.float32(0.5))

    @T.prim_func
    def fused_matmul9_add7_multiply5(lv106: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), lv107: T.Buffer((T.int64(320), T.int64(1280)), "float32"), unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias: T.Buffer((T.int64(1280),), "float32"), lv113: T.Buffer((T.int64(2), T.int64(4096), T.int64(1280)), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(4096), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(1280)), scope="local")
        lv106_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="shared")
        lv107_shared = T.alloc_buffer((T.int64(320), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(2) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(40) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(160) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(80) + i0_2_i1_2_i2_2_fused % T.int64(40) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(80)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("lv106_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(32) // T.int64(4))
                                        v2 = T.axis.spatial(T.int64(320), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(4))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(64))
                                        T.reads(lv106[v0, v1, v2])
                                        T.writes(lv106_shared[v0, v1, v2])
                                        lv106_shared[v0, v1, v2] = lv106[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("lv107_shared"):
                                        v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(640) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(160))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(160) + (ax0_ax1_fused_0 * T.int64(640) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(160))
                                        T.reads(lv107[v0, v1])
                                        T.writes(lv107_shared[v0, v1])
                                        lv107_shared[v0, v1] = lv107[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(2) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(40) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(160) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(80) + i0_2_i1_2_i2_2_fused % T.int64(40) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(4) + k_1 * T.int64(2) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1, v_i2], lv106_shared[v_i0, v_i1, v_k], lv107_shared[v_k, v_i2])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1, v_i2] = var_matmul_intermediate_local[v_i0, v_i1, v_i2] + lv106_shared[v_i0, v_i1, v_k] * lv107_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused // T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(8) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(40) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(160) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(80) + i0_2_i1_2_i2_2_fused % T.int64(40) * T.int64(2) + ax2)
                            T.reads(var_matmul_intermediate_local[v0, v1, v2], unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2], lv113[v0, v1, v2])
                            T.writes(var_T_multiply_intermediate[v0, v1, v2])
                            var_T_multiply_intermediate[v0, v1, v2] = (var_matmul_intermediate_local[v0, v1, v2] + unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj1_bias[v2]) * lv113[v0, v1, v2]

    @T.prim_func
    def fused_matmul_add_silu(lv14: T.Buffer((T.int64(2), T.int64(320)), "float32"), lv15: T.Buffer((T.int64(320), T.int64(1280)), "float32"), unet_time_embedding_linear_1_bias: T.Buffer((T.int64(1280),), "float32"), var_T_multiply_intermediate: T.Buffer((T.int64(2), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_matmul_intermediate = T.alloc_buffer((T.int64(2), T.int64(1280)))
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(2), T.int64(1280)), scope="local")
        lv14_shared = T.alloc_buffer((T.int64(2), T.int64(320)), scope="shared")
        lv15_shared = T.alloc_buffer((T.int64(320), T.int64(1280)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused * T.int64(160) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_matmul_intermediate_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(8)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("lv14_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) // T.int64(40))
                                    v1 = T.axis.spatial(T.int64(320), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) % T.int64(40))
                                    T.where(ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 < T.int64(80))
                                    T.reads(lv14[v0, v1])
                                    T.writes(lv14_shared[v0, v1])
                                    lv14_shared[v0, v1] = lv14[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(40)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("lv15_shared"):
                                    v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) // T.int64(160))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused * T.int64(160) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1) % T.int64(160))
                                    T.reads(lv15[v0, v1])
                                    T.writes(lv15_shared[v0, v1])
                                    lv15_shared[v0, v1] = lv15[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(10), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused * T.int64(160) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(40) + k_1 * T.int64(4) + k_2)
                                T.reads(var_matmul_intermediate_local[v_i0, v_i1], lv14_shared[v_i0, v_k], lv15_shared[v_k, v_i1])
                                T.writes(var_matmul_intermediate_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_matmul_intermediate_local[v_i0, v_i1] = var_matmul_intermediate_local[v_i0, v_i1] + lv14_shared[v_i0, v_k] * lv15_shared[v_k, v_i1]
                    for ax0, ax1 in T.grid(T.int64(2), T.int64(1)):
                        with T.block("var_matmul_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_fused * T.int64(160) + i0_2_i1_2_fused + ax1)
                            T.reads(var_matmul_intermediate_local[v0, v1])
                            T.writes(var_matmul_intermediate[v0, v1])
                            var_matmul_intermediate[v0, v1] = var_matmul_intermediate_local[v0, v1]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) // T.int64(1280))
                    v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) % T.int64(1280))
                    T.reads(var_matmul_intermediate[v_ax0, v_ax1], unet_time_embedding_linear_1_bias[v_ax1])
                    T.writes(var_T_multiply_intermediate[v_ax0, v_ax1])
                    var_T_multiply_intermediate[v_ax0, v_ax1] = (var_matmul_intermediate[v_ax0, v_ax1] + unet_time_embedding_linear_1_bias[v_ax1]) * T.sigmoid(var_matmul_intermediate[v_ax0, v_ax1] + unet_time_embedding_linear_1_bias[v_ax1])

    @T.prim_func
    def fused_reshape11_transpose11(lv118: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(320))
                        v_ax3 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(320))
                        T.reads(lv118[((v_ax1 * T.int64(64) + v_ax3 // T.int64(320) + v_ax2) // T.int64(4096) + v_ax0) % T.int64(2), (v_ax1 * T.int64(64) + v_ax3 // T.int64(320) + v_ax2) % T.int64(4096), v_ax3 % T.int64(320)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2])
                        var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2] = lv118[((v_ax1 * T.int64(64) + v_ax3 // T.int64(320) + v_ax2) // T.int64(4096) + v_ax0) % T.int64(2), (v_ax1 * T.int64(64) + v_ax3 // T.int64(320) + v_ax2) % T.int64(4096), v_ax3 % T.int64(320)]

    @T.prim_func
    def fused_reshape15_transpose15_reshape16(lv258: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(640))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640) // T.int64(80))
                        v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(80))
                        T.reads(lv258[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(1024) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)])
                        T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                        var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv258[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(1024) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)]

    @T.prim_func
    def fused_reshape15_transpose15_reshape16_transpose16(lv260: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(80), T.int64(1024)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(640))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640) // T.int64(80))
                        v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(80))
                        T.reads(lv260[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(1024) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)])
                        T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv260[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(1024) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(1024), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)]

    @T.prim_func
    def fused_reshape17_transpose17_reshape18(lv276: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(81920))
                        v_ax2 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(80))
                        v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(80))
                        T.reads(lv276[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(80) + v_ax2) // T.int64(1024) + v_ax1) % T.int64(16), (v_ax3 // T.int64(80) + v_ax2) % T.int64(1024), v_ax3 % T.int64(80)])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(80)])
                        var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(80)] = lv276[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(80) + v_ax2) // T.int64(1024) + v_ax1) % T.int64(16), (v_ax3 // T.int64(80) + v_ax2) % T.int64(1024), v_ax3 % T.int64(80)]

    @T.prim_func
    def fused_reshape19_transpose19_reshape20(lv290: T.Buffer((T.int64(2), T.int64(77), T.int64(640)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(77), T.int64(80)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49280))
                        v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49280) // T.int64(640))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640) // T.int64(80))
                        v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(80))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(98560))
                        T.reads(lv290[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)])
                        T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                        var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv290[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)]

    @T.prim_func
    def fused_reshape19_transpose19_reshape20_transpose20(lv288: T.Buffer((T.int64(2), T.int64(77), T.int64(640)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(80), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49280))
                        v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49280) // T.int64(640))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640) // T.int64(80))
                        v_ax3 = T.axis.spatial(T.int64(80), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(80))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(98560))
                        T.reads(lv288[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)])
                        T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv288[(((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(640) + v_ax1) % T.int64(77), (v_ax2 * T.int64(80) + v_ax3) % T.int64(640)]

    @T.prim_func
    def fused_reshape21_transpose23(lv324: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(640))
                        v_ax3 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640))
                        T.reads(lv324[((v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) // T.int64(1024) + v_ax0) % T.int64(2), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2])
                        var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2] = lv324[((v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) // T.int64(1024) + v_ax0) % T.int64(2), (v_ax1 * T.int64(32) + v_ax3 // T.int64(640) + v_ax2) % T.int64(1024), v_ax3 % T.int64(640)]

    @T.prim_func
    def fused_reshape25_transpose25_reshape26(lv464: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.reads(lv464[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(256) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                        var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv464[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(256) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape25_transpose25_reshape26_transpose26(lv466: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(160), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.reads(lv466[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(256) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv466[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(256) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(256), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape27_transpose27_reshape28(lv482: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(40960))
                        v_ax2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40960) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.reads(lv482[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(160) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16), (v_ax3 // T.int64(160) + v_ax2) % T.int64(256), v_ax3 % T.int64(160)])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(160)])
                        var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(160)] = lv482[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(160) + v_ax2) // T.int64(256) + v_ax1) % T.int64(16), (v_ax3 // T.int64(160) + v_ax2) % T.int64(256), v_ax3 % T.int64(160)]

    @T.prim_func
    def fused_reshape29_transpose29_reshape30(lv496: T.Buffer((T.int64(2), T.int64(77), T.int64(1280)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(77), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98560))
                        v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98560) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(197120))
                        T.reads(lv496[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                        var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv496[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape29_transpose29_reshape30_transpose30(lv494: T.Buffer((T.int64(2), T.int64(77), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(160), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98560))
                        v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98560) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(197120))
                        T.reads(lv494[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv494[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(77), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape31_transpose33(lv530: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(1280))
                        v_ax3 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280))
                        T.reads(lv530[((v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) // T.int64(256) + v_ax0) % T.int64(2), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2])
                        var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2] = lv530[((v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) // T.int64(256) + v_ax0) % T.int64(2), (v_ax1 * T.int64(16) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(256), v_ax3 % T.int64(1280)]

    @T.prim_func
    def fused_reshape33_transpose35_reshape34(lv705: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv705[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(64) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(64), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                        var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv705[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(64) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(64), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape33_transpose35_reshape34_transpose36(lv707: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(160), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv707[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(64) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(64), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)])
                        T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv707[(((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) // T.int64(64) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(160) + v_ax3) // T.int64(1280) + v_ax1) % T.int64(64), (v_ax2 * T.int64(160) + v_ax3) % T.int64(1280)]

    @T.prim_func
    def fused_reshape35_transpose37_reshape36(lv723: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(10240))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10240) // T.int64(160))
                        v_ax3 = T.axis.spatial(T.int64(160), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(160))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv723[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(160) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16), (v_ax3 // T.int64(160) + v_ax2) % T.int64(64), v_ax3 % T.int64(160)])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(160)])
                        var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(160)] = lv723[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(160) + v_ax2) // T.int64(64) + v_ax1) % T.int64(16), (v_ax3 // T.int64(160) + v_ax2) % T.int64(64), v_ax3 % T.int64(160)]

    @T.prim_func
    def fused_reshape37_transpose38(lv771: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(10240))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10240) // T.int64(1280))
                        v_ax3 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv771[((v_ax1 * T.int64(8) + v_ax3 // T.int64(1280) + v_ax2) // T.int64(64) + v_ax0) % T.int64(2), (v_ax1 * T.int64(8) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(64), v_ax3 % T.int64(1280)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2])
                        var_T_transpose_intermediate[v_ax0, v_ax3, v_ax1, v_ax2] = lv771[((v_ax1 * T.int64(8) + v_ax3 // T.int64(1280) + v_ax2) // T.int64(64) + v_ax0) % T.int64(2), (v_ax1 * T.int64(8) + v_ax3 // T.int64(1280) + v_ax2) % T.int64(64), v_ax3 % T.int64(1280)]

    @T.prim_func
    def fused_reshape39_cast1_reshape40(inp_0: T.Buffer((T.int64(1), T.int64(77)), "int32"), var_T_reshape_intermediate: T.Buffer((T.int64(77),), "int32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(77))
                    T.reads(inp_0[T.int64(0), v_ax1 % T.int64(77)])
                    T.writes(var_T_reshape_intermediate[v_ax1])
                    var_T_reshape_intermediate[v_ax1] = inp_0[T.int64(0), v_ax1 % T.int64(77)]

    @T.prim_func
    def fused_reshape41_reshape41_add33(lv3: T.Buffer((T.int64(77), T.int64(768)), "float32"), lv7: T.Buffer((T.int64(77), T.int64(768)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(1848), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) // T.int64(768))
                    v_ax2 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1) % T.int64(768))
                    T.reads(lv3[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)], lv7[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)])
                    T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2])
                    var_T_add_intermediate[v_ax0, v_ax1, v_ax2] = lv3[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)] + lv7[(v_ax2 // T.int64(768) + v_ax1) % T.int64(77), v_ax2 % T.int64(768)]

    @T.prim_func
    def fused_reshape44_transpose40_reshape45(lv33: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1848), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(768))
                    v_ax2 = T.axis.spatial(T.int64(12), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(768) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(lv33[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)])
                    T.writes(var_T_reshape_intermediate[v_ax2, v_ax1, v_ax3])
                    var_T_reshape_intermediate[v_ax2, v_ax1, v_ax3] = lv33[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)]

    @T.prim_func
    def fused_reshape44_transpose40_reshape45_transpose41(lv28: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(12), T.int64(64), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1848), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(768))
                    v_ax2 = T.axis.spatial(T.int64(12), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(768) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(lv28[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)])
                    T.writes(var_T_transpose_intermediate[v_ax2, v_ax3, v_ax1])
                    var_T_transpose_intermediate[v_ax2, v_ax3, v_ax1] = lv28[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(768) + v_ax1) % T.int64(77), (v_ax2 * T.int64(64) + v_ax3) % T.int64(768)]

    @T.prim_func
    def fused_reshape46_add36_reshape47(lv42: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32"), param_0: T.Buffer((T.int64(1), T.int64(1), T.int64(77), T.int64(77)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(12), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(5929))
                        v_ax2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(5929) // T.int64(77))
                        v_ax3 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(77))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(71148))
                        T.reads(lv42[((v_ax3 // T.int64(77) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(77) + v_ax2) % T.int64(77), v_ax3 % T.int64(77)], param_0[v_ax0, T.int64(0), v_ax2, v_ax3])
                        T.writes(var_T_reshape_intermediate[v_ax1, v_ax2, v_ax3])
                        var_T_reshape_intermediate[v_ax1, v_ax2, v_ax3] = lv42[((v_ax3 // T.int64(77) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(77) + v_ax2) % T.int64(77), v_ax3 % T.int64(77)] + param_0[v_ax0, T.int64(0), v_ax2, v_ax3]

    @T.prim_func
    def fused_reshape48_transpose42_reshape49(lv47: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1848), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(12), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4928))
                    v_ax2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4928) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(lv47[((v_ax3 // T.int64(64) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(64) + v_ax2) % T.int64(77), v_ax3 % T.int64(64)])
                    T.writes(var_T_reshape_intermediate[T.int64(0), v_ax2, v_ax3 + v_ax1 * T.int64(64)])
                    var_T_reshape_intermediate[T.int64(0), v_ax2, v_ax3 + v_ax1 * T.int64(64)] = lv47[((v_ax3 // T.int64(64) + v_ax2) // T.int64(77) + v_ax1) % T.int64(12), (v_ax3 // T.int64(64) + v_ax2) % T.int64(77), v_ax3 % T.int64(64)]

    @T.prim_func
    def fused_reshape51_transpose45_transpose46(lv18: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(4096))
                        T.reads(lv18[T.int64(0), (v_ax2 // T.int64(4096) + v_ax1) % T.int64(512), v_ax2 % T.int64(4096) // T.int64(64), v_ax2 % T.int64(64)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2])
                        var_T_transpose_intermediate[v_ax0, v_ax1, v_ax2] = lv18[T.int64(0), (v_ax2 // T.int64(4096) + v_ax1) % T.int64(512), v_ax2 % T.int64(4096) // T.int64(64), v_ax2 % T.int64(64)]

    @T.prim_func
    def fused_reshape52_transpose48(lv26: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                        v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv26[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax2, v_ax1, v_ax3])
                        var_T_transpose_intermediate[v_ax0, v_ax2, v_ax1, v_ax3] = lv26[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)]

    @T.prim_func
    def fused_reshape52_transpose48_transpose49(lv29: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(512), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                        v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv29[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)])
                        T.writes(var_T_transpose_intermediate[v_ax0, v_ax2, v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax0, v_ax2, v_ax3, v_ax1] = lv29[T.int64(0), (v_ax3 // T.int64(512) + v_ax1 + v_ax2) % T.int64(4096), v_ax3 % T.int64(512)]

    @T.prim_func
    def fused_reshape5_transpose5_reshape6(lv52: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(320))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(320) // T.int64(40))
                        v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40))
                        T.reads(lv52[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(4096) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(4096), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)])
                        T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                        var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv52[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(4096) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(4096), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)]

    @T.prim_func
    def fused_reshape5_transpose5_reshape6_transpose6(lv54: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(40), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(320))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(320) // T.int64(40))
                        v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40))
                        T.reads(lv54[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(4096) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(4096), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)])
                        T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                        var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv54[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(4096) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(4096), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)]

    @T.prim_func
    def fused_reshape7_transpose7_reshape8(lv70: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_reshape"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(163840))
                        v_ax2 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(163840) // T.int64(40))
                        v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(40))
                        T.reads(lv70[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(40) + v_ax2) // T.int64(4096) + v_ax1) % T.int64(16), (v_ax3 // T.int64(40) + v_ax2) % T.int64(4096), v_ax3 % T.int64(40)])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(40)])
                        var_T_reshape_intermediate[v_ax0, v_ax2, v_ax3 + v_ax1 * T.int64(40)] = lv70[(v_ax0 * T.int64(8) + (v_ax3 // T.int64(40) + v_ax2) // T.int64(4096) + v_ax1) % T.int64(16), (v_ax3 // T.int64(40) + v_ax2) % T.int64(4096), v_ax3 % T.int64(40)]

    @T.prim_func
    def fused_reshape9_transpose9_reshape10(lv84: T.Buffer((T.int64(2), T.int64(77), T.int64(320)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(16), T.int64(77), T.int64(40)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1540), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(24640))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(24640) // T.int64(320))
                    v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(320) // T.int64(40))
                    v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(40))
                    T.reads(lv84[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(77), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)])
                    T.writes(var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3])
                    var_T_reshape_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax1, v_ax3] = lv84[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(77), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)]

    @T.prim_func
    def fused_reshape9_transpose9_reshape10_transpose10(lv82: T.Buffer((T.int64(2), T.int64(77), T.int64(320)), "float32"), var_T_transpose_intermediate: T.Buffer((T.int64(16), T.int64(40), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(385), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(24640))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(24640) // T.int64(320))
                    v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(320) // T.int64(40))
                    v_ax3 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(40))
                    T.reads(lv82[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(77), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)])
                    T.writes(var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1])
                    var_T_transpose_intermediate[v_ax2 + v_ax0 * T.int64(8), v_ax3, v_ax1] = lv82[(((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) // T.int64(77) + v_ax0) % T.int64(2), ((v_ax2 * T.int64(40) + v_ax3) // T.int64(320) + v_ax1) % T.int64(77), (v_ax2 * T.int64(40) + v_ax3) % T.int64(320)]

    @T.prim_func
    def fused_split_subtract_multiply15_add32(lv1818: T.Buffer((T.int64(2), T.int64(4), T.int64(64), T.int64(64)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(lv1818[v_ax0:v_ax0 + T.int64(2), v_ax1, v_ax2, v_ax3])
                    T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                    var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv1818[v_ax0, v_ax1, v_ax2, v_ax3] + T.float32(7.5) * (lv1818[v_ax0 + T.int64(1), v_ax1, v_ax2, v_ax3] - lv1818[v_ax0, v_ax1, v_ax2, v_ax3])

    @T.prim_func
    def fused_transpose13_reshape14(lv253: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(655360) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(640))
                        v_ax3 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(640))
                        T.reads(lv253[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(32), v_ax3])
                        var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(32), v_ax3] = lv253[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose24_reshape24(lv459: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(327680) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(1280))
                        v_ax3 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280))
                        T.reads(lv459[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(16), v_ax3])
                        var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(16), v_ax3] = lv459[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose34_reshape32(lv700: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(81920) // T.int64(10240))
                        v_ax2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10240) // T.int64(1280))
                        v_ax3 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280))
                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_fused_2 < T.int64(163840))
                        T.reads(lv700[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(8), v_ax3])
                        var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(8), v_ax3] = lv700[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose3_reshape4(lv47: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1310720) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(20480) // T.int64(320))
                        v_ax3 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(320))
                        T.reads(lv47[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(64), v_ax3])
                        var_T_reshape_intermediate[v_ax0, v_ax2 + v_ax1 * T.int64(64), v_ax3] = lv47[v_ax0, v_ax3, v_ax1, v_ax2]

    @T.prim_func
    def fused_transpose46_reshape54_add40_divide7(lv50: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32"), lv18: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), var_T_divide_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(32)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(4096))
                        v_ax2 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(4096))
                        T.reads(lv50[v_ax0, v_ax2, v_ax1], lv18[T.int64(0), v_ax1, v_ax2 // T.int64(64), v_ax2 % T.int64(64)])
                        T.writes(var_T_divide_intermediate[T.int64(0), v_ax1, v_ax2 // T.int64(64), v_ax2 % T.int64(64)])
                        var_T_divide_intermediate[T.int64(0), v_ax1, v_ax2 // T.int64(64), v_ax2 % T.int64(64)] = lv50[v_ax0, v_ax2, v_ax1] + lv18[T.int64(0), v_ax1, v_ax2 // T.int64(64), v_ax2 % T.int64(64)]

    @T.prim_func
    def fused_transpose50_reshape53(lv45: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32"), var_T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                        v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                        T.reads(lv45[v_ax0, v_ax2, v_ax1, v_ax3])
                        T.writes(var_T_reshape_intermediate[T.int64(0), v_ax1, v_ax3])
                        var_T_reshape_intermediate[T.int64(0), v_ax1, v_ax3] = lv45[v_ax0, v_ax2, v_ax1, v_ax3]

    @T.prim_func
    def fused_transpose51_multiply30_tir_round(lv237: T.Buffer((T.int64(1), T.int64(3), T.int64(512), T.int64(512)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(512), T.int64(512), T.int64(3)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(12)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1536))
                        v_ax2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1536) // T.int64(3))
                        v_ax3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                        T.reads(lv237[v_ax0, v_ax3, v_ax1, v_ax2])
                        T.writes(var_compute_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                        var_compute_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = T.round(lv237[v_ax0, v_ax3, v_ax1, v_ax2] * T.float32(255))

    @T.prim_func
    def group_norm1(A: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(320),), "float32"), C: T.Buffer((T.int64(320),), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(320), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(10), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(4096) // T.int64(64))
                        v_k4 = T.axis.reduce(T.int64(64), (k2_k3_k4_fused_0 * T.int64(256) + k2_k3_k4_fused_1) % T.int64(64))
                        T.reads(A[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)] * A[((v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_k4 // T.int64(64) + v_k3) // T.int64(64) + v_k2) % T.int64(320), (v_k4 // T.int64(64) + v_k3) % T.int64(64), v_k4 % T.int64(64)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int64(40)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(1310720) // T.int64(40960))
                        v_ax2 = T.axis.spatial(T.int64(10), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(40960) // T.int64(4096))
                        v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(4096) // T.int64(64))
                        v_ax4 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(64))
                        T.reads(A[((v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(320), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[(v_ax1 * T.int64(10) + v_ax2) % T.int64(320)], C[(v_ax1 * T.int64(10) + v_ax2) % T.int64(320)])
                        T.writes(T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(10), v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(10), v_ax3, v_ax4] = (A[((v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 * T.int64(10) + (v_ax4 // T.int64(64) + v_ax3) // T.int64(64) + v_ax2) % T.int64(320), (v_ax4 // T.int64(64) + v_ax3) % T.int64(64), v_ax4 % T.int64(64)] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(2.4414062500000001e-05)) + T.float32(9.9999999999999995e-07)) * B[(v_ax1 * T.int64(10) + v_ax2) % T.int64(320)] + C[(v_ax1 * T.int64(10) + v_ax2) % T.int64(320)]

    @T.prim_func
    def group_norm19(A: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32"), B: T.Buffer((T.int64(512),), "float32"), C: T.Buffer((T.int64(512),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_fused_0 in range(T.int64(512)):
                for k2_k3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(16), (k2_k3_fused_0 * T.int64(128) + k2_k3_fused_1) // T.int64(4096))
                        v_k3 = T.axis.reduce(T.int64(4096), (k2_k3_fused_0 * T.int64(128) + k2_k3_fused_1) % T.int64(4096))
                        T.reads(A[T.int64(0), (v_ax1 * T.int64(16) + v_k3 // T.int64(4096) + v_k2) % T.int64(512), v_k3 % T.int64(4096)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[T.int64(0), (v_ax1 * T.int64(16) + v_k3 // T.int64(4096) + v_k2) % T.int64(512), v_k3 % T.int64(4096)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[T.int64(0), (v_ax1 * T.int64(16) + v_k3 // T.int64(4096) + v_k2) % T.int64(512), v_k3 % T.int64(4096)] * A[T.int64(0), (v_ax1 * T.int64(16) + v_k3 // T.int64(4096) + v_k2) % T.int64(512), v_k3 % T.int64(4096)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) // T.int64(65536))
                        v_ax2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(65536) // T.int64(4096))
                        v_ax3 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_ax3_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096))
                        T.reads(A[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(4096) + v_ax2) % T.int64(512), v_ax3 % T.int64(4096)], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[(v_ax1 * T.int64(16) + v_ax2) % T.int64(512)], C[(v_ax1 * T.int64(16) + v_ax2) % T.int64(512)])
                        T.writes(T_reshape[T.int64(0), v_ax2 + v_ax1 * T.int64(16), v_ax3])
                        T_reshape[T.int64(0), v_ax2 + v_ax1 * T.int64(16), v_ax3] = (A[T.int64(0), (v_ax1 * T.int64(16) + v_ax3 // T.int64(4096) + v_ax2) % T.int64(512), v_ax3 % T.int64(4096)] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(1.52587890625e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(1.52587890625e-05)) + T.float32(9.9999999999999995e-07)) * B[(v_ax1 * T.int64(16) + v_ax2) % T.int64(512)] + C[(v_ax1 * T.int64(16) + v_ax2) % T.int64(512)]

    @T.prim_func
    def group_norm4(A: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), B: T.Buffer((T.int64(640),), "float32"), C: T.Buffer((T.int64(640),), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(20), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(1024))
                        v_k3 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(1024) // T.int64(32))
                        v_k4 = T.axis.reduce(T.int64(32), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(32))
                        T.reads(A[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)] * A[((v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_k4 // T.int64(32) + v_k3) // T.int64(32) + v_k2) % T.int64(640), (v_k4 // T.int64(32) + v_k3) % T.int64(32), v_k4 % T.int64(32)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int64(20)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(655360) // T.int64(20480))
                        v_ax2 = T.axis.spatial(T.int64(20), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(20480) // T.int64(1024))
                        v_ax3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(1024) // T.int64(32))
                        v_ax4 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(32))
                        T.reads(A[((v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(640), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[(v_ax1 * T.int64(20) + v_ax2) % T.int64(640)], C[(v_ax1 * T.int64(20) + v_ax2) % T.int64(640)])
                        T.writes(T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(20), v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(20), v_ax3, v_ax4] = (A[((v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 * T.int64(20) + (v_ax4 // T.int64(32) + v_ax3) // T.int64(32) + v_ax2) % T.int64(640), (v_ax4 // T.int64(32) + v_ax3) % T.int64(32), v_ax4 % T.int64(32)] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(4.8828125000000003e-05)) + T.float32(9.9999999999999995e-07)) * B[(v_ax1 * T.int64(20) + v_ax2) % T.int64(640)] + C[(v_ax1 * T.int64(20) + v_ax2) % T.int64(640)]

    @T.prim_func
    def group_norm7(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), B: T.Buffer((T.int64(1280),), "float32"), C: T.Buffer((T.int64(1280),), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(80)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) // T.int64(256))
                        v_k3 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(256) // T.int64(16))
                        v_k4 = T.axis.reduce(T.int64(16), (k2_k3_k4_fused_0 * T.int64(128) + k2_k3_k4_fused_1) % T.int64(16))
                        T.reads(A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)] * A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(16) + v_k3) // T.int64(16) + v_k2) % T.int64(1280), (v_k4 // T.int64(16) + v_k3) % T.int64(16), v_k4 % T.int64(16)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int64(10)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(327680) // T.int64(10240))
                        v_ax2 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(10240) // T.int64(256))
                        v_ax3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(256) // T.int64(16))
                        v_ax4 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(16))
                        T.reads(A[((v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)], C[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)])
                        T.writes(T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(40), v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(40), v_ax3, v_ax4] = (A[((v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(16) + v_ax3) // T.int64(16) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(16) + v_ax3) % T.int64(16), v_ax4 % T.int64(16)] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(9.7656250000000005e-05)) + T.float32(9.9999999999999995e-07)) * B[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)] + C[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)]

    @T.prim_func
    def group_norm9(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), B: T.Buffer((T.int64(1280),), "float32"), C: T.Buffer((T.int64(1280),), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(32)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(32)))
        for ax0_ax1_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_k3_k4_fused_0 in range(T.int64(160)):
                for k2_k3_k4_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(32))
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused % T.int64(32))
                        v_k2 = T.axis.reduce(T.int64(40), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) // T.int64(64))
                        v_k3 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) % T.int64(64) // T.int64(8))
                        v_k4 = T.axis.reduce(T.int64(8), (k2_k3_k4_fused_0 * T.int64(16) + k2_k3_k4_fused_1) % T.int64(8))
                        T.reads(A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)] * A[((v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_k4 // T.int64(8) + v_k3) // T.int64(8) + v_k2) % T.int64(1280), (v_k4 // T.int64(8) + v_k3) % T.int64(8), v_k4 % T.int64(8)]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_ax3_ax4_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_ax4_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_ax3_ax4_fused_0 in range(T.int64(3)):
                    with T.block("T_group_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(81920) // T.int64(2560))
                        v_ax2 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(2560) // T.int64(64))
                        v_ax3 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(64) // T.int64(8))
                        v_ax4 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(65536) + ax0_ax1_ax2_ax3_ax4_fused_1 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2) % T.int64(8))
                        T.where((ax0_ax1_ax2_ax3_ax4_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_1) * T.int64(256) + ax0_ax1_ax2_ax3_ax4_fused_2 < T.int64(163840))
                        T.reads(A[((v_ax1 * T.int64(40) + (v_ax4 // T.int64(8) + v_ax3) // T.int64(8) + v_ax2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(8) + v_ax3) // T.int64(8) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(8) + v_ax3) % T.int64(8), v_ax4 % T.int64(8)], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)], C[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)])
                        T.writes(T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(40), v_ax3, v_ax4])
                        T_reshape[v_ax0, v_ax2 + v_ax1 * T.int64(40), v_ax3, v_ax4] = (A[((v_ax1 * T.int64(40) + (v_ax4 // T.int64(8) + v_ax3) // T.int64(8) + v_ax2) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 * T.int64(40) + (v_ax4 // T.int64(8) + v_ax3) // T.int64(8) + v_ax2) % T.int64(1280), (v_ax4 // T.int64(8) + v_ax3) % T.int64(8), v_ax4 % T.int64(8)] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00039062500000000002) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) + T.float32(9.9999999999999995e-07)) * B[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)] + C[(v_ax1 * T.int64(40) + v_ax2) % T.int64(1280)]

    @T.prim_func
    def layer_norm(A: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), B: T.Buffer((T.int64(320),), "float32"), C: T.Buffer((T.int64(320),), "float32"), T_layer_norm: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(4096)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(4096)))
        for ax0_ax1_fused in T.thread_binding(T.int64(8192), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for k2_0 in range(T.int64(10)):
                for k2_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(4096))
                        v_ax1 = T.axis.spatial(T.int64(4096), ax0_ax1_fused % T.int64(4096))
                        v_k2 = T.axis.reduce(T.int64(320), k2_0 * T.int64(32) + k2_1)
                        T.reads(A[v_ax0, v_ax1, v_k2])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(40)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(1310720))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1310720) // T.int64(320))
                        v_ax2 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(320))
                        T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0031250000000000002)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0031250000000000002) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0031250000000000002) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0031250000000000002)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func
    def layer_norm1(A: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), B: T.Buffer((T.int64(640),), "float32"), C: T.Buffer((T.int64(640),), "float32"), T_layer_norm: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(1024)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(1024)))
        for ax0_ax1_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_0 in range(T.int64(80)):
                for k2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(1024))
                        v_ax1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024))
                        v_k2 = T.axis.reduce(T.int64(640), k2_0 * T.int64(8) + k2_1)
                        T.reads(A[v_ax0, v_ax1, v_k2])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(20)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(655360))
                        v_ax1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(655360) // T.int64(640))
                        v_ax2 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(640))
                        T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0015625000000000001) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0015625000000000001)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func
    def layer_norm2(A: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1280),), "float32"), C: T.Buffer((T.int64(1280),), "float32"), T_layer_norm: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(256)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(256)))
        for ax0_ax1_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for k2_0 in range(T.int64(80)):
                for k2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(256))
                        v_ax1 = T.axis.spatial(T.int64(256), ax0_ax1_fused % T.int64(256))
                        v_k2 = T.axis.reduce(T.int64(1280), k2_0 * T.int64(16) + k2_1)
                        T.reads(A[v_ax0, v_ax1, v_k2])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(10)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(327680))
                        v_ax1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(327680) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1280))
                        T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00078125000000000004) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func
    def layer_norm3(A: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1280),), "float32"), C: T.Buffer((T.int64(1280),), "float32"), T_layer_norm: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(2), T.int64(64)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(2), T.int64(64)))
        for ax0_ax1_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_0 in range(T.int64(320)):
                for k2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(2), ax0_ax1_fused // T.int64(64))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_fused % T.int64(64))
                        v_k2 = T.axis.reduce(T.int64(1280), k2_0 * T.int64(4) + k2_1)
                        T.reads(A[v_ax0, v_ax1, v_k2])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(3)):
                    with T.block("T_layer_norm"):
                        v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(81920))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(81920) // T.int64(1280))
                        v_ax2 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(1280))
                        T.where((ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) * T.int64(256) + ax0_ax1_ax2_fused_2 < T.int64(163840))
                        T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                        T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                        T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00078125000000000004) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00078125000000000004)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func
    def layer_norm4(A: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32"), B: T.Buffer((T.int64(768),), "float32"), C: T.Buffer((T.int64(768),), "float32"), T_layer_norm: T.Buffer((T.int64(1), T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        A_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(77)))
        A_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(77)))
        for ax0_ax1_fused in T.thread_binding(T.int64(77), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for k2_0 in range(T.int64(96)):
                for k2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("A_red_temp"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(77), ax0_ax1_fused)
                        v_k2 = T.axis.reduce(T.int64(768), k2_0 * T.int64(8) + k2_1)
                        T.reads(A[v_ax0, v_ax1, v_k2])
                        T.writes(A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1])
                        with T.init():
                            A_red_temp_v0[v_ax0, v_ax1] = T.float32(0)
                            A_red_temp_v1[v_ax0, v_ax1] = T.float32(0)
                        v_A_red_temp_v0: T.float32 = A_red_temp_v0[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2]
                        v_A_red_temp_v1: T.float32 = A_red_temp_v1[v_ax0, v_ax1] + A[v_ax0, v_ax1, v_k2] * A[v_ax0, v_ax1, v_k2]
                        A_red_temp_v0[v_ax0, v_ax1] = v_A_red_temp_v0
                        A_red_temp_v1[v_ax0, v_ax1] = v_A_red_temp_v1
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(462), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_layer_norm"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(768))
                    v_ax2 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(768))
                    T.reads(A[v_ax0, v_ax1, v_ax2], A_red_temp_v0[v_ax0, v_ax1], A_red_temp_v1[v_ax0, v_ax1], B[v_ax2], C[v_ax2])
                    T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                    T_layer_norm[v_ax0, v_ax1, v_ax2] = (A[v_ax0, v_ax1, v_ax2] - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333)) * T.rsqrt(A_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0013020833333333333) - A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333) * (A_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0013020833333333333)) + T.float32(1.0000000000000001e-05)) * B[v_ax2] + C[v_ax2]

    @T.prim_func
    def matmul12(A: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32"), B: T.Buffer((T.int64(640), T.int64(640)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(1024), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(1024), T.int64(640)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(640), T.int64(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(128) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(128) // T.int64(16) * T.int64(4) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + i2_3_init * T.int64(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(10)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(2048))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(2048) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(640), k_0 * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(64))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(640), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(B[v0, v1])
                                    T.writes(B_shared[v0, v1])
                                    B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(16), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(128) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(128) // T.int64(16) * T.int64(4) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + i2_3 * T.int64(4) + i2_4)
                                v_k = T.axis.reduce(T.int64(640), k_0 * T.int64(64) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(128) // T.int64(16) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(16) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul14(A: T.Buffer((T.int64(16), T.int64(1024), T.int64(1024)), "float32"), B: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(1024)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1280), thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(80) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(80))
                                        v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(5) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(8))
                                        v2 = T.axis.spatial(T.int64(1024), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(8))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(80))
                                        v1 = T.axis.spatial(T.int64(1024), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(16))
                                        T.reads(B[v0, v1, v2])
                                        T.writes(B_shared[v0, v1, v2])
                                        B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(8), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(80) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(1024), k_0 * T.int64(8) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(8), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(80) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(80) // T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(8) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul15(A: T.Buffer((T.int64(2), T.int64(77), T.int64(768)), "float32"), B: T.Buffer((T.int64(768), T.int64(640)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(77), T.int64(640)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(640)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(768)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(768), T.int64(640)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(112), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(10), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(44), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(4) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(4) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(44), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(528))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(528) // T.int64(48))
                                        v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(48) + (ax0_ax1_ax2_fused_0 * T.int64(176) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(48))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(11)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(44), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(48) + (ax0_ax1_fused_0 * T.int64(176) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(40))
                                        v1 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + (ax0_ax1_fused_0 * T.int64(176) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(40))
                                        T.where((ax0_ax1_fused_0 * T.int64(44) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(1920))
                                        T.reads(B[v0, v1])
                                        T.writes(B_shared[v0, v1])
                                        B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(16), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(4) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(4) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(48) + k_1 * T.int64(3) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + i0_2_i1_2_i2_2_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(640), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul17(A: T.Buffer((T.int64(16), T.int64(1024), T.int64(77)), "float32"), B: T.Buffer((T.int64(16), T.int64(77), T.int64(80)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(1024), T.int64(80)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(80)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(1024), T.int64(77)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(77), T.int64(80)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(80) * T.int64(4) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(256) // T.int64(2) * T.int64(8) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(80) // T.int64(40) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(40) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(7)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) // T.int64(88))
                                    v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(256) // T.int64(2) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(88) // T.int64(11))
                                    v2 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(11))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1 < T.int64(704))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(22)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(160), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) // T.int64(440))
                                    v1 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(440) // T.int64(40))
                                    v2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(160) + ax0_ax1_ax2_fused_1) % T.int64(40))
                                    T.reads(B[v0, v1, v2])
                                    T.writes(B_shared[v0, v1, v2])
                                    B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(11), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(80) * T.int64(4) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(256) // T.int64(2) * T.int64(8) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(80) // T.int64(40) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(40) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(11) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(4), T.int64(2), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(256) * T.int64(8) + i0_2_i1_2_i2_2_fused // T.int64(80) * T.int64(4) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), i0_0_i1_0_i2_0_fused % T.int64(256) // T.int64(2) * T.int64(8) + i0_1_i1_1_i2_1_fused * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(80) // T.int64(40) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(80), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(40) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul20(A: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(256), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(256), T.int64(1280)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(640), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(640))
                                        v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(640) // T.int64(20))
                                        v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(20) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(20))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(3)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(20) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(16))
                                    T.where(ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 < T.int64(320))
                                    T.reads(B[v0, v1])
                                    T.writes(B_shared[v0, v1])
                                    B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(10), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(20) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused // T.int64(80) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(80) * T.int64(16) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul22(A: T.Buffer((T.int64(16), T.int64(256), T.int64(256)), "float32"), B: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(256)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(5)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(32) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(4) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(4) * T.int64(4) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(5) + i2_3_init * T.int64(5) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(4) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(256) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(256), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(320))
                                        v1 = T.axis.spatial(T.int64(256), k_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(320) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.reads(B[v0, v1, v2])
                                        T.writes(B_shared[v0, v1, v2])
                                        B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(2), T.int64(5)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(32) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(4) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(4) * T.int64(4) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(5) + i2_3 * T.int64(5) + i2_4)
                                v_k = T.axis.reduce(T.int64(256), k_0 * T.int64(8) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(4), T.int64(5)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(32) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(32) // T.int64(4) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(32) // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(5) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul23(A: T.Buffer((T.int64(2), T.int64(77), T.int64(768)), "float32"), B: T.Buffer((T.int64(768), T.int64(1280)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(77), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(1280)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(768)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(768), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(55), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(32) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(256) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(8) + i2_3_init * T.int64(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(448) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(84))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(448) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(84) // T.int64(12))
                                        v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(12) + (ax0_ax1_ax2_fused_0 * T.int64(448) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(12))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(224) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(168))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(12) + (ax0_ax1_fused_0 * T.int64(896) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(256) + (ax0_ax1_fused_0 * T.int64(896) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.where((ax0_ax1_fused_0 * T.int64(224) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(3072))
                                        T.reads(B[v0, v1])
                                        T.writes(B_shared[v0, v1])
                                        B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(6), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(32) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(256) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(8) + i2_3 * T.int64(4) + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(12) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(8)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_1_i1_1_i2_1_fused + ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(7) + i0_2_i1_2_i2_2_fused // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(256) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(8) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul25(A: T.Buffer((T.int64(16), T.int64(256), T.int64(77)), "float32"), B: T.Buffer((T.int64(16), T.int64(77), T.int64(160)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(256), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(160)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(256), T.int64(77)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(77), T.int64(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(40) // T.int64(5) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(7)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(352))
                                    v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(40) // T.int64(5) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(352) // T.int64(11))
                                    v2 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(11))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 < T.int64(1408))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) // T.int64(352))
                                    v1 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(352) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1 < T.int64(1408))
                                    T.reads(B[v0, v1, v2])
                                    T.writes(B_shared[v0, v1, v2])
                                    B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(11), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(40) // T.int64(5) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(11) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(40) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(256), i0_0_i1_0_i2_0_fused % T.int64(40) // T.int64(5) * T.int64(32) + i0_2_i1_2_i2_2_fused % T.int64(64) // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(32) + i0_1_i1_1_i2_1_fused * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul28(A: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32"), B: T.Buffer((T.int64(1280), T.int64(1280)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(64), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(64), T.int64(1280)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(1280), T.int64(1280)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(80), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init * T.int64(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(8) + i0_1_i1_1_i2_1_fused // T.int64(2) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(128) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(64) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(320) // T.int64(40))
                                    v2 = T.axis.spatial(T.int64(1280), k_0 * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(40))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(40)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(1280), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(128))
                                    T.reads(B[v0, v1])
                                    T.writes(B_shared[v0, v1])
                                    B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(2), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 * T.int64(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(8) + i0_1_i1_1_i2_1_fused // T.int64(2) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(128) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(64) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(1280), k_0 * T.int64(40) + k_1 * T.int64(5) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused // T.int64(10) * T.int64(8) + i0_1_i1_1_i2_1_fused // T.int64(2) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(64) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(1280), i0_0_i1_0_i2_0_fused % T.int64(10) * T.int64(128) + i0_1_i1_1_i2_1_fused % T.int64(2) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(64) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul3(A: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32"), B: T.Buffer((T.int64(320), T.int64(320)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(4096), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(4096), T.int64(320)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(320), T.int64(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_3_init * T.int64(2) + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(2560))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(2560) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(320), k_0 * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(320), k_0 * T.int64(40) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) % T.int64(64))
                                    T.reads(B[v0, v1])
                                    T.writes(B_shared[v0, v1])
                                    B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(10), T.int64(1), T.int64(8), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_3 * T.int64(2) + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(320), k_0 * T.int64(40) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(8), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused // T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(32) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(5) * T.int64(64) + i0_2_i1_2_i2_2_fused % T.int64(32) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul30(A: T.Buffer((T.int64(16), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(64)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(40) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(40) // T.int64(10) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(2) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(16)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1) % T.int64(16) // T.int64(4))
                                    v2 = T.axis.spatial(T.int64(64), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1) % T.int64(4))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 < T.int64(32))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(80), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(160))
                                        v1 = T.axis.spatial(T.int64(64), k_0 * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(160) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(320) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.reads(B[v0, v1, v2])
                                        T.writes(B_shared[v0, v1, v2])
                                        B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(40) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(40) // T.int64(10) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(2) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(64), k_0 * T.int64(4) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(64) * T.int64(2) + i0_2_i1_2_i2_2_fused // T.int64(40) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(64) // T.int64(4) * T.int64(4) + i0_2_i1_2_i2_2_fused % T.int64(40) // T.int64(10) + ax1)
                            v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(10) * T.int64(2) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul32(A: T.Buffer((T.int64(16), T.int64(64), T.int64(77)), "float32"), B: T.Buffer((T.int64(16), T.int64(77), T.int64(160)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(64), T.int64(160)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(160)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(64), T.int64(77)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(77), T.int64(160)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(5)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(4) * T.int64(2) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(64), i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + i2_3_init * T.int64(5) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(7)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(4) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(704))
                                        v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(704) // T.int64(11))
                                        v2 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(11))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(1408))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(4) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(440))
                                        v1 = T.axis.spatial(T.int64(77), k_0 * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(440) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + (ax0_ax1_ax2_fused_0 * T.int64(512) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(880))
                                        T.reads(B[v0, v1, v2])
                                        T.writes(B_shared[v0, v1, v2])
                                        B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(11), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(5)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(4) * T.int64(2) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(64), i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + i2_3 * T.int64(5) + i2_4)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(11) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(1), T.int64(5)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(4) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(64), i0_1_i1_1_i2_1_fused * T.int64(32) + i0_2_i1_2_i2_2_fused // T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(160), i0_0_i1_0_i2_0_fused % T.int64(4) * T.int64(40) + i0_2_i1_2_i2_2_fused % T.int64(8) * T.int64(5) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul36(A: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32"), B: T.Buffer((T.int64(12), T.int64(64), T.int64(77)), "float32"), matmul: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(77)), scope="local")
        A_shared = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(64)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(12), T.int64(64), T.int64(77)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(44), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(7), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(231), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(3) + i0_2_i1_2_i2_2_fused // T.int64(77) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_1_i1_1_i2_1_fused + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(77) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(2)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(231), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(3) + (ax0_ax1_ax2_fused_0 * T.int64(924) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) // T.int64(224))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(924) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(224) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(64), k_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(924) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(231) + ax0_ax1_ax2_fused_1) * T.int64(4) + ax0_ax1_ax2_fused_2 < T.int64(672))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(231), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(3) + (ax0_ax1_ax2_fused_0 * T.int64(231) + ax0_ax1_ax2_fused_1) // T.int64(2464))
                                    v1 = T.axis.spatial(T.int64(64), k_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(231) + ax0_ax1_ax2_fused_1) % T.int64(2464) // T.int64(77))
                                    v2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(231) + ax0_ax1_ax2_fused_1) % T.int64(77))
                                    T.reads(B[v0, v1, v2])
                                    T.writes(B_shared[v0, v1, v2])
                                    B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(3) + i0_2_i1_2_i2_2_fused // T.int64(77) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_1_i1_1_i2_1_fused + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(77) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(64), k_0 * T.int64(32) + k_1 * T.int64(2) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(11) * T.int64(3) + i0_2_i1_2_i2_2_fused // T.int64(77) + ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(11) * T.int64(7) + i0_1_i1_1_i2_1_fused + ax1)
                            v2 = T.axis.spatial(T.int64(77), i0_2_i1_2_i2_2_fused % T.int64(77) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul37(A: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32"), B: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32"), matmul: T.Buffer((T.int64(12), T.int64(77), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(64)), scope="local")
        A_shared = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(77)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(12), T.int64(77), T.int64(64)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(66), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(56) * T.int64(3) + i0_2_i1_2_i2_2_fused // T.int64(22) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(56) // T.int64(8) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(22) // T.int64(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(2) * T.int64(4) + i2_3_init * T.int64(4) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(1)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(39)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(66), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(56) * T.int64(3) + (ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1) // T.int64(847))
                                    v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(56) // T.int64(8) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1) % T.int64(847) // T.int64(77))
                                    v2 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1) % T.int64(77))
                                    T.where(ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1 < T.int64(2541))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(28)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(66), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(56) * T.int64(3) + (ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1) // T.int64(616))
                                    v1 = T.axis.spatial(T.int64(77), (ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1) % T.int64(616) // T.int64(8))
                                    v2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(66) + ax0_ax1_ax2_fused_1) % T.int64(8))
                                    T.reads(B[v0, v1, v2])
                                    T.writes(B_shared[v0, v1, v2])
                                    B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(11), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(56) * T.int64(3) + i0_2_i1_2_i2_2_fused // T.int64(22) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(56) // T.int64(8) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(22) // T.int64(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(2) * T.int64(4) + i2_3 * T.int64(4) + i2_4)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(77) + k_1 * T.int64(11) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(12), i0_0_i1_0_i2_0_fused // T.int64(56) * T.int64(3) + i0_2_i1_2_i2_2_fused // T.int64(22) + ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused % T.int64(56) // T.int64(8) * T.int64(11) + i0_2_i1_2_i2_2_fused % T.int64(22) // T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(64), i0_0_i1_0_i2_0_fused % T.int64(8) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(2) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul42(A: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), "float32"), B: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), scope="local")
        A_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(512)), scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i3_3_init, i0_4_init, i1_4_init, i2_4_init, i3_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(1), i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_i3_1_fused * T.int64(64) + i0_2_i1_2_i2_2_i3_2_fused // T.int64(8) * T.int64(4) + i2_3_init * T.int64(2) + i2_4_init)
                            v_i3 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_i3_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_i3_2_fused % T.int64(8) * T.int64(4) + i3_3_init + i3_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2, v_i3])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2, v_i3] = T.float32(0)
                    for k_0 in range(T.int64(1024)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(16) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v3 = T.axis.spatial(T.int64(4096), k_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        T.reads(A[v0, v1, v2, v3])
                                        T.writes(A_shared[v0, v1, v2, v3])
                                        A_shared[v0, v1, v2, v3] = A[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v2 = T.axis.spatial(T.int64(4096), k_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_i3_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(128))
                                        T.reads(B[v0, v1, v2, v3])
                                        T.writes(B_shared[v0, v1, v2, v3])
                                        B_shared[v0, v1, v2, v3] = B[v0, v1, v2, v3]
                        for k_1, i0_3, i1_3, i2_3, i3_3, k_2, i0_4, i1_4, i2_4, i3_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(1), i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_i3_1_fused * T.int64(64) + i0_2_i1_2_i2_2_i3_2_fused // T.int64(8) * T.int64(4) + i2_3 * T.int64(2) + i2_4)
                                v_i3 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_i3_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_i3_2_fused % T.int64(8) * T.int64(4) + i3_3 + i3_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2, v_i3], A_shared[v_i0, v_i1, v_i2, v_k], B_shared[v_i0, v_i1, v_k, v_i3])
                                T.writes(matmul_local[v_i0, v_i1, v_i2, v_i3])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2, v_i3] = matmul_local[v_i0, v_i1, v_i2, v_i3] + A_shared[v_i0, v_i1, v_i2, v_k] * B_shared[v_i0, v_i1, v_k, v_i3]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("matmul_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_i3_0_fused // T.int64(16) * T.int64(128) + i0_1_i1_1_i2_1_i3_1_fused * T.int64(64) + i0_2_i1_2_i2_2_i3_2_fused // T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(512), i0_0_i1_0_i2_0_i3_0_fused % T.int64(16) * T.int64(32) + i0_2_i1_2_i2_2_i3_2_fused % T.int64(8) * T.int64(4) + ax3)
                            T.reads(matmul_local[v0, v1, v2, v3])
                            T.writes(matmul[v0, v1, v2, v3])
                            matmul[v0, v1, v2, v3] = matmul_local[v0, v1, v2, v3]

    @T.prim_func
    def matmul5(A: T.Buffer((T.int64(16), T.int64(4096), T.int64(4096)), "float32"), B: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(4096)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(1), T.int64(5), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(128) * T.int64(2) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(2) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(40), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(5) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("A_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(128) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(2) * T.int64(64) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(1024) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(16))
                                    T.reads(A[v0, v1, v2])
                                    T.writes(A_shared[v0, v1, v2])
                                    A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("B_shared"):
                                    v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(128) * T.int64(2) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) // T.int64(320))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(320) // T.int64(20))
                                    v2 = T.axis.spatial(T.int64(40), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(20) + (ax0_ax1_ax2_fused_0 * T.int64(128) + ax0_ax1_ax2_fused_1) % T.int64(20))
                                    T.reads(B[v0, v1, v2])
                                    T.writes(B_shared[v0, v1, v2])
                                    B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(4), T.int64(2), T.int64(1), T.int64(5), T.int64(4), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(128) * T.int64(2) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(2) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_i2 = T.axis.spatial(T.int64(40), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(5) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(16) + k_1 * T.int64(4) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(2), T.int64(5)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(128) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(128) // T.int64(2) * T.int64(64) + i0_2_i1_2_i2_2_fused // T.int64(4) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(40), i0_0_i1_0_i2_0_fused % T.int64(2) * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(4) * T.int64(5) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul6(A: T.Buffer((T.int64(2), T.int64(77), T.int64(768)), "float32"), B: T.Buffer((T.int64(768), T.int64(320)), "float32"), matmul: T.Buffer((T.int64(2), T.int64(77), T.int64(320)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(320)), scope="local")
        A_shared = T.alloc_buffer((T.int64(2), T.int64(77), T.int64(768)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(768), T.int64(320)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(112), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(11), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(20) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + i0_1_i1_1_i2_1_fused + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(20) + i2_3_init + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(18)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(120) + ax0_ax1_ax2_fused_1 * T.int64(3) + ax0_ax1_ax2_fused_2) // T.int64(1056))
                                        v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + (ax0_ax1_ax2_fused_0 * T.int64(120) + ax0_ax1_ax2_fused_1 * T.int64(3) + ax0_ax1_ax2_fused_2) % T.int64(1056) // T.int64(96))
                                        v2 = T.axis.spatial(T.int64(768), k_0 * T.int64(96) + (ax0_ax1_ax2_fused_0 * T.int64(120) + ax0_ax1_ax2_fused_1 * T.int64(3) + ax0_ax1_ax2_fused_2) % T.int64(96))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(40) + ax0_ax1_ax2_fused_1) * T.int64(3) + ax0_ax1_ax2_fused_2 < T.int64(2112))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_fused_0 in range(T.int64(12)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(768), k_0 * T.int64(96) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(20))
                                        v1 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(20) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(20))
                                        T.reads(B[v0, v1])
                                        T.writes(B_shared[v0, v1])
                                        B_shared[v0, v1] = B[v0, v1]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(12), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(20) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + i0_1_i1_1_i2_1_fused + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(20) + i2_3 + i2_4)
                                v_k = T.axis.reduce(T.int64(768), k_0 * T.int64(96) + k_1 * T.int64(12) + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(2), i0_2_i1_2_i2_2_fused // T.int64(20) + ax0)
                            v1 = T.axis.spatial(T.int64(77), i0_0_i1_0_i2_0_fused // T.int64(16) * T.int64(11) + i0_1_i1_1_i2_1_fused + ax1)
                            v2 = T.axis.spatial(T.int64(320), i0_0_i1_0_i2_0_fused % T.int64(16) * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(20) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def matmul8(A: T.Buffer((T.int64(16), T.int64(4096), T.int64(77)), "float32"), B: T.Buffer((T.int64(16), T.int64(77), T.int64(40)), "float32"), matmul: T.Buffer((T.int64(16), T.int64(4096), T.int64(40)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        matmul_local = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(40)), scope="local")
        A_shared = T.alloc_buffer((T.int64(16), T.int64(4096), T.int64(77)), scope="shared")
        B_shared = T.alloc_buffer((T.int64(16), T.int64(77), T.int64(40)), scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i2_3_init, i0_4_init, i1_4_init, i2_4_init in T.grid(T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("matmul_init"):
                            v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(512) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(2) + i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(512) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(20) // T.int64(5) * T.int64(2) + i1_3_init + i1_4_init)
                            v_i2 = T.axis.spatial(T.int64(40), i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(5) * T.int64(4) + i2_3_init * T.int64(2) + i2_4_init)
                            T.reads()
                            T.writes(matmul_local[v_i0, v_i1, v_i2])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                            matmul_local[v_i0, v_i1, v_i2] = T.float32(0)
                    for k_0 in range(T.int64(11)):
                        for ax0_ax1_ax2_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("A_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(512) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(512) * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(56) // T.int64(7))
                                        v2 = T.axis.spatial(T.int64(77), k_0 * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(7))
                                        T.where((ax0_ax1_ax2_fused_0 * T.int64(40) + ax0_ax1_ax2_fused_1) * T.int64(2) + ax0_ax1_ax2_fused_2 < T.int64(224))
                                        T.reads(A[v0, v1, v2])
                                        T.writes(A_shared[v0, v1, v2])
                                        A_shared[v0, v1, v2] = A[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in range(T.int64(14)):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("B_shared"):
                                        v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(512) * T.int64(4) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) // T.int64(280))
                                        v1 = T.axis.spatial(T.int64(77), k_0 * T.int64(7) + (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(280) // T.int64(40))
                                        v2 = T.axis.spatial(T.int64(40), (ax0_ax1_ax2_fused_0 * T.int64(80) + ax0_ax1_ax2_fused_1 * T.int64(2) + ax0_ax1_ax2_fused_2) % T.int64(40))
                                        T.reads(B[v0, v1, v2])
                                        T.writes(B_shared[v0, v1, v2])
                                        B_shared[v0, v1, v2] = B[v0, v1, v2]
                        for k_1, i0_3, i1_3, i2_3, k_2, i0_4, i1_4, i2_4 in T.grid(T.int64(7), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("matmul_update"):
                                v_i0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(512) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(2) + i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(512) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(20) // T.int64(5) * T.int64(2) + i1_3 + i1_4)
                                v_i2 = T.axis.spatial(T.int64(40), i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(5) * T.int64(4) + i2_3 * T.int64(2) + i2_4)
                                v_k = T.axis.reduce(T.int64(77), k_0 * T.int64(7) + k_1 + k_2)
                                T.reads(matmul_local[v_i0, v_i1, v_i2], A_shared[v_i0, v_i1, v_k], B_shared[v_i0, v_k, v_i2])
                                T.writes(matmul_local[v_i0, v_i1, v_i2])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": T.int64(256), "meta_schedule.thread_extent_low_inclusive": T.int64(32), "meta_schedule.tiling_structure": "SSSRRSRS"})
                                matmul_local[v_i0, v_i1, v_i2] = matmul_local[v_i0, v_i1, v_i2] + A_shared[v_i0, v_i1, v_k] * B_shared[v_i0, v_k, v_i2]
                    for ax0, ax1, ax2 in T.grid(T.int64(2), T.int64(2), T.int64(4)):
                        with T.block("matmul_local"):
                            v0 = T.axis.spatial(T.int64(16), i0_0_i1_0_i2_0_fused // T.int64(512) * T.int64(4) + i0_2_i1_2_i2_2_fused // T.int64(20) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_i2_0_fused % T.int64(512) * T.int64(8) + i0_2_i1_2_i2_2_fused % T.int64(20) // T.int64(5) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(40), i0_1_i1_1_i2_1_fused * T.int64(20) + i0_2_i1_2_i2_2_fused % T.int64(5) * T.int64(4) + ax2)
                            T.reads(matmul_local[v0, v1, v2])
                            T.writes(matmul[v0, v1, v2])
                            matmul[v0, v1, v2] = matmul_local[v0, v1, v2]

    @T.prim_func
    def multiply(A: T.Buffer((), "float32"), B: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[()], B[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = A[()] * B[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply16(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(55) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply17(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(59) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply18(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(37) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply19(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(9) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply20(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0.041666667908430099) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply24(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(3) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply25(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(23) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply26(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(16) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply27(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(5) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def multiply28(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(7.6775431632995605) * A[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def reshape13(A: T.Buffer((T.int64(2), T.int64(640)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(640), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(640))
                    v_ax1 = T.axis.spatial(T.int64(640), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(640))
                    v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax3 = T.axis.spatial(T.int64(1), T.int64(0))
                    T.reads(A[((v_ax1 + v_ax2 + v_ax3) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(640)])
                    T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = A[((v_ax1 + v_ax2 + v_ax3) // T.int64(640) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(640)]

    @T.prim_func
    def reshape23(A: T.Buffer((T.int64(2), T.int64(1280)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(1280), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1280))
                    v_ax1 = T.axis.spatial(T.int64(1280), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1280))
                    v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax3 = T.axis.spatial(T.int64(1), T.int64(0))
                    T.reads(A[((v_ax1 + v_ax2 + v_ax3) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(1280)])
                    T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = A[((v_ax1 + v_ax2 + v_ax3) // T.int64(1280) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(1280)]

    @T.prim_func
    def reshape3(A: T.Buffer((T.int64(2), T.int64(320)), "float32"), T_reshape: T.Buffer((T.int64(2), T.int64(320), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v_ax0 = T.axis.spatial(T.int64(2), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(320))
                    v_ax1 = T.axis.spatial(T.int64(320), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(320))
                    v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax3 = T.axis.spatial(T.int64(1), T.int64(0))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(640))
                    T.reads(A[((v_ax1 + v_ax2 + v_ax3) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(320)])
                    T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = A[((v_ax1 + v_ax2 + v_ax3) // T.int64(320) + v_ax0) % T.int64(2), (v_ax1 + v_ax2 + v_ax3) % T.int64(320)]

    @T.prim_func
    def resize2d(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(8), T.int64(8)), "float32"), resize: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(10)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(2), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(327680))
                        v_i1 = T.axis.spatial(T.int64(1280), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(327680) // T.int64(256))
                        v_i2 = T.axis.spatial(T.int64(16), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(256) // T.int64(16))
                        v_i3 = T.axis.spatial(T.int64(16), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(16))
                        T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(7)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(7)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(7)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(7)), T.int64(0))]

    @T.prim_func
    def resize2d1(A: T.Buffer((T.int64(2), T.int64(1280), T.int64(16), T.int64(16)), "float32"), resize: T.Buffer((T.int64(2), T.int64(1280), T.int64(32), T.int64(32)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(40)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(2), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(1310720))
                        v_i1 = T.axis.spatial(T.int64(1280), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(1310720) // T.int64(1024))
                        v_i2 = T.axis.spatial(T.int64(32), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(1024) // T.int64(32))
                        v_i3 = T.axis.spatial(T.int64(32), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(32))
                        T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(15)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(15)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(15)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(15)), T.int64(0))]

    @T.prim_func
    def resize2d2(A: T.Buffer((T.int64(2), T.int64(640), T.int64(32), T.int64(32)), "float32"), resize: T.Buffer((T.int64(2), T.int64(640), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(80)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(2), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(2621440))
                        v_i1 = T.axis.spatial(T.int64(640), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(2621440) // T.int64(4096))
                        v_i2 = T.axis.spatial(T.int64(64), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(4096) // T.int64(64))
                        v_i3 = T.axis.spatial(T.int64(64), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(64))
                        T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(31)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(31)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(31)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(31)), T.int64(0))]

    @T.prim_func
    def resize2d3(A: T.Buffer((T.int64(1), T.int64(512), T.int64(64), T.int64(64)), "float32"), resize: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(128)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i1 = T.axis.spatial(T.int64(512), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(16384))
                        v_i2 = T.axis.spatial(T.int64(128), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(16384) // T.int64(128))
                        v_i3 = T.axis.spatial(T.int64(128), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(128))
                        T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(63)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(63)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(63)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(63)), T.int64(0))]

    @T.prim_func
    def resize2d4(A: T.Buffer((T.int64(1), T.int64(512), T.int64(128), T.int64(128)), "float32"), resize: T.Buffer((T.int64(1), T.int64(512), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(512)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i1 = T.axis.spatial(T.int64(512), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(65536))
                        v_i2 = T.axis.spatial(T.int64(256), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(65536) // T.int64(256))
                        v_i3 = T.axis.spatial(T.int64(256), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(256))
                        T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(127)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(127)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(127)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(127)), T.int64(0))]

    @T.prim_func
    def resize2d5(A: T.Buffer((T.int64(1), T.int64(256), T.int64(256), T.int64(256)), "float32"), resize: T.Buffer((T.int64(1), T.int64(256), T.int64(512), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_i2_i3_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for i0_i1_i2_i3_fused_0 in range(T.int64(1024)):
                    with T.block("resize"):
                        v_i0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i1 = T.axis.spatial(T.int64(256), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) // T.int64(262144))
                        v_i2 = T.axis.spatial(T.int64(512), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(262144) // T.int64(512))
                        v_i3 = T.axis.spatial(T.int64(512), (i0_i1_i2_i3_fused_0 * T.int64(65536) + i0_i1_i2_i3_fused_1 * T.int64(256) + i0_i1_i2_i3_fused_2) % T.int64(512))
                        T.reads(A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(255)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(255)), T.int64(0))])
                        T.writes(resize[v_i0, v_i1, v_i2, v_i3])
                        resize[v_i0, v_i1, v_i2, v_i3] = A[v_i0, v_i1, T.max(T.min(T.Div(v_i2, T.int64(2)), T.int64(255)), T.int64(0)), T.max(T.min(T.Div(v_i3, T.int64(2)), T.int64(255)), T.int64(0))]

    @T.prim_func
    def silu(A: T.Buffer((T.int64(2), T.int64(1280)), "float32"), T_multiply: T.Buffer((T.int64(2), T.int64(1280)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0_i1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x"):
            for i0_i1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("compute"):
                    v_i0 = T.axis.spatial(T.int64(2), (i0_i1_fused_0 * T.int64(256) + i0_i1_fused_1) // T.int64(1280))
                    v_i1 = T.axis.spatial(T.int64(1280), (i0_i1_fused_0 * T.int64(256) + i0_i1_fused_1) % T.int64(1280))
                    T.reads(A[v_i0, v_i1])
                    T.writes(T_multiply[v_i0, v_i1])
                    T_multiply[v_i0, v_i1] = A[v_i0, v_i1] * T.sigmoid(A[v_i0, v_i1])

    @T.prim_func
    def softmax(A: T.Buffer((T.int64(16), T.int64(4096), T.int64(4096)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(4096)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(4096)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(65536), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096) + ax1)
                        v_k = T.axis.reduce(T.int64(4096), ax2_0 * T.int64(256) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096) + ax1)
                        v_k = T.axis.reduce(T.int64(4096), ax2_0 * T.int64(256) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(16)):
                for i2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096))
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096))
                        v_i2 = T.axis.spatial(T.int64(4096), i2_0 * T.int64(256) + i2_1)
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax1(A: T.Buffer((T.int64(16), T.int64(4096), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(4096), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(4096)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(4096)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(65536), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(1024), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(10)):
                for ax2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(8) + ax2_1)
                        T.where(ax2_0 * T.int64(8) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(10)):
                for ax2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096) + ax0)
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(8) + ax2_1)
                        T.where(ax2_0 * T.int64(8) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(10)):
                for i2_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(4096))
                        v_i1 = T.axis.spatial(T.int64(4096), i0_i1_fused % T.int64(4096))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(8) + i2_1)
                        T.where(i2_0 * T.int64(8) + i2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax2(A: T.Buffer((T.int64(16), T.int64(1024), T.int64(1024)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(1024), T.int64(1024)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(1024)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(1024)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(16384), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024) + ax1)
                        v_k = T.axis.reduce(T.int64(1024), ax2_0 * T.int64(256) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                for ax2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024) + ax1)
                        v_k = T.axis.reduce(T.int64(1024), ax2_0 * T.int64(256) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(4)):
                for i2_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024))
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024))
                        v_i2 = T.axis.spatial(T.int64(1024), i2_0 * T.int64(256) + i2_1)
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax3(A: T.Buffer((T.int64(16), T.int64(1024), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(1024), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(1024)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(1024)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(16384), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(5)):
                for ax2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(16) + ax2_1)
                        T.where(ax2_0 * T.int64(16) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(5)):
                for ax2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024) + ax0)
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(16) + ax2_1)
                        T.where(ax2_0 * T.int64(16) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(5)):
                for i2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(1024))
                        v_i1 = T.axis.spatial(T.int64(1024), i0_i1_fused % T.int64(1024))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(16) + i2_1)
                        T.where(i2_0 * T.int64(16) + i2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax4(A: T.Buffer((T.int64(16), T.int64(256), T.int64(256)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(256), T.int64(256)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(256)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(256)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256) + ax0)
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256) + ax1)
                        v_k = T.axis.reduce(T.int64(256), ax2_0 * T.int64(16) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256) + ax0)
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256) + ax1)
                        v_k = T.axis.reduce(T.int64(256), ax2_0 * T.int64(16) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(16)):
                for i2_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256))
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256))
                        v_i2 = T.axis.spatial(T.int64(256), i2_0 * T.int64(16) + i2_1)
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax5(A: T.Buffer((T.int64(16), T.int64(256), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(256), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(256)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(256)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(2)):
                for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256) + ax0)
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(2)):
                for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256) + ax0)
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(2)):
                for i2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(256))
                        v_i1 = T.axis.spatial(T.int64(256), i0_i1_fused % T.int64(256))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(64) + i2_1)
                        T.where(i2_0 * T.int64(64) + i2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax6(A: T.Buffer((T.int64(16), T.int64(64), T.int64(64)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(64)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(64)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(16), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64) + ax0)
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64) + ax1)
                        v_k = T.axis.reduce(T.int64(64), ax2_0 * T.int64(4) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(16)):
                for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64) + ax0)
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64) + ax1)
                        v_k = T.axis.reduce(T.int64(64), ax2_0 * T.int64(4) + ax2_1)
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(16)):
                for i2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64))
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64))
                        v_i2 = T.axis.spatial(T.int64(64), i2_0 * T.int64(4) + i2_1)
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax7(A: T.Buffer((T.int64(16), T.int64(64), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(16), T.int64(64), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(16), T.int64(64)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(16), T.int64(64)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(20)):
                for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64) + ax0)
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(4) + ax2_1)
                        T.where(ax2_0 * T.int64(4) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(20)):
                for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64) + ax0)
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(4) + ax2_1)
                        T.where(ax2_0 * T.int64(4) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(20)):
                for i2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(16), i0_i1_fused // T.int64(64))
                        v_i1 = T.axis.spatial(T.int64(64), i0_i1_fused % T.int64(64))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(4) + i2_1)
                        T.where(i2_0 * T.int64(4) + i2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax8(A: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32"), T_softmax_norm: T.Buffer((T.int64(12), T.int64(77), T.int64(77)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(12), T.int64(77)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(12), T.int64(77)), scope="shared")
        for i0_i1_fused in T.thread_binding(T.int64(924), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(64), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(20)):
                for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0 = T.axis.spatial(T.int64(12), i0_i1_fused // T.int64(77) + ax0)
                        v_i1 = T.axis.spatial(T.int64(77), i0_i1_fused % T.int64(77) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(4) + ax2_1)
                        T.where(ax2_0 * T.int64(4) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1] = T.max(T_softmax_maxelem_shared[v_i0, v_i1], A[v_i0, v_i1, v_k])
            for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(1), T.int64(20)):
                for ax2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0 = T.axis.spatial(T.int64(12), i0_i1_fused // T.int64(77) + ax0)
                        v_i1 = T.axis.spatial(T.int64(77), i0_i1_fused % T.int64(77) + ax1)
                        v_k = T.axis.reduce(T.int64(77), ax2_0 * T.int64(4) + ax2_1)
                        T.where(ax2_0 * T.int64(4) + ax2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_k], T_softmax_maxelem_shared[v_i0, v_i1])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1] = T_softmax_expsum_shared[v_i0, v_i1] + T.exp(A[v_i0, v_i1, v_k] - T_softmax_maxelem_shared[v_i0, v_i1])
            for i2_0 in range(T.int64(20)):
                for i2_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(12), i0_i1_fused // T.int64(77))
                        v_i1 = T.axis.spatial(T.int64(77), i0_i1_fused % T.int64(77))
                        v_i2 = T.axis.spatial(T.int64(77), i2_0 * T.int64(4) + i2_1)
                        T.where(i2_0 * T.int64(4) + i2_1 < T.int64(77))
                        T.reads(A[v_i0, v_i1, v_i2], T_softmax_maxelem_shared[v_i0, v_i1], T_softmax_expsum_shared[v_i0, v_i1])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                        T.block_attr({"axis": 2})
                        T_softmax_norm[v_i0, v_i1, v_i2] = T.exp(A[v_i0, v_i1, v_i2] - T_softmax_maxelem_shared[v_i0, v_i1]) / T_softmax_expsum_shared[v_i0, v_i1]

    @T.prim_func
    def softmax9(A: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), "float32"), T_softmax_norm: T.Buffer((T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 4, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_softmax_maxelem_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)), scope="shared")
        T_softmax_expsum_shared = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)), scope="shared")
        for i0_i1_i2_fused in T.thread_binding(T.int64(4096), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": T.int64(512), "pragma_unroll_explicit": T.int64(1)}):
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        v_i0, v_i1 = T.axis.remap("SS", [ax0, ax1])
                        v_i2 = T.axis.spatial(T.int64(4096), i0_i1_i2_fused + ax2)
                        v_k = T.axis.reduce(T.int64(4096), ax3_0 * T.int64(256) + ax3_1)
                        T.reads(A[v_i0, v_i1, v_i2, v_k])
                        T.writes(T_softmax_maxelem_shared[v_i0, v_i1, v_i2])
                        with T.init():
                            T_softmax_maxelem_shared[v_i0, v_i1, v_i2] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem_shared[v_i0, v_i1, v_i2] = T.max(T_softmax_maxelem_shared[v_i0, v_i1, v_i2], A[v_i0, v_i1, v_i2, v_k])
            for ax0, ax1, ax2, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16)):
                for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_expsum"):
                        v_i0, v_i1 = T.axis.remap("SS", [ax0, ax1])
                        v_i2 = T.axis.spatial(T.int64(4096), i0_i1_i2_fused + ax2)
                        v_k = T.axis.reduce(T.int64(4096), ax3_0 * T.int64(256) + ax3_1)
                        T.reads(A[v_i0, v_i1, v_i2, v_k], T_softmax_maxelem_shared[v_i0, v_i1, v_i2])
                        T.writes(T_softmax_expsum_shared[v_i0, v_i1, v_i2])
                        with T.init():
                            T_softmax_expsum_shared[v_i0, v_i1, v_i2] = T.float32(0)
                        T_softmax_expsum_shared[v_i0, v_i1, v_i2] = T_softmax_expsum_shared[v_i0, v_i1, v_i2] + T.exp(A[v_i0, v_i1, v_i2, v_k] - T_softmax_maxelem_shared[v_i0, v_i1, v_i2])
            for i3_0 in range(T.int64(16)):
                for i3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("T_softmax_norm"):
                        v_i0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i1 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_i2 = T.axis.spatial(T.int64(4096), i0_i1_i2_fused)
                        v_i3 = T.axis.spatial(T.int64(4096), i3_0 * T.int64(256) + i3_1)
                        T.reads(A[v_i0, v_i1, v_i2, v_i3], T_softmax_maxelem_shared[v_i0, v_i1, v_i2], T_softmax_expsum_shared[v_i0, v_i1, v_i2])
                        T.writes(T_softmax_norm[v_i0, v_i1, v_i2, v_i3])
                        T.block_attr({"axis": 3})
                        T_softmax_norm[v_i0, v_i1, v_i2, v_i3] = T.exp(A[v_i0, v_i1, v_i2, v_i3] - T_softmax_maxelem_shared[v_i0, v_i1, v_i2]) / T_softmax_expsum_shared[v_i0, v_i1, v_i2]

    @T.prim_func
    def subtract(A: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), B: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32"), T_subtract: T.Buffer((T.int64(1), T.int64(4), T.int64(64), T.int64(64)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 0, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_subtract"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                    v_ax2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(64))
                    v_ax3 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                    T.reads(A[v_ax0, v_ax1, v_ax2, v_ax3], B[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(T_subtract[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_subtract[v_ax0, v_ax1, v_ax2, v_ax3] = A[v_ax0, v_ax1, v_ax2, v_ax3] - B[v_ax0, v_ax1, v_ax2, v_ax3]

    @T.prim_func
    def take(A: T.Buffer((T.int64(49408), T.int64(768)), "float32"), B: T.Buffer((T.int64(77),), "int32"), T_take: T.Buffer((T.int64(77), T.int64(768)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 8, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(462), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_take"):
                    v_ax0 = T.axis.spatial(T.int64(77), (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(768))
                    v_ax1 = T.axis.spatial(T.int64(768), (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(768))
                    T.reads(A[B[v_ax0], v_ax1], B[v_ax0])
                    T.writes(T_take[v_ax0, v_ax1])
                    T_take[v_ax0, v_ax1] = A[B[v_ax0], v_ax1]

    @T.prim_func
    def tir_image_to_rgba(A: T.Buffer((T.int64(1), T.int64(512), T.int64(512), T.int64(3)), "float32"), image_to_rgba: T.Buffer((T.int64(512), T.int64(512)), "uint32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for y_x_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for y_x_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for y_x_fused_0 in range(T.int64(4)):
                    with T.block("image_to_rgba"):
                        v_y = T.axis.spatial(T.int64(512), (y_x_fused_0 * T.int64(65536) + y_x_fused_1 * T.int64(256) + y_x_fused_2) // T.int64(512))
                        v_x = T.axis.spatial(T.int64(512), (y_x_fused_0 * T.int64(65536) + y_x_fused_1 * T.int64(256) + y_x_fused_2) % T.int64(512))
                        T.reads(A[T.int64(0), v_y, v_x, T.int64(0):T.int64(3)])
                        T.writes(image_to_rgba[v_y, v_x])
                        image_to_rgba[v_y, v_x] = T.bitwise_or(T.bitwise_or(T.bitwise_or(T.Cast("uint32", A[T.int64(0), v_y, v_x, T.int64(0)]), T.shift_left(T.Cast("uint32", A[T.int64(0), v_y, v_x, T.int64(1)]), T.uint32(8))), T.shift_left(T.Cast("uint32", A[T.int64(0), v_y, v_x, T.int64(2)]), T.uint32(16))), T.uint32(4278190080))

    @T.prim_func
    def transpose45(A: T.Buffer((T.int64(1), T.int64(512), T.int64(4096)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(4096), T.int64(512)), "float32")):
        T.func_attr({"global_symbol": "main", "op_pattern": 2, "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0_ax1_ax2_fused_0 in range(T.int64(32)):
                    with T.block("T_transpose"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(4096), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) // T.int64(512))
                        v_ax2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(65536) + ax0_ax1_ax2_fused_1 * T.int64(256) + ax0_ax1_ax2_fused_2) % T.int64(512))
                        T.reads(A[v_ax0, v_ax2, v_ax1])
                        T.writes(T_transpose[v_ax0, v_ax1, v_ax2])
                        T_transpose[v_ax0, v_ax1, v_ax2] = A[v_ax0, v_ax2, v_ax1]

    @R.function
    def clip(inp_0: R.Tensor((1, 77), dtype="int32"), model_params: R.Tuple(R.Tensor((49408, 768), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((3072,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((77, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 768), dtype="float32"), R.Tensor((768, 3072), dtype="float32"), R.Tensor((3072, 768), dtype="float32"))) -> R.Tensor((1, 77, 768), dtype="float32"):
        R.func_attr({"global_symbol": "subgraph_0", "num_input": 1})
        cls = Module
        with R.dataflow():
            lv441 = R.call_tir(cls.fused_reshape39_cast1_reshape40, (inp_0,), out_sinfo=R.Tensor((77,), dtype="int32"))
            lv718: R.Tensor((49408, 768), dtype="float32") = model_params[0]
            lv3 = R.call_tir(cls.take, (lv718, lv441), out_sinfo=R.Tensor((77, 768), dtype="float32"))
            lv719: R.Tensor((77, 768), dtype="float32") = model_params[123]
            lv442 = R.call_tir(cls.fused_reshape41_reshape41_add33, (lv3, lv719), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv720: R.Tensor((768,), dtype="float32") = model_params[2]
            lv721: R.Tensor((768,), dtype="float32") = model_params[1]
            lv21 = R.call_tir(cls.layer_norm4, (lv442, lv720, lv721), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv722: R.Tensor((768, 768), dtype="float32") = model_params[124]
            lv723: R.Tensor((768,), dtype="float32") = model_params[9]
            lv443 = R.call_tir(cls.fused_matmul35_add35_multiply21, (lv21, lv722, lv723), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv724: R.Tensor((768, 768), dtype="float32") = model_params[125]
            lv725: R.Tensor((768,), dtype="float32") = model_params[7]
            lv444 = R.call_tir(cls.fused_matmul35_add35, (lv21, lv724, lv725), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv445 = R.call_tir(cls.fused_reshape44_transpose40_reshape45_transpose41, (lv444,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv726: R.Tensor((768, 768), dtype="float32") = model_params[126]
            lv727: R.Tensor((768,), dtype="float32") = model_params[10]
            lv446 = R.call_tir(cls.fused_matmul35_add35, (lv21, lv726, lv727), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv447 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv446,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv448 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv443,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv42 = R.call_tir(cls.matmul36, (lv448, lv445), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv449 = R.call_tir(cls.fused_reshape46_add36_reshape47, (lv42, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv46 = R.call_tir(cls.softmax8, (lv449,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv47 = R.call_tir(cls.matmul37, (lv46, lv447), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv450 = R.call_tir(cls.fused_reshape48_transpose42_reshape49, (lv47,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv728: R.Tensor((768, 768), dtype="float32") = model_params[127]
            lv729: R.Tensor((768,), dtype="float32") = model_params[8]
            lv451 = R.call_tir(cls.fused_matmul35_add35_add33, (lv450, lv728, lv729, lv442), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv730: R.Tensor((768,), dtype="float32") = model_params[4]
            lv731: R.Tensor((768,), dtype="float32") = model_params[3]
            lv55 = R.call_tir(cls.layer_norm4, (lv451, lv730, lv731), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv732: R.Tensor((768, 3072), dtype="float32") = model_params[128]
            lv733: R.Tensor((3072,), dtype="float32") = model_params[5]
            lv452 = R.call_tir(cls.fused_matmul38_add37_multiply22_tir_sigmoid_multiply23, (lv55, lv732, lv733), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv734: R.Tensor((3072, 768), dtype="float32") = model_params[129]
            lv735: R.Tensor((768,), dtype="float32") = model_params[6]
            lv453 = R.call_tir(cls.fused_matmul39_add35_add33, (lv452, lv734, lv735, lv451), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv736: R.Tensor((768,), dtype="float32") = model_params[32]
            lv737: R.Tensor((768,), dtype="float32") = model_params[31]
            lv66 = R.call_tir(cls.layer_norm4, (lv453, lv736, lv737), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv738: R.Tensor((768, 768), dtype="float32") = model_params[130]
            lv739: R.Tensor((768,), dtype="float32") = model_params[39]
            lv454 = R.call_tir(cls.fused_matmul35_add35_multiply21, (lv66, lv738, lv739), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv740: R.Tensor((768, 768), dtype="float32") = model_params[131]
            lv741: R.Tensor((768,), dtype="float32") = model_params[37]
            lv455 = R.call_tir(cls.fused_matmul35_add35, (lv66, lv740, lv741), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv456 = R.call_tir(cls.fused_reshape44_transpose40_reshape45_transpose41, (lv455,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv742: R.Tensor((768, 768), dtype="float32") = model_params[132]
            lv743: R.Tensor((768,), dtype="float32") = model_params[40]
            lv457 = R.call_tir(cls.fused_matmul35_add35, (lv66, lv742, lv743), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv458 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv457,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv459 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv454,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv87 = R.call_tir(cls.matmul36, (lv459, lv456), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv460 = R.call_tir(cls.fused_reshape46_add36_reshape47, (lv87, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv91 = R.call_tir(cls.softmax8, (lv460,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv92 = R.call_tir(cls.matmul37, (lv91, lv458), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv461 = R.call_tir(cls.fused_reshape48_transpose42_reshape49, (lv92,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv744: R.Tensor((768, 768), dtype="float32") = model_params[133]
            lv745: R.Tensor((768,), dtype="float32") = model_params[38]
            lv462 = R.call_tir(cls.fused_matmul35_add35_add33, (lv461, lv744, lv745, lv453), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv746: R.Tensor((768,), dtype="float32") = model_params[34]
            lv747: R.Tensor((768,), dtype="float32") = model_params[33]
            lv100 = R.call_tir(cls.layer_norm4, (lv462, lv746, lv747), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv748: R.Tensor((768, 3072), dtype="float32") = model_params[134]
            lv749: R.Tensor((3072,), dtype="float32") = model_params[35]
            lv463 = R.call_tir(cls.fused_matmul38_add37_multiply22_tir_sigmoid_multiply23, (lv100, lv748, lv749), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv750: R.Tensor((3072, 768), dtype="float32") = model_params[135]
            lv751: R.Tensor((768,), dtype="float32") = model_params[36]
            lv464 = R.call_tir(cls.fused_matmul39_add35_add33, (lv463, lv750, lv751, lv462), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv752: R.Tensor((768,), dtype="float32") = model_params[42]
            lv753: R.Tensor((768,), dtype="float32") = model_params[41]
            lv111 = R.call_tir(cls.layer_norm4, (lv464, lv752, lv753), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv754: R.Tensor((768, 768), dtype="float32") = model_params[136]
            lv755: R.Tensor((768,), dtype="float32") = model_params[49]
            lv465 = R.call_tir(cls.fused_matmul35_add35_multiply21, (lv111, lv754, lv755), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv756: R.Tensor((768, 768), dtype="float32") = model_params[137]
            lv757: R.Tensor((768,), dtype="float32") = model_params[47]
            lv466 = R.call_tir(cls.fused_matmul35_add35, (lv111, lv756, lv757), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv467 = R.call_tir(cls.fused_reshape44_transpose40_reshape45_transpose41, (lv466,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv758: R.Tensor((768, 768), dtype="float32") = model_params[138]
            lv759: R.Tensor((768,), dtype="float32") = model_params[50]
            lv468 = R.call_tir(cls.fused_matmul35_add35, (lv111, lv758, lv759), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv469 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv468,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv470 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv465,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv132 = R.call_tir(cls.matmul36, (lv470, lv467), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv471 = R.call_tir(cls.fused_reshape46_add36_reshape47, (lv132, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv136 = R.call_tir(cls.softmax8, (lv471,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv137 = R.call_tir(cls.matmul37, (lv136, lv469), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv472 = R.call_tir(cls.fused_reshape48_transpose42_reshape49, (lv137,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv760: R.Tensor((768, 768), dtype="float32") = model_params[139]
            lv761: R.Tensor((768,), dtype="float32") = model_params[48]
            lv473 = R.call_tir(cls.fused_matmul35_add35_add33, (lv472, lv760, lv761, lv464), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv762: R.Tensor((768,), dtype="float32") = model_params[44]
            lv763: R.Tensor((768,), dtype="float32") = model_params[43]
            lv145 = R.call_tir(cls.layer_norm4, (lv473, lv762, lv763), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv764: R.Tensor((768, 3072), dtype="float32") = model_params[140]
            lv765: R.Tensor((3072,), dtype="float32") = model_params[45]
            lv474 = R.call_tir(cls.fused_matmul38_add37_multiply22_tir_sigmoid_multiply23, (lv145, lv764, lv765), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv766: R.Tensor((3072, 768), dtype="float32") = model_params[141]
            lv767: R.Tensor((768,), dtype="float32") = model_params[46]
            lv475 = R.call_tir(cls.fused_matmul39_add35_add33, (lv474, lv766, lv767, lv473), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv768: R.Tensor((768,), dtype="float32") = model_params[52]
            lv769: R.Tensor((768,), dtype="float32") = model_params[51]
            lv156 = R.call_tir(cls.layer_norm4, (lv475, lv768, lv769), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv770: R.Tensor((768, 768), dtype="float32") = model_params[142]
            lv771: R.Tensor((768,), dtype="float32") = model_params[59]
            lv476 = R.call_tir(cls.fused_matmul35_add35_multiply21, (lv156, lv770, lv771), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv772: R.Tensor((768, 768), dtype="float32") = model_params[143]
            lv773: R.Tensor((768,), dtype="float32") = model_params[57]
            lv477 = R.call_tir(cls.fused_matmul35_add35, (lv156, lv772, lv773), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv478 = R.call_tir(cls.fused_reshape44_transpose40_reshape45_transpose41, (lv477,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv774: R.Tensor((768, 768), dtype="float32") = model_params[144]
            lv775: R.Tensor((768,), dtype="float32") = model_params[60]
            lv479 = R.call_tir(cls.fused_matmul35_add35, (lv156, lv774, lv775), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv480 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv479,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv481 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv476,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv177 = R.call_tir(cls.matmul36, (lv481, lv478), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv482 = R.call_tir(cls.fused_reshape46_add36_reshape47, (lv177, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv181 = R.call_tir(cls.softmax8, (lv482,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv182 = R.call_tir(cls.matmul37, (lv181, lv480), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv483 = R.call_tir(cls.fused_reshape48_transpose42_reshape49, (lv182,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv776: R.Tensor((768, 768), dtype="float32") = model_params[145]
            lv777: R.Tensor((768,), dtype="float32") = model_params[58]
            lv484 = R.call_tir(cls.fused_matmul35_add35_add33, (lv483, lv776, lv777, lv475), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv778: R.Tensor((768,), dtype="float32") = model_params[54]
            lv779: R.Tensor((768,), dtype="float32") = model_params[53]
            lv190 = R.call_tir(cls.layer_norm4, (lv484, lv778, lv779), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv780: R.Tensor((768, 3072), dtype="float32") = model_params[146]
            lv781: R.Tensor((3072,), dtype="float32") = model_params[55]
            lv485 = R.call_tir(cls.fused_matmul38_add37_multiply22_tir_sigmoid_multiply23, (lv190, lv780, lv781), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv782: R.Tensor((3072, 768), dtype="float32") = model_params[147]
            lv783: R.Tensor((768,), dtype="float32") = model_params[56]
            lv486 = R.call_tir(cls.fused_matmul39_add35_add33, (lv485, lv782, lv783, lv484), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv784: R.Tensor((768,), dtype="float32") = model_params[62]
            lv785: R.Tensor((768,), dtype="float32") = model_params[61]
            lv201 = R.call_tir(cls.layer_norm4, (lv486, lv784, lv785), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv786: R.Tensor((768, 768), dtype="float32") = model_params[148]
            lv787: R.Tensor((768,), dtype="float32") = model_params[69]
            lv487 = R.call_tir(cls.fused_matmul35_add35_multiply21, (lv201, lv786, lv787), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv788: R.Tensor((768, 768), dtype="float32") = model_params[149]
            lv789: R.Tensor((768,), dtype="float32") = model_params[67]
            lv488 = R.call_tir(cls.fused_matmul35_add35, (lv201, lv788, lv789), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv489 = R.call_tir(cls.fused_reshape44_transpose40_reshape45_transpose41, (lv488,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv790: R.Tensor((768, 768), dtype="float32") = model_params[150]
            lv791: R.Tensor((768,), dtype="float32") = model_params[70]
            lv490 = R.call_tir(cls.fused_matmul35_add35, (lv201, lv790, lv791), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv491 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv490,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv492 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv487,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv222 = R.call_tir(cls.matmul36, (lv492, lv489), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv493 = R.call_tir(cls.fused_reshape46_add36_reshape47, (lv222, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv226 = R.call_tir(cls.softmax8, (lv493,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv227 = R.call_tir(cls.matmul37, (lv226, lv491), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv494 = R.call_tir(cls.fused_reshape48_transpose42_reshape49, (lv227,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv792: R.Tensor((768, 768), dtype="float32") = model_params[151]
            lv793: R.Tensor((768,), dtype="float32") = model_params[68]
            lv495 = R.call_tir(cls.fused_matmul35_add35_add33, (lv494, lv792, lv793, lv486), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv794: R.Tensor((768,), dtype="float32") = model_params[64]
            lv795: R.Tensor((768,), dtype="float32") = model_params[63]
            lv235 = R.call_tir(cls.layer_norm4, (lv495, lv794, lv795), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv796: R.Tensor((768, 3072), dtype="float32") = model_params[152]
            lv797: R.Tensor((3072,), dtype="float32") = model_params[65]
            lv496 = R.call_tir(cls.fused_matmul38_add37_multiply22_tir_sigmoid_multiply23, (lv235, lv796, lv797), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv798: R.Tensor((3072, 768), dtype="float32") = model_params[153]
            lv799: R.Tensor((768,), dtype="float32") = model_params[66]
            lv497 = R.call_tir(cls.fused_matmul39_add35_add33, (lv496, lv798, lv799, lv495), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv800: R.Tensor((768,), dtype="float32") = model_params[72]
            lv801: R.Tensor((768,), dtype="float32") = model_params[71]
            lv246 = R.call_tir(cls.layer_norm4, (lv497, lv800, lv801), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv802: R.Tensor((768, 768), dtype="float32") = model_params[154]
            lv803: R.Tensor((768,), dtype="float32") = model_params[79]
            lv498 = R.call_tir(cls.fused_matmul35_add35_multiply21, (lv246, lv802, lv803), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv804: R.Tensor((768, 768), dtype="float32") = model_params[155]
            lv805: R.Tensor((768,), dtype="float32") = model_params[77]
            lv499 = R.call_tir(cls.fused_matmul35_add35, (lv246, lv804, lv805), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv500 = R.call_tir(cls.fused_reshape44_transpose40_reshape45_transpose41, (lv499,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv806: R.Tensor((768, 768), dtype="float32") = model_params[156]
            lv807: R.Tensor((768,), dtype="float32") = model_params[80]
            lv501 = R.call_tir(cls.fused_matmul35_add35, (lv246, lv806, lv807), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv502 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv501,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv503 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv498,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv267 = R.call_tir(cls.matmul36, (lv503, lv500), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv504 = R.call_tir(cls.fused_reshape46_add36_reshape47, (lv267, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv271 = R.call_tir(cls.softmax8, (lv504,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv272 = R.call_tir(cls.matmul37, (lv271, lv502), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv505 = R.call_tir(cls.fused_reshape48_transpose42_reshape49, (lv272,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv808: R.Tensor((768, 768), dtype="float32") = model_params[157]
            lv809: R.Tensor((768,), dtype="float32") = model_params[78]
            lv506 = R.call_tir(cls.fused_matmul35_add35_add33, (lv505, lv808, lv809, lv497), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv810: R.Tensor((768,), dtype="float32") = model_params[74]
            lv811: R.Tensor((768,), dtype="float32") = model_params[73]
            lv280 = R.call_tir(cls.layer_norm4, (lv506, lv810, lv811), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv812: R.Tensor((768, 3072), dtype="float32") = model_params[158]
            lv813: R.Tensor((3072,), dtype="float32") = model_params[75]
            lv507 = R.call_tir(cls.fused_matmul38_add37_multiply22_tir_sigmoid_multiply23, (lv280, lv812, lv813), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv814: R.Tensor((3072, 768), dtype="float32") = model_params[159]
            lv815: R.Tensor((768,), dtype="float32") = model_params[76]
            lv508 = R.call_tir(cls.fused_matmul39_add35_add33, (lv507, lv814, lv815, lv506), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv816: R.Tensor((768,), dtype="float32") = model_params[82]
            lv817: R.Tensor((768,), dtype="float32") = model_params[81]
            lv291 = R.call_tir(cls.layer_norm4, (lv508, lv816, lv817), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv818: R.Tensor((768, 768), dtype="float32") = model_params[160]
            lv819: R.Tensor((768,), dtype="float32") = model_params[89]
            lv509 = R.call_tir(cls.fused_matmul35_add35_multiply21, (lv291, lv818, lv819), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv820: R.Tensor((768, 768), dtype="float32") = model_params[161]
            lv821: R.Tensor((768,), dtype="float32") = model_params[87]
            lv510 = R.call_tir(cls.fused_matmul35_add35, (lv291, lv820, lv821), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv511 = R.call_tir(cls.fused_reshape44_transpose40_reshape45_transpose41, (lv510,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv822: R.Tensor((768, 768), dtype="float32") = model_params[162]
            lv823: R.Tensor((768,), dtype="float32") = model_params[90]
            lv512 = R.call_tir(cls.fused_matmul35_add35, (lv291, lv822, lv823), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv513 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv512,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv514 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv509,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv312 = R.call_tir(cls.matmul36, (lv514, lv511), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv515 = R.call_tir(cls.fused_reshape46_add36_reshape47, (lv312, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv316 = R.call_tir(cls.softmax8, (lv515,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv317 = R.call_tir(cls.matmul37, (lv316, lv513), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv516 = R.call_tir(cls.fused_reshape48_transpose42_reshape49, (lv317,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv824: R.Tensor((768, 768), dtype="float32") = model_params[163]
            lv825: R.Tensor((768,), dtype="float32") = model_params[88]
            lv517 = R.call_tir(cls.fused_matmul35_add35_add33, (lv516, lv824, lv825, lv508), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv826: R.Tensor((768,), dtype="float32") = model_params[84]
            lv827: R.Tensor((768,), dtype="float32") = model_params[83]
            lv325 = R.call_tir(cls.layer_norm4, (lv517, lv826, lv827), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv828: R.Tensor((768, 3072), dtype="float32") = model_params[164]
            lv829: R.Tensor((3072,), dtype="float32") = model_params[85]
            lv518 = R.call_tir(cls.fused_matmul38_add37_multiply22_tir_sigmoid_multiply23, (lv325, lv828, lv829), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv830: R.Tensor((3072, 768), dtype="float32") = model_params[165]
            lv831: R.Tensor((768,), dtype="float32") = model_params[86]
            lv519 = R.call_tir(cls.fused_matmul39_add35_add33, (lv518, lv830, lv831, lv517), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv832: R.Tensor((768,), dtype="float32") = model_params[92]
            lv833: R.Tensor((768,), dtype="float32") = model_params[91]
            lv336 = R.call_tir(cls.layer_norm4, (lv519, lv832, lv833), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv834: R.Tensor((768, 768), dtype="float32") = model_params[166]
            lv835: R.Tensor((768,), dtype="float32") = model_params[99]
            lv520 = R.call_tir(cls.fused_matmul35_add35_multiply21, (lv336, lv834, lv835), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv836: R.Tensor((768, 768), dtype="float32") = model_params[167]
            lv837: R.Tensor((768,), dtype="float32") = model_params[97]
            lv521 = R.call_tir(cls.fused_matmul35_add35, (lv336, lv836, lv837), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv522 = R.call_tir(cls.fused_reshape44_transpose40_reshape45_transpose41, (lv521,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv838: R.Tensor((768, 768), dtype="float32") = model_params[168]
            lv839: R.Tensor((768,), dtype="float32") = model_params[100]
            lv523 = R.call_tir(cls.fused_matmul35_add35, (lv336, lv838, lv839), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv524 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv523,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv525 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv520,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv357 = R.call_tir(cls.matmul36, (lv525, lv522), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv526 = R.call_tir(cls.fused_reshape46_add36_reshape47, (lv357, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv361 = R.call_tir(cls.softmax8, (lv526,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv362 = R.call_tir(cls.matmul37, (lv361, lv524), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv527 = R.call_tir(cls.fused_reshape48_transpose42_reshape49, (lv362,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv840: R.Tensor((768, 768), dtype="float32") = model_params[169]
            lv841: R.Tensor((768,), dtype="float32") = model_params[98]
            lv528 = R.call_tir(cls.fused_matmul35_add35_add33, (lv527, lv840, lv841, lv519), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv842: R.Tensor((768,), dtype="float32") = model_params[94]
            lv843: R.Tensor((768,), dtype="float32") = model_params[93]
            lv370 = R.call_tir(cls.layer_norm4, (lv528, lv842, lv843), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv844: R.Tensor((768, 3072), dtype="float32") = model_params[170]
            lv845: R.Tensor((3072,), dtype="float32") = model_params[95]
            lv529 = R.call_tir(cls.fused_matmul38_add37_multiply22_tir_sigmoid_multiply23, (lv370, lv844, lv845), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv846: R.Tensor((3072, 768), dtype="float32") = model_params[171]
            lv847: R.Tensor((768,), dtype="float32") = model_params[96]
            lv530 = R.call_tir(cls.fused_matmul39_add35_add33, (lv529, lv846, lv847, lv528), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv848: R.Tensor((768,), dtype="float32") = model_params[102]
            lv849: R.Tensor((768,), dtype="float32") = model_params[101]
            lv381 = R.call_tir(cls.layer_norm4, (lv530, lv848, lv849), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv850: R.Tensor((768, 768), dtype="float32") = model_params[172]
            lv851: R.Tensor((768,), dtype="float32") = model_params[109]
            lv531 = R.call_tir(cls.fused_matmul35_add35_multiply21, (lv381, lv850, lv851), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv852: R.Tensor((768, 768), dtype="float32") = model_params[173]
            lv853: R.Tensor((768,), dtype="float32") = model_params[107]
            lv532 = R.call_tir(cls.fused_matmul35_add35, (lv381, lv852, lv853), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv533 = R.call_tir(cls.fused_reshape44_transpose40_reshape45_transpose41, (lv532,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv854: R.Tensor((768, 768), dtype="float32") = model_params[174]
            lv855: R.Tensor((768,), dtype="float32") = model_params[110]
            lv534 = R.call_tir(cls.fused_matmul35_add35, (lv381, lv854, lv855), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv535 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv534,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv536 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv531,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv402 = R.call_tir(cls.matmul36, (lv536, lv533), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv537 = R.call_tir(cls.fused_reshape46_add36_reshape47, (lv402, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv406 = R.call_tir(cls.softmax8, (lv537,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv407 = R.call_tir(cls.matmul37, (lv406, lv535), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv538 = R.call_tir(cls.fused_reshape48_transpose42_reshape49, (lv407,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv856: R.Tensor((768, 768), dtype="float32") = model_params[175]
            lv857: R.Tensor((768,), dtype="float32") = model_params[108]
            lv539 = R.call_tir(cls.fused_matmul35_add35_add33, (lv538, lv856, lv857, lv530), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv858: R.Tensor((768,), dtype="float32") = model_params[104]
            lv859: R.Tensor((768,), dtype="float32") = model_params[103]
            lv415 = R.call_tir(cls.layer_norm4, (lv539, lv858, lv859), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv860: R.Tensor((768, 3072), dtype="float32") = model_params[176]
            lv861: R.Tensor((3072,), dtype="float32") = model_params[105]
            lv540 = R.call_tir(cls.fused_matmul38_add37_multiply22_tir_sigmoid_multiply23, (lv415, lv860, lv861), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv862: R.Tensor((3072, 768), dtype="float32") = model_params[177]
            lv863: R.Tensor((768,), dtype="float32") = model_params[106]
            lv541 = R.call_tir(cls.fused_matmul39_add35_add33, (lv540, lv862, lv863, lv539), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv864: R.Tensor((768,), dtype="float32") = model_params[112]
            lv865: R.Tensor((768,), dtype="float32") = model_params[111]
            lv426 = R.call_tir(cls.layer_norm4, (lv541, lv864, lv865), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv866: R.Tensor((768, 768), dtype="float32") = model_params[178]
            lv867: R.Tensor((768,), dtype="float32") = model_params[119]
            lv542 = R.call_tir(cls.fused_matmul35_add35_multiply21, (lv426, lv866, lv867), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv868: R.Tensor((768, 768), dtype="float32") = model_params[179]
            lv869: R.Tensor((768,), dtype="float32") = model_params[117]
            lv543 = R.call_tir(cls.fused_matmul35_add35, (lv426, lv868, lv869), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv544 = R.call_tir(cls.fused_reshape44_transpose40_reshape45_transpose41, (lv543,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv870: R.Tensor((768, 768), dtype="float32") = model_params[180]
            lv871: R.Tensor((768,), dtype="float32") = model_params[120]
            lv545 = R.call_tir(cls.fused_matmul35_add35, (lv426, lv870, lv871), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv546 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv545,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv547 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv542,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv447_1 = R.call_tir(cls.matmul36, (lv547, lv544), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv548 = R.call_tir(cls.fused_reshape46_add36_reshape47, (lv447_1, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv451_1 = R.call_tir(cls.softmax8, (lv548,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv452_1 = R.call_tir(cls.matmul37, (lv451_1, lv546), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv549 = R.call_tir(cls.fused_reshape48_transpose42_reshape49, (lv452_1,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv872: R.Tensor((768, 768), dtype="float32") = model_params[181]
            lv873: R.Tensor((768,), dtype="float32") = model_params[118]
            lv550 = R.call_tir(cls.fused_matmul35_add35_add33, (lv549, lv872, lv873, lv541), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv874: R.Tensor((768,), dtype="float32") = model_params[114]
            lv875: R.Tensor((768,), dtype="float32") = model_params[113]
            lv460_1 = R.call_tir(cls.layer_norm4, (lv550, lv874, lv875), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv876: R.Tensor((768, 3072), dtype="float32") = model_params[182]
            lv877: R.Tensor((3072,), dtype="float32") = model_params[115]
            lv551 = R.call_tir(cls.fused_matmul38_add37_multiply22_tir_sigmoid_multiply23, (lv460_1, lv876, lv877), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv878: R.Tensor((3072, 768), dtype="float32") = model_params[183]
            lv879: R.Tensor((768,), dtype="float32") = model_params[116]
            lv552 = R.call_tir(cls.fused_matmul39_add35_add33, (lv551, lv878, lv879, lv550), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv880: R.Tensor((768,), dtype="float32") = model_params[12]
            lv881: R.Tensor((768,), dtype="float32") = model_params[11]
            lv471_1 = R.call_tir(cls.layer_norm4, (lv552, lv880, lv881), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv882: R.Tensor((768, 768), dtype="float32") = model_params[184]
            lv883: R.Tensor((768,), dtype="float32") = model_params[19]
            lv553 = R.call_tir(cls.fused_matmul35_add35_multiply21, (lv471_1, lv882, lv883), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv884: R.Tensor((768, 768), dtype="float32") = model_params[185]
            lv885: R.Tensor((768,), dtype="float32") = model_params[17]
            lv554 = R.call_tir(cls.fused_matmul35_add35, (lv471_1, lv884, lv885), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv555 = R.call_tir(cls.fused_reshape44_transpose40_reshape45_transpose41, (lv554,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv886: R.Tensor((768, 768), dtype="float32") = model_params[186]
            lv887: R.Tensor((768,), dtype="float32") = model_params[20]
            lv556 = R.call_tir(cls.fused_matmul35_add35, (lv471_1, lv886, lv887), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv557 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv556,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv558 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv553,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv492_1 = R.call_tir(cls.matmul36, (lv558, lv555), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv559 = R.call_tir(cls.fused_reshape46_add36_reshape47, (lv492_1, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv496_1 = R.call_tir(cls.softmax8, (lv559,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv497_1 = R.call_tir(cls.matmul37, (lv496_1, lv557), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv560 = R.call_tir(cls.fused_reshape48_transpose42_reshape49, (lv497_1,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv888: R.Tensor((768, 768), dtype="float32") = model_params[187]
            lv889: R.Tensor((768,), dtype="float32") = model_params[18]
            lv561 = R.call_tir(cls.fused_matmul35_add35_add33, (lv560, lv888, lv889, lv552), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv890: R.Tensor((768,), dtype="float32") = model_params[14]
            lv891: R.Tensor((768,), dtype="float32") = model_params[13]
            lv505_1 = R.call_tir(cls.layer_norm4, (lv561, lv890, lv891), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv892: R.Tensor((768, 3072), dtype="float32") = model_params[188]
            lv893: R.Tensor((3072,), dtype="float32") = model_params[15]
            lv562 = R.call_tir(cls.fused_matmul38_add37_multiply22_tir_sigmoid_multiply23, (lv505_1, lv892, lv893), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv894: R.Tensor((3072, 768), dtype="float32") = model_params[189]
            lv895: R.Tensor((768,), dtype="float32") = model_params[16]
            lv563 = R.call_tir(cls.fused_matmul39_add35_add33, (lv562, lv894, lv895, lv561), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv896: R.Tensor((768,), dtype="float32") = model_params[22]
            lv897: R.Tensor((768,), dtype="float32") = model_params[21]
            lv516_1 = R.call_tir(cls.layer_norm4, (lv563, lv896, lv897), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv898: R.Tensor((768, 768), dtype="float32") = model_params[190]
            lv899: R.Tensor((768,), dtype="float32") = model_params[29]
            lv564 = R.call_tir(cls.fused_matmul35_add35_multiply21, (lv516_1, lv898, lv899), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv900: R.Tensor((768, 768), dtype="float32") = model_params[191]
            lv901: R.Tensor((768,), dtype="float32") = model_params[27]
            lv565 = R.call_tir(cls.fused_matmul35_add35, (lv516_1, lv900, lv901), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv566 = R.call_tir(cls.fused_reshape44_transpose40_reshape45_transpose41, (lv565,), out_sinfo=R.Tensor((12, 64, 77), dtype="float32"))
            lv902: R.Tensor((768, 768), dtype="float32") = model_params[192]
            lv903: R.Tensor((768,), dtype="float32") = model_params[30]
            lv567 = R.call_tir(cls.fused_matmul35_add35, (lv516_1, lv902, lv903), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv568 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv567,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv569 = R.call_tir(cls.fused_reshape44_transpose40_reshape45, (lv564,), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv537_1 = R.call_tir(cls.matmul36, (lv569, lv566), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv570 = R.call_tir(cls.fused_reshape46_add36_reshape47, (lv537_1, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv541_1 = R.call_tir(cls.softmax8, (lv570,), out_sinfo=R.Tensor((12, 77, 77), dtype="float32"))
            lv542_1 = R.call_tir(cls.matmul37, (lv541_1, lv568), out_sinfo=R.Tensor((12, 77, 64), dtype="float32"))
            lv571 = R.call_tir(cls.fused_reshape48_transpose42_reshape49, (lv542_1,), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv904: R.Tensor((768, 768), dtype="float32") = model_params[193]
            lv905: R.Tensor((768,), dtype="float32") = model_params[28]
            lv572 = R.call_tir(cls.fused_matmul35_add35_add33, (lv571, lv904, lv905, lv563), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv906: R.Tensor((768,), dtype="float32") = model_params[24]
            lv907: R.Tensor((768,), dtype="float32") = model_params[23]
            lv550_1 = R.call_tir(cls.layer_norm4, (lv572, lv906, lv907), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv908: R.Tensor((768, 3072), dtype="float32") = model_params[194]
            lv909: R.Tensor((3072,), dtype="float32") = model_params[25]
            lv573 = R.call_tir(cls.fused_matmul38_add37_multiply22_tir_sigmoid_multiply23, (lv550_1, lv908, lv909), out_sinfo=R.Tensor((1, 77, 3072), dtype="float32"))
            lv910: R.Tensor((3072, 768), dtype="float32") = model_params[195]
            lv911: R.Tensor((768,), dtype="float32") = model_params[26]
            lv574 = R.call_tir(cls.fused_matmul39_add35_add33, (lv573, lv910, lv911, lv572), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            lv912: R.Tensor((768,), dtype="float32") = model_params[122]
            lv913: R.Tensor((768,), dtype="float32") = model_params[121]
            lv561_1 = R.call_tir(cls.layer_norm4, (lv574, lv912, lv913), out_sinfo=R.Tensor((1, 77, 768), dtype="float32"))
            gv: R.Tensor((1, 77, 768), dtype="float32") = lv561_1
            R.output(gv)
        return gv

    @R.function
    def concat_embeddings(cond_embeddings: R.Tensor((1, 77, 768), dtype="float32"), uncond_embeddings: R.Tensor((1, 77, 768), dtype="float32")) -> R.Tensor((2, 77, 768), dtype="float32"):
        cls = Module
        with R.dataflow():
            gv = R.call_tir(cls.concatenate10, (cond_embeddings, uncond_embeddings), out_sinfo=R.Tensor((2, 77, 768), dtype="float32"))
            R.output(gv)
        return gv

    @R.function
    def dpm_solver_multistep_scheduler_convert_model_output(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), alpha: R.Tensor((), dtype="float32"), sigma: R.Tensor((), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv = R.call_tir(cls.multiply, (sigma, model_output), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv1 = R.call_tir(cls.subtract, (sample, gv), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        converted_model_output = R.call_tir(cls.divide, (gv1, alpha), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return converted_model_output

    @R.function
    def dpm_solver_multistep_scheduler_step(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), last_model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), c0: R.Tensor((), dtype="float32"), c1: R.Tensor((), dtype="float32"), c2: R.Tensor((), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv2 = R.call_tir(cls.multiply, (c0, sample), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv3 = R.call_tir(cls.multiply, (c1, model_output), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv4 = R.call_tir(cls.subtract, (gv2, gv3), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv5 = R.call_tir(cls.subtract, (model_output, last_model_output), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv6 = R.call_tir(cls.multiply, (c2, gv5), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        prev_sample = R.call_tir(cls.subtract, (gv4, gv6), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return prev_sample

    @R.function
    def image_to_rgba(x: R.Tensor((1, 512, 512, 3), dtype="float32")) -> R.Tensor((512, 512), dtype="uint32"):
        cls = Module
        with R.dataflow():
            gv = R.call_tir(cls.tir_image_to_rgba, (x,), out_sinfo=R.Tensor((512, 512), dtype="uint32"))
            R.output(gv)
        return gv

    @R.function
    def pndm_scheduler_step_0(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), sample_coeff: R.Tensor((), dtype="float32"), alpha_diff: R.Tensor((), dtype="float32"), model_output_denom_coeff: R.Tensor((), dtype="float32"), ets0: R.Tensor((1, 4, 64, 64), dtype="float32"), ets1: R.Tensor((1, 4, 64, 64), dtype="float32"), ets2: R.Tensor((1, 4, 64, 64), dtype="float32"), ets3: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv = R.call_tir(cls.multiply, (sample_coeff, sample), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv1 = R.call_tir(cls.multiply, (alpha_diff, model_output), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv2 = R.call_tir(cls.divide, (gv1, model_output_denom_coeff), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        prev_sample = R.call_tir(cls.subtract, (gv, gv2), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return prev_sample

    @R.function
    def pndm_scheduler_step_1(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), sample_coeff: R.Tensor((), dtype="float32"), alpha_diff: R.Tensor((), dtype="float32"), model_output_denom_coeff: R.Tensor((), dtype="float32"), ets0: R.Tensor((1, 4, 64, 64), dtype="float32"), ets1: R.Tensor((1, 4, 64, 64), dtype="float32"), ets2: R.Tensor((1, 4, 64, 64), dtype="float32"), ets3: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv3 = R.call_tir(cls.multiply, (sample_coeff, sample), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv4 = R.call_tir(cls.add32, (model_output, ets3), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv5 = R.call_tir(cls.divide5, (gv4,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv6 = R.call_tir(cls.multiply, (alpha_diff, gv5), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv7 = R.call_tir(cls.divide, (gv6, model_output_denom_coeff), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        prev_sample1 = R.call_tir(cls.subtract, (gv3, gv7), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return prev_sample1

    @R.function
    def pndm_scheduler_step_2(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), sample_coeff: R.Tensor((), dtype="float32"), alpha_diff: R.Tensor((), dtype="float32"), model_output_denom_coeff: R.Tensor((), dtype="float32"), ets0: R.Tensor((1, 4, 64, 64), dtype="float32"), ets1: R.Tensor((1, 4, 64, 64), dtype="float32"), ets2: R.Tensor((1, 4, 64, 64), dtype="float32"), ets3: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv8 = R.call_tir(cls.multiply, (sample_coeff, sample), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv9 = R.call_tir(cls.multiply24, (ets3,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv10 = R.call_tir(cls.subtract, (gv9, ets2), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv11 = R.call_tir(cls.divide5, (gv10,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv12 = R.call_tir(cls.multiply, (alpha_diff, gv11), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv13 = R.call_tir(cls.divide, (gv12, model_output_denom_coeff), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        prev_sample2 = R.call_tir(cls.subtract, (gv8, gv13), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return prev_sample2

    @R.function
    def pndm_scheduler_step_3(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), sample_coeff: R.Tensor((), dtype="float32"), alpha_diff: R.Tensor((), dtype="float32"), model_output_denom_coeff: R.Tensor((), dtype="float32"), ets0: R.Tensor((1, 4, 64, 64), dtype="float32"), ets1: R.Tensor((1, 4, 64, 64), dtype="float32"), ets2: R.Tensor((1, 4, 64, 64), dtype="float32"), ets3: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv14 = R.call_tir(cls.multiply, (sample_coeff, sample), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv15 = R.call_tir(cls.multiply25, (ets3,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv16 = R.call_tir(cls.multiply26, (ets2,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv17 = R.call_tir(cls.subtract, (gv15, gv16), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv18 = R.call_tir(cls.multiply27, (ets1,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv19 = R.call_tir(cls.add32, (gv17, gv18), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv20 = R.call_tir(cls.divide6, (gv19,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv21 = R.call_tir(cls.multiply, (alpha_diff, gv20), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv22 = R.call_tir(cls.divide, (gv21, model_output_denom_coeff), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        prev_sample3 = R.call_tir(cls.subtract, (gv14, gv22), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return prev_sample3

    @R.function
    def pndm_scheduler_step_4(sample: R.Tensor((1, 4, 64, 64), dtype="float32"), model_output: R.Tensor((1, 4, 64, 64), dtype="float32"), sample_coeff: R.Tensor((), dtype="float32"), alpha_diff: R.Tensor((), dtype="float32"), model_output_denom_coeff: R.Tensor((), dtype="float32"), ets0: R.Tensor((1, 4, 64, 64), dtype="float32"), ets1: R.Tensor((1, 4, 64, 64), dtype="float32"), ets2: R.Tensor((1, 4, 64, 64), dtype="float32"), ets3: R.Tensor((1, 4, 64, 64), dtype="float32")) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        cls = Module
        gv23 = R.call_tir(cls.multiply, (sample_coeff, sample), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv24 = R.call_tir(cls.multiply16, (ets3,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv25 = R.call_tir(cls.multiply17, (ets2,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv26 = R.call_tir(cls.subtract, (gv24, gv25), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv27 = R.call_tir(cls.multiply18, (ets1,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv28 = R.call_tir(cls.add32, (gv26, gv27), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv29 = R.call_tir(cls.multiply19, (ets0,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv30 = R.call_tir(cls.subtract, (gv28, gv29), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv31 = R.call_tir(cls.multiply20, (gv30,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv32 = R.call_tir(cls.multiply, (alpha_diff, gv31), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        gv33 = R.call_tir(cls.divide, (gv32, model_output_denom_coeff), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        prev_sample4 = R.call_tir(cls.subtract, (gv23, gv33), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
        return prev_sample4

    @R.function
    def unet(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), inp_1: R.Tensor((), dtype="int32"), inp_2: R.Tensor((2, 77, 768), dtype="float32"), model_params: R.Tuple(R.Tensor((320, 4, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((4, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 320, 3, 3), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 640, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((5120,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 2560, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 2560, 1, 1), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1920, 3, 3), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((1280, 1920, 1, 1), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280, 1280, 3, 3), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 1920, 3, 3), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 1920, 1, 1), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((1920,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 1280, 3, 3), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 1280, 1, 1), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 960, 3, 3), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((640, 960, 1, 1), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640, 640, 3, 3), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320, 320, 1, 1), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((1280,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 960, 3, 3), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 960, 1, 1), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((960,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 640, 3, 3), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 640, 3, 3), dtype="float32"), R.Tensor((320, 320, 3, 3), dtype="float32"), R.Tensor((320, 640, 1, 1), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((640,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320,), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((768, 1280), dtype="float32"), R.Tensor((1280, 1280), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((1280, 5120), dtype="float32"), R.Tensor((5120, 1280), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 1280, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1280, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((768, 640), dtype="float32"), R.Tensor((640, 640), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((640, 2560), dtype="float32"), R.Tensor((2560, 640), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 640, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((768, 320), dtype="float32"), R.Tensor((320, 320), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((320, 1280), dtype="float32"), R.Tensor((1280, 320), dtype="float32"), R.Tensor((1, 320, 1, 1), dtype="float32"), R.Tensor((1, 4, 1, 1), dtype="float32"))) -> R.Tensor((1, 4, 64, 64), dtype="float32"):
        R.func_attr({"global_symbol": "main", "num_input": 3})
        cls = Module
        with R.dataflow():
            lv = R.call_tir(cls.concatenate, (inp_0, inp_0), out_sinfo=R.Tensor((2, 4, 64, 64), dtype="float32"))
            lv_1 = R.call_tir(cls.fused_broadcast_to_strided_slice_reshape_cast_multiply1_multiply2_tir_sin_tir_cos_concatenate1_strided_slice1_reshape1_strided_slice2_reshape1_concatenate1, (inp_1, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv_2: R.Tensor((320, 1280), dtype="float32") = model_params[420]
            lv1: R.Tensor((1280,), dtype="float32") = model_params[184]
            lv1_1 = R.call_tir(cls.fused_matmul_add_silu, (lv_1, lv_2, lv1), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv2: R.Tensor((1280, 1280), dtype="float32") = model_params[421]
            lv3: R.Tensor((1280,), dtype="float32") = model_params[185]
            lv2_1 = R.call_tir(cls.fused_matmul1_add, (lv1_1, lv2, lv3), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv4: R.Tensor((320, 4, 3, 3), dtype="float32") = model_params[0]
            lv5: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[422]
            lv3_1 = R.call_tir(cls.fused_conv2d_add1, (lv, lv4, lv5), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv6: R.Tensor((320,), dtype="float32") = model_params[38]
            lv7: R.Tensor((320,), dtype="float32") = model_params[37]
            lv4_1 = R.call_tir(cls.fused_group_norm_silu1, (lv3_1, lv6, lv7), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv30 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv8: R.Tensor((1280, 320), dtype="float32") = model_params[424]
            lv9: R.Tensor((320,), dtype="float32") = model_params[41]
            lv5_1 = R.call_tir(cls.fused_matmul2_add2_strided_slice3, (lv30, lv8, lv9), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv35 = R.call_tir(cls.reshape3, (lv5_1,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv10: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[35]
            lv11: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[423]
            lv6_1 = R.call_tir(cls.fused_conv2d1_add1_add3, (lv4_1, lv10, lv11, lv35), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv12: R.Tensor((320,), dtype="float32") = model_params[40]
            lv13: R.Tensor((320,), dtype="float32") = model_params[39]
            lv7_1 = R.call_tir(cls.fused_group_norm_silu1, (lv6_1, lv12, lv13), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv14: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[36]
            lv15: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[425]
            lv8_1 = R.call_tir(cls.fused_conv2d1_add1_add4_divide1, (lv7_1, lv14, lv15, lv3_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv16: R.Tensor((320,), dtype="float32") = model_params[5]
            lv17: R.Tensor((320,), dtype="float32") = model_params[4]
            lv44 = R.call_tir(cls.group_norm1, (lv8_1, lv16, lv17), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv18: R.Tensor((320, 320, 1, 1), dtype="float32") = model_params[6]
            lv19: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[426]
            lv9_1 = R.call_tir(cls.fused_conv2d2_add1, (lv44, lv18, lv19), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv10_1 = R.call_tir(cls.fused_transpose3_reshape4, (lv9_1,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv20: R.Tensor((320,), dtype="float32") = model_params[14]
            lv21: R.Tensor((320,), dtype="float32") = model_params[13]
            lv50 = R.call_tir(cls.layer_norm, (lv10_1, lv20, lv21), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv22: R.Tensor((320, 320), dtype="float32") = model_params[427]
            lv52 = R.call_tir(cls.matmul3, (lv50, lv22), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv23: R.Tensor((320, 320), dtype="float32") = model_params[428]
            lv54 = R.call_tir(cls.matmul3, (lv50, lv23), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv24: R.Tensor((320, 320), dtype="float32") = model_params[429]
            lv56 = R.call_tir(cls.matmul3, (lv50, lv24), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv11_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv52,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv12_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6_transpose6, (lv54,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv13_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv56,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv14_1 = R.call_tir(cls.fused_matmul4_multiply3, (lv11_1, lv12_1), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv69 = R.call_tir(cls.softmax, (lv14_1,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv70 = R.call_tir(cls.matmul5, (lv69, lv13_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv15_1 = R.call_tir(cls.fused_reshape7_transpose7_reshape8, (lv70,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv25: R.Tensor((320, 320), dtype="float32") = model_params[430]
            lv26: R.Tensor((320,), dtype="float32") = model_params[8]
            lv16_1 = R.call_tir(cls.fused_matmul3_add5_add6, (lv15_1, lv25, lv26, lv10_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv27: R.Tensor((320,), dtype="float32") = model_params[16]
            lv28: R.Tensor((320,), dtype="float32") = model_params[15]
            lv78 = R.call_tir(cls.layer_norm, (lv16_1, lv27, lv28), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv29: R.Tensor((320, 320), dtype="float32") = model_params[431]
            lv80 = R.call_tir(cls.matmul3, (lv78, lv29), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv30_1: R.Tensor((768, 320), dtype="float32") = model_params[432]
            lv82 = R.call_tir(cls.matmul6, (inp_2, lv30_1), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv31: R.Tensor((768, 320), dtype="float32") = model_params[433]
            lv84 = R.call_tir(cls.matmul6, (inp_2, lv31), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv17_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv80,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv18_1 = R.call_tir(cls.fused_reshape9_transpose9_reshape10_transpose10, (lv82,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv19_1 = R.call_tir(cls.fused_reshape9_transpose9_reshape10, (lv84,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv20_1 = R.call_tir(cls.fused_matmul7_multiply4, (lv17_1, lv18_1), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv97 = R.call_tir(cls.softmax1, (lv20_1,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv98 = R.call_tir(cls.matmul8, (lv97, lv19_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv21_1 = R.call_tir(cls.fused_reshape7_transpose7_reshape8, (lv98,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv32: R.Tensor((320, 320), dtype="float32") = model_params[434]
            lv33: R.Tensor((320,), dtype="float32") = model_params[9]
            lv22_1 = R.call_tir(cls.fused_matmul3_add5_add6, (lv21_1, lv32, lv33, lv16_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv34: R.Tensor((320,), dtype="float32") = model_params[18]
            lv35_1: R.Tensor((320,), dtype="float32") = model_params[17]
            lv106 = R.call_tir(cls.layer_norm, (lv22_1, lv34, lv35_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv36: R.Tensor((320, 1280), dtype="float32") = model_params[436]
            lv37: R.Tensor((1280,), dtype="float32") = model_params[11]
            lv23_1 = R.call_tir(cls.fused_matmul9_add7_gelu, (lv106, lv36, lv37), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv38: R.Tensor((320, 1280), dtype="float32") = model_params[435]
            lv39: R.Tensor((1280,), dtype="float32") = model_params[10]
            lv24_1 = R.call_tir(cls.fused_matmul9_add7_multiply5, (lv106, lv38, lv39, lv23_1), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv40: R.Tensor((1280, 320), dtype="float32") = model_params[437]
            lv41: R.Tensor((320,), dtype="float32") = model_params[12]
            lv25_1 = R.call_tir(cls.fused_matmul10_add5_add6, (lv24_1, lv40, lv41, lv22_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv26_1 = R.call_tir(cls.fused_reshape11_transpose11, (lv25_1,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv42: R.Tensor((320, 320, 1, 1), dtype="float32") = model_params[7]
            lv43: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[438]
            lv27_1 = R.call_tir(cls.fused_conv2d2_add1_add4, (lv26_1, lv42, lv43, lv8_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv44_1: R.Tensor((320,), dtype="float32") = model_params[45]
            lv45: R.Tensor((320,), dtype="float32") = model_params[44]
            lv28_1 = R.call_tir(cls.fused_group_norm_silu1, (lv27_1, lv44_1, lv45), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv130 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv46: R.Tensor((1280, 320), dtype="float32") = model_params[440]
            lv47: R.Tensor((320,), dtype="float32") = model_params[48]
            lv29_1 = R.call_tir(cls.fused_matmul2_add2_strided_slice3, (lv130, lv46, lv47), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv135 = R.call_tir(cls.reshape3, (lv29_1,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv48: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[42]
            lv49: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[439]
            lv30_2 = R.call_tir(cls.fused_conv2d1_add1_add3, (lv28_1, lv48, lv49, lv135), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv50_1: R.Tensor((320,), dtype="float32") = model_params[47]
            lv51: R.Tensor((320,), dtype="float32") = model_params[46]
            lv31_1 = R.call_tir(cls.fused_group_norm_silu1, (lv30_2, lv50_1, lv51), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv52_1: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[43]
            lv53: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[441]
            lv32_1 = R.call_tir(cls.fused_conv2d1_add1_add4_divide1, (lv31_1, lv52_1, lv53, lv27_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv54_1: R.Tensor((320,), dtype="float32") = model_params[20]
            lv55: R.Tensor((320,), dtype="float32") = model_params[19]
            lv144 = R.call_tir(cls.group_norm1, (lv32_1, lv54_1, lv55), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv56_1: R.Tensor((320, 320, 1, 1), dtype="float32") = model_params[21]
            lv57: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[442]
            lv33_1 = R.call_tir(cls.fused_conv2d2_add1, (lv144, lv56_1, lv57), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv34_1 = R.call_tir(cls.fused_transpose3_reshape4, (lv33_1,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv58: R.Tensor((320,), dtype="float32") = model_params[29]
            lv59: R.Tensor((320,), dtype="float32") = model_params[28]
            lv150 = R.call_tir(cls.layer_norm, (lv34_1, lv58, lv59), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv60: R.Tensor((320, 320), dtype="float32") = model_params[443]
            lv152 = R.call_tir(cls.matmul3, (lv150, lv60), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv61: R.Tensor((320, 320), dtype="float32") = model_params[444]
            lv154 = R.call_tir(cls.matmul3, (lv150, lv61), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv62: R.Tensor((320, 320), dtype="float32") = model_params[445]
            lv156 = R.call_tir(cls.matmul3, (lv150, lv62), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv35_2 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv152,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv36_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6_transpose6, (lv154,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv37_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv156,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv38_1 = R.call_tir(cls.fused_matmul4_multiply3, (lv35_2, lv36_1), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv169 = R.call_tir(cls.softmax, (lv38_1,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv170 = R.call_tir(cls.matmul5, (lv169, lv37_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv39_1 = R.call_tir(cls.fused_reshape7_transpose7_reshape8, (lv170,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv63: R.Tensor((320, 320), dtype="float32") = model_params[446]
            lv64: R.Tensor((320,), dtype="float32") = model_params[23]
            lv40_1 = R.call_tir(cls.fused_matmul3_add5_add6, (lv39_1, lv63, lv64, lv34_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv65: R.Tensor((320,), dtype="float32") = model_params[31]
            lv66: R.Tensor((320,), dtype="float32") = model_params[30]
            lv178 = R.call_tir(cls.layer_norm, (lv40_1, lv65, lv66), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv67: R.Tensor((320, 320), dtype="float32") = model_params[447]
            lv180 = R.call_tir(cls.matmul3, (lv178, lv67), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv68: R.Tensor((768, 320), dtype="float32") = model_params[448]
            lv182 = R.call_tir(cls.matmul6, (inp_2, lv68), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv69_1: R.Tensor((768, 320), dtype="float32") = model_params[449]
            lv184 = R.call_tir(cls.matmul6, (inp_2, lv69_1), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv41_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv180,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv42_1 = R.call_tir(cls.fused_reshape9_transpose9_reshape10_transpose10, (lv182,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv43_1 = R.call_tir(cls.fused_reshape9_transpose9_reshape10, (lv184,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv44_2 = R.call_tir(cls.fused_matmul7_multiply4, (lv41_1, lv42_1), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv197 = R.call_tir(cls.softmax1, (lv44_2,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv198 = R.call_tir(cls.matmul8, (lv197, lv43_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv45_1 = R.call_tir(cls.fused_reshape7_transpose7_reshape8, (lv198,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv70_1: R.Tensor((320, 320), dtype="float32") = model_params[450]
            lv71: R.Tensor((320,), dtype="float32") = model_params[24]
            lv46_1 = R.call_tir(cls.fused_matmul3_add5_add6, (lv45_1, lv70_1, lv71, lv40_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv72: R.Tensor((320,), dtype="float32") = model_params[33]
            lv73: R.Tensor((320,), dtype="float32") = model_params[32]
            lv206 = R.call_tir(cls.layer_norm, (lv46_1, lv72, lv73), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv74: R.Tensor((320, 1280), dtype="float32") = model_params[452]
            lv75: R.Tensor((1280,), dtype="float32") = model_params[26]
            lv47_1 = R.call_tir(cls.fused_matmul9_add7_gelu, (lv206, lv74, lv75), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv76: R.Tensor((320, 1280), dtype="float32") = model_params[451]
            lv77: R.Tensor((1280,), dtype="float32") = model_params[25]
            lv48_1 = R.call_tir(cls.fused_matmul9_add7_multiply5, (lv206, lv76, lv77, lv47_1), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv78_1: R.Tensor((1280, 320), dtype="float32") = model_params[453]
            lv79: R.Tensor((320,), dtype="float32") = model_params[27]
            lv49_1 = R.call_tir(cls.fused_matmul10_add5_add6, (lv48_1, lv78_1, lv79, lv46_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv50_2 = R.call_tir(cls.fused_reshape11_transpose11, (lv49_1,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv80_1: R.Tensor((320, 320, 1, 1), dtype="float32") = model_params[22]
            lv81: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[454]
            lv51_1 = R.call_tir(cls.fused_conv2d2_add1_add4, (lv50_2, lv80_1, lv81, lv32_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv82_1: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[34]
            lv83: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[455]
            lv52_2 = R.call_tir(cls.fused_conv2d3_add8, (lv51_1, lv82_1, lv83), out_sinfo=R.Tensor((2, 320, 32, 32), dtype="float32"))
            lv84_1: R.Tensor((320,), dtype="float32") = model_params[84]
            lv85: R.Tensor((320,), dtype="float32") = model_params[83]
            lv53_1 = R.call_tir(cls.fused_group_norm2_silu2, (lv52_2, lv84_1, lv85), out_sinfo=R.Tensor((2, 320, 32, 32), dtype="float32"))
            lv233 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv86: R.Tensor((1280, 640), dtype="float32") = model_params[457]
            lv87: R.Tensor((640,), dtype="float32") = model_params[87]
            lv54_2 = R.call_tir(cls.fused_matmul11_add10_strided_slice4, (lv233, lv86, lv87), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv238 = R.call_tir(cls.reshape13, (lv54_2,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv88: R.Tensor((640, 320, 3, 3), dtype="float32") = model_params[80]
            lv89: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[456]
            lv55_1 = R.call_tir(cls.fused_conv2d4_add9_add11, (lv53_1, lv88, lv89, lv238), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv90: R.Tensor((640,), dtype="float32") = model_params[86]
            lv91: R.Tensor((640,), dtype="float32") = model_params[85]
            lv56_2 = R.call_tir(cls.fused_group_norm3_silu3, (lv55_1, lv90, lv91), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv92: R.Tensor((640, 320, 1, 1), dtype="float32") = model_params[82]
            lv93: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[459]
            lv57_1 = R.call_tir(cls.fused_conv2d6_add9, (lv52_2, lv92, lv93), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv94: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[81]
            lv95: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[458]
            lv58_1 = R.call_tir(cls.fused_conv2d5_add9_add12_divide2, (lv56_2, lv94, lv95, lv57_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv96: R.Tensor((640,), dtype="float32") = model_params[50]
            lv97_1: R.Tensor((640,), dtype="float32") = model_params[49]
            lv250 = R.call_tir(cls.group_norm4, (lv58_1, lv96, lv97_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv98_1: R.Tensor((640, 640, 1, 1), dtype="float32") = model_params[51]
            lv99: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[460]
            lv59_1 = R.call_tir(cls.fused_conv2d7_add9, (lv250, lv98_1, lv99), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv60_1 = R.call_tir(cls.fused_transpose13_reshape14, (lv59_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv100: R.Tensor((640,), dtype="float32") = model_params[59]
            lv101: R.Tensor((640,), dtype="float32") = model_params[58]
            lv256 = R.call_tir(cls.layer_norm1, (lv60_1, lv100, lv101), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv102: R.Tensor((640, 640), dtype="float32") = model_params[461]
            lv258 = R.call_tir(cls.matmul12, (lv256, lv102), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv103: R.Tensor((640, 640), dtype="float32") = model_params[462]
            lv260 = R.call_tir(cls.matmul12, (lv256, lv103), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv104: R.Tensor((640, 640), dtype="float32") = model_params[463]
            lv262 = R.call_tir(cls.matmul12, (lv256, lv104), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv61_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv258,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv62_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16_transpose16, (lv260,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv63_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv262,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv64_1 = R.call_tir(cls.fused_matmul13_multiply6, (lv61_1, lv62_1), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv275 = R.call_tir(cls.softmax2, (lv64_1,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv276 = R.call_tir(cls.matmul14, (lv275, lv63_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv65_1 = R.call_tir(cls.fused_reshape17_transpose17_reshape18, (lv276,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv105: R.Tensor((640, 640), dtype="float32") = model_params[464]
            lv106_1: R.Tensor((640,), dtype="float32") = model_params[53]
            lv66_1 = R.call_tir(cls.fused_matmul12_add13_add14, (lv65_1, lv105, lv106_1, lv60_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv107: R.Tensor((640,), dtype="float32") = model_params[61]
            lv108: R.Tensor((640,), dtype="float32") = model_params[60]
            lv284 = R.call_tir(cls.layer_norm1, (lv66_1, lv107, lv108), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv109: R.Tensor((640, 640), dtype="float32") = model_params[465]
            lv286 = R.call_tir(cls.matmul12, (lv284, lv109), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv110: R.Tensor((768, 640), dtype="float32") = model_params[466]
            lv288 = R.call_tir(cls.matmul15, (inp_2, lv110), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv111: R.Tensor((768, 640), dtype="float32") = model_params[467]
            lv290 = R.call_tir(cls.matmul15, (inp_2, lv111), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv67_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv286,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv68_1 = R.call_tir(cls.fused_reshape19_transpose19_reshape20_transpose20, (lv288,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv69_2 = R.call_tir(cls.fused_reshape19_transpose19_reshape20, (lv290,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv70_2 = R.call_tir(cls.fused_matmul16_multiply7, (lv67_1, lv68_1), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv303 = R.call_tir(cls.softmax3, (lv70_2,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv304 = R.call_tir(cls.matmul17, (lv303, lv69_2), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv71_1 = R.call_tir(cls.fused_reshape17_transpose17_reshape18, (lv304,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv112: R.Tensor((640, 640), dtype="float32") = model_params[468]
            lv113: R.Tensor((640,), dtype="float32") = model_params[54]
            lv72_1 = R.call_tir(cls.fused_matmul12_add13_add14, (lv71_1, lv112, lv113, lv66_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv114: R.Tensor((640,), dtype="float32") = model_params[63]
            lv115: R.Tensor((640,), dtype="float32") = model_params[62]
            lv312 = R.call_tir(cls.layer_norm1, (lv72_1, lv114, lv115), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv116: R.Tensor((640, 2560), dtype="float32") = model_params[470]
            lv117: R.Tensor((2560,), dtype="float32") = model_params[56]
            lv73_1 = R.call_tir(cls.fused_matmul18_add15_gelu1, (lv312, lv116, lv117), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv118: R.Tensor((640, 2560), dtype="float32") = model_params[469]
            lv119: R.Tensor((2560,), dtype="float32") = model_params[55]
            lv74_1 = R.call_tir(cls.fused_matmul18_add15_multiply8, (lv312, lv118, lv119, lv73_1), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv120: R.Tensor((2560, 640), dtype="float32") = model_params[471]
            lv121: R.Tensor((640,), dtype="float32") = model_params[57]
            lv75_1 = R.call_tir(cls.fused_matmul19_add13_add14, (lv74_1, lv120, lv121, lv72_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv76_1 = R.call_tir(cls.fused_reshape21_transpose23, (lv75_1,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv122: R.Tensor((640, 640, 1, 1), dtype="float32") = model_params[52]
            lv123: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[472]
            lv77_1 = R.call_tir(cls.fused_conv2d7_add9_add12, (lv76_1, lv122, lv123, lv58_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv124: R.Tensor((640,), dtype="float32") = model_params[91]
            lv125: R.Tensor((640,), dtype="float32") = model_params[90]
            lv78_2 = R.call_tir(cls.fused_group_norm3_silu3, (lv77_1, lv124, lv125), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv336 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv126: R.Tensor((1280, 640), dtype="float32") = model_params[474]
            lv127: R.Tensor((640,), dtype="float32") = model_params[94]
            lv79_1 = R.call_tir(cls.fused_matmul11_add10_strided_slice4, (lv336, lv126, lv127), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv341 = R.call_tir(cls.reshape13, (lv79_1,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv128: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[88]
            lv129: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[473]
            lv80_2 = R.call_tir(cls.fused_conv2d5_add9_add11, (lv78_2, lv128, lv129, lv341), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv130_1: R.Tensor((640,), dtype="float32") = model_params[93]
            lv131: R.Tensor((640,), dtype="float32") = model_params[92]
            lv81_1 = R.call_tir(cls.fused_group_norm3_silu3, (lv80_2, lv130_1, lv131), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv132: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[89]
            lv133: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[475]
            lv82_2 = R.call_tir(cls.fused_conv2d5_add9_add12_divide2, (lv81_1, lv132, lv133, lv77_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv134: R.Tensor((640,), dtype="float32") = model_params[65]
            lv135_1: R.Tensor((640,), dtype="float32") = model_params[64]
            lv350 = R.call_tir(cls.group_norm4, (lv82_2, lv134, lv135_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv136: R.Tensor((640, 640, 1, 1), dtype="float32") = model_params[66]
            lv137: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[476]
            lv83_1 = R.call_tir(cls.fused_conv2d7_add9, (lv350, lv136, lv137), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv84_2 = R.call_tir(cls.fused_transpose13_reshape14, (lv83_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv138: R.Tensor((640,), dtype="float32") = model_params[74]
            lv139: R.Tensor((640,), dtype="float32") = model_params[73]
            lv356 = R.call_tir(cls.layer_norm1, (lv84_2, lv138, lv139), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv140: R.Tensor((640, 640), dtype="float32") = model_params[477]
            lv358 = R.call_tir(cls.matmul12, (lv356, lv140), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv141: R.Tensor((640, 640), dtype="float32") = model_params[478]
            lv360 = R.call_tir(cls.matmul12, (lv356, lv141), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv142: R.Tensor((640, 640), dtype="float32") = model_params[479]
            lv362 = R.call_tir(cls.matmul12, (lv356, lv142), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv85_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv358,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv86_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16_transpose16, (lv360,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv87_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv362,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv88_1 = R.call_tir(cls.fused_matmul13_multiply6, (lv85_1, lv86_1), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv375 = R.call_tir(cls.softmax2, (lv88_1,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv376 = R.call_tir(cls.matmul14, (lv375, lv87_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv89_1 = R.call_tir(cls.fused_reshape17_transpose17_reshape18, (lv376,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv143: R.Tensor((640, 640), dtype="float32") = model_params[480]
            lv144_1: R.Tensor((640,), dtype="float32") = model_params[68]
            lv90_1 = R.call_tir(cls.fused_matmul12_add13_add14, (lv89_1, lv143, lv144_1, lv84_2), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv145: R.Tensor((640,), dtype="float32") = model_params[76]
            lv146: R.Tensor((640,), dtype="float32") = model_params[75]
            lv384 = R.call_tir(cls.layer_norm1, (lv90_1, lv145, lv146), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv147: R.Tensor((640, 640), dtype="float32") = model_params[481]
            lv386 = R.call_tir(cls.matmul12, (lv384, lv147), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv148: R.Tensor((768, 640), dtype="float32") = model_params[482]
            lv388 = R.call_tir(cls.matmul15, (inp_2, lv148), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv149: R.Tensor((768, 640), dtype="float32") = model_params[483]
            lv390 = R.call_tir(cls.matmul15, (inp_2, lv149), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv91_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv386,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv92_1 = R.call_tir(cls.fused_reshape19_transpose19_reshape20_transpose20, (lv388,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv93_1 = R.call_tir(cls.fused_reshape19_transpose19_reshape20, (lv390,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv94_1 = R.call_tir(cls.fused_matmul16_multiply7, (lv91_1, lv92_1), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv403 = R.call_tir(cls.softmax3, (lv94_1,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv404 = R.call_tir(cls.matmul17, (lv403, lv93_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv95_1 = R.call_tir(cls.fused_reshape17_transpose17_reshape18, (lv404,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv150_1: R.Tensor((640, 640), dtype="float32") = model_params[484]
            lv151: R.Tensor((640,), dtype="float32") = model_params[69]
            lv96_1 = R.call_tir(cls.fused_matmul12_add13_add14, (lv95_1, lv150_1, lv151, lv90_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv152_1: R.Tensor((640,), dtype="float32") = model_params[78]
            lv153: R.Tensor((640,), dtype="float32") = model_params[77]
            lv412 = R.call_tir(cls.layer_norm1, (lv96_1, lv152_1, lv153), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv154_1: R.Tensor((640, 2560), dtype="float32") = model_params[486]
            lv155: R.Tensor((2560,), dtype="float32") = model_params[71]
            lv97_2 = R.call_tir(cls.fused_matmul18_add15_gelu1, (lv412, lv154_1, lv155), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv156_1: R.Tensor((640, 2560), dtype="float32") = model_params[485]
            lv157: R.Tensor((2560,), dtype="float32") = model_params[70]
            lv98_2 = R.call_tir(cls.fused_matmul18_add15_multiply8, (lv412, lv156_1, lv157, lv97_2), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv158: R.Tensor((2560, 640), dtype="float32") = model_params[487]
            lv159: R.Tensor((640,), dtype="float32") = model_params[72]
            lv99_1 = R.call_tir(cls.fused_matmul19_add13_add14, (lv98_2, lv158, lv159, lv96_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv100_1 = R.call_tir(cls.fused_reshape21_transpose23, (lv99_1,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv160: R.Tensor((640, 640, 1, 1), dtype="float32") = model_params[67]
            lv161: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[488]
            lv101_1 = R.call_tir(cls.fused_conv2d7_add9_add12, (lv100_1, lv160, lv161, lv82_2), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv162: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[79]
            lv163: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[489]
            lv102_1 = R.call_tir(cls.fused_conv2d8_add16, (lv101_1, lv162, lv163), out_sinfo=R.Tensor((2, 640, 16, 16), dtype="float32"))
            lv164: R.Tensor((640,), dtype="float32") = model_params[130]
            lv165: R.Tensor((640,), dtype="float32") = model_params[129]
            lv103_1 = R.call_tir(cls.fused_group_norm5_silu4, (lv102_1, lv164, lv165), out_sinfo=R.Tensor((2, 640, 16, 16), dtype="float32"))
            lv439 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv166: R.Tensor((1280, 1280), dtype="float32") = model_params[491]
            lv167: R.Tensor((1280,), dtype="float32") = model_params[133]
            lv104_1 = R.call_tir(cls.fused_matmul1_add_strided_slice5, (lv439, lv166, lv167), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv444 = R.call_tir(cls.reshape23, (lv104_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv168: R.Tensor((1280, 640, 3, 3), dtype="float32") = model_params[126]
            lv169_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[490]
            lv105_1 = R.call_tir(cls.fused_conv2d9_add17_add18, (lv103_1, lv168, lv169_1, lv444), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv170_1: R.Tensor((1280,), dtype="float32") = model_params[132]
            lv171: R.Tensor((1280,), dtype="float32") = model_params[131]
            lv106_2 = R.call_tir(cls.fused_group_norm6_silu5, (lv105_1, lv170_1, lv171), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv172: R.Tensor((1280, 640, 1, 1), dtype="float32") = model_params[128]
            lv173: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[493]
            lv107_1 = R.call_tir(cls.fused_conv2d11_add17, (lv102_1, lv172, lv173), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv174: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[127]
            lv175: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[492]
            lv108_1 = R.call_tir(cls.fused_conv2d10_add17_add19_divide3, (lv106_2, lv174, lv175, lv107_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv176: R.Tensor((1280,), dtype="float32") = model_params[96]
            lv177: R.Tensor((1280,), dtype="float32") = model_params[95]
            lv456 = R.call_tir(cls.group_norm7, (lv108_1, lv176, lv177), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv178_1: R.Tensor((1280, 1280, 1, 1), dtype="float32") = model_params[97]
            lv179: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[494]
            lv109_1 = R.call_tir(cls.fused_conv2d12_add17, (lv456, lv178_1, lv179), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv110_1 = R.call_tir(cls.fused_transpose24_reshape24, (lv109_1,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv180_1: R.Tensor((1280,), dtype="float32") = model_params[105]
            lv181: R.Tensor((1280,), dtype="float32") = model_params[104]
            lv462 = R.call_tir(cls.layer_norm2, (lv110_1, lv180_1, lv181), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv182_1: R.Tensor((1280, 1280), dtype="float32") = model_params[495]
            lv464 = R.call_tir(cls.matmul20, (lv462, lv182_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv183: R.Tensor((1280, 1280), dtype="float32") = model_params[496]
            lv466 = R.call_tir(cls.matmul20, (lv462, lv183), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv184_1: R.Tensor((1280, 1280), dtype="float32") = model_params[497]
            lv468 = R.call_tir(cls.matmul20, (lv462, lv184_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv111_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv464,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv112_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26_transpose26, (lv466,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv113_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv468,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv114_1 = R.call_tir(cls.fused_matmul21_multiply9, (lv111_1, lv112_1), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv481 = R.call_tir(cls.softmax4, (lv114_1,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv482 = R.call_tir(cls.matmul22, (lv481, lv113_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv115_1 = R.call_tir(cls.fused_reshape27_transpose27_reshape28, (lv482,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv185: R.Tensor((1280, 1280), dtype="float32") = model_params[498]
            lv186: R.Tensor((1280,), dtype="float32") = model_params[99]
            lv116_1 = R.call_tir(cls.fused_matmul20_add20_add21, (lv115_1, lv185, lv186, lv110_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv187: R.Tensor((1280,), dtype="float32") = model_params[107]
            lv188: R.Tensor((1280,), dtype="float32") = model_params[106]
            lv490 = R.call_tir(cls.layer_norm2, (lv116_1, lv187, lv188), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv189: R.Tensor((1280, 1280), dtype="float32") = model_params[499]
            lv492 = R.call_tir(cls.matmul20, (lv490, lv189), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv190: R.Tensor((768, 1280), dtype="float32") = model_params[500]
            lv494 = R.call_tir(cls.matmul23, (inp_2, lv190), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv191: R.Tensor((768, 1280), dtype="float32") = model_params[501]
            lv496 = R.call_tir(cls.matmul23, (inp_2, lv191), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv117_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv492,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv118_1 = R.call_tir(cls.fused_reshape29_transpose29_reshape30_transpose30, (lv494,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv119_1 = R.call_tir(cls.fused_reshape29_transpose29_reshape30, (lv496,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv120_1 = R.call_tir(cls.fused_matmul24_multiply10, (lv117_1, lv118_1), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv509 = R.call_tir(cls.softmax5, (lv120_1,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv510 = R.call_tir(cls.matmul25, (lv509, lv119_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv121_1 = R.call_tir(cls.fused_reshape27_transpose27_reshape28, (lv510,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv192: R.Tensor((1280, 1280), dtype="float32") = model_params[502]
            lv193: R.Tensor((1280,), dtype="float32") = model_params[100]
            lv122_1 = R.call_tir(cls.fused_matmul20_add20_add21, (lv121_1, lv192, lv193, lv116_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv194: R.Tensor((1280,), dtype="float32") = model_params[109]
            lv195: R.Tensor((1280,), dtype="float32") = model_params[108]
            lv518 = R.call_tir(cls.layer_norm2, (lv122_1, lv194, lv195), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv196: R.Tensor((1280, 5120), dtype="float32") = model_params[504]
            lv197_1: R.Tensor((5120,), dtype="float32") = model_params[102]
            lv123_1 = R.call_tir(cls.fused_matmul26_add22_gelu2, (lv518, lv196, lv197_1), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv198_1: R.Tensor((1280, 5120), dtype="float32") = model_params[503]
            lv199: R.Tensor((5120,), dtype="float32") = model_params[101]
            lv124_1 = R.call_tir(cls.fused_matmul26_add22_multiply11, (lv518, lv198_1, lv199, lv123_1), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv200: R.Tensor((5120, 1280), dtype="float32") = model_params[505]
            lv201: R.Tensor((1280,), dtype="float32") = model_params[103]
            lv125_1 = R.call_tir(cls.fused_matmul27_add20_add21, (lv124_1, lv200, lv201, lv122_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv126_1 = R.call_tir(cls.fused_reshape31_transpose33, (lv125_1,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv202: R.Tensor((1280, 1280, 1, 1), dtype="float32") = model_params[98]
            lv203: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[506]
            lv127_1 = R.call_tir(cls.fused_conv2d12_add17_add19, (lv126_1, lv202, lv203, lv108_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv204: R.Tensor((1280,), dtype="float32") = model_params[137]
            lv205: R.Tensor((1280,), dtype="float32") = model_params[136]
            lv128_1 = R.call_tir(cls.fused_group_norm6_silu5, (lv127_1, lv204, lv205), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv542 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv206_1: R.Tensor((1280, 1280), dtype="float32") = model_params[508]
            lv207: R.Tensor((1280,), dtype="float32") = model_params[140]
            lv129_1 = R.call_tir(cls.fused_matmul1_add_strided_slice5, (lv542, lv206_1, lv207), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv547 = R.call_tir(cls.reshape23, (lv129_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv208: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[134]
            lv209: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[507]
            lv130_2 = R.call_tir(cls.fused_conv2d10_add17_add18, (lv128_1, lv208, lv209, lv547), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv210: R.Tensor((1280,), dtype="float32") = model_params[139]
            lv211: R.Tensor((1280,), dtype="float32") = model_params[138]
            lv131_1 = R.call_tir(cls.fused_group_norm6_silu5, (lv130_2, lv210, lv211), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv212: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[135]
            lv213: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[509]
            lv132_1 = R.call_tir(cls.fused_conv2d10_add17_add19_divide3, (lv131_1, lv212, lv213, lv127_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv214: R.Tensor((1280,), dtype="float32") = model_params[111]
            lv215: R.Tensor((1280,), dtype="float32") = model_params[110]
            lv556 = R.call_tir(cls.group_norm7, (lv132_1, lv214, lv215), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv216: R.Tensor((1280, 1280, 1, 1), dtype="float32") = model_params[112]
            lv217: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[510]
            lv133_1 = R.call_tir(cls.fused_conv2d12_add17, (lv556, lv216, lv217), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv134_1 = R.call_tir(cls.fused_transpose24_reshape24, (lv133_1,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv218: R.Tensor((1280,), dtype="float32") = model_params[120]
            lv219: R.Tensor((1280,), dtype="float32") = model_params[119]
            lv562 = R.call_tir(cls.layer_norm2, (lv134_1, lv218, lv219), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv220: R.Tensor((1280, 1280), dtype="float32") = model_params[511]
            lv564 = R.call_tir(cls.matmul20, (lv562, lv220), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv221: R.Tensor((1280, 1280), dtype="float32") = model_params[512]
            lv566 = R.call_tir(cls.matmul20, (lv562, lv221), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv222: R.Tensor((1280, 1280), dtype="float32") = model_params[513]
            lv568 = R.call_tir(cls.matmul20, (lv562, lv222), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv135_2 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv564,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv136_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26_transpose26, (lv566,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv137_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv568,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv138_1 = R.call_tir(cls.fused_matmul21_multiply9, (lv135_2, lv136_1), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv581 = R.call_tir(cls.softmax4, (lv138_1,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv582 = R.call_tir(cls.matmul22, (lv581, lv137_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv139_1 = R.call_tir(cls.fused_reshape27_transpose27_reshape28, (lv582,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv223: R.Tensor((1280, 1280), dtype="float32") = model_params[514]
            lv224: R.Tensor((1280,), dtype="float32") = model_params[114]
            lv140_1 = R.call_tir(cls.fused_matmul20_add20_add21, (lv139_1, lv223, lv224, lv134_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv225: R.Tensor((1280,), dtype="float32") = model_params[122]
            lv226: R.Tensor((1280,), dtype="float32") = model_params[121]
            lv590 = R.call_tir(cls.layer_norm2, (lv140_1, lv225, lv226), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv227: R.Tensor((1280, 1280), dtype="float32") = model_params[515]
            lv592 = R.call_tir(cls.matmul20, (lv590, lv227), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv228: R.Tensor((768, 1280), dtype="float32") = model_params[516]
            lv594 = R.call_tir(cls.matmul23, (inp_2, lv228), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv229: R.Tensor((768, 1280), dtype="float32") = model_params[517]
            lv596 = R.call_tir(cls.matmul23, (inp_2, lv229), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv141_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv592,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv142_1 = R.call_tir(cls.fused_reshape29_transpose29_reshape30_transpose30, (lv594,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv143_1 = R.call_tir(cls.fused_reshape29_transpose29_reshape30, (lv596,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv144_2 = R.call_tir(cls.fused_matmul24_multiply10, (lv141_1, lv142_1), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv609 = R.call_tir(cls.softmax5, (lv144_2,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv610 = R.call_tir(cls.matmul25, (lv609, lv143_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv145_1 = R.call_tir(cls.fused_reshape27_transpose27_reshape28, (lv610,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv230: R.Tensor((1280, 1280), dtype="float32") = model_params[518]
            lv231: R.Tensor((1280,), dtype="float32") = model_params[115]
            lv146_1 = R.call_tir(cls.fused_matmul20_add20_add21, (lv145_1, lv230, lv231, lv140_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv232: R.Tensor((1280,), dtype="float32") = model_params[124]
            lv233_1: R.Tensor((1280,), dtype="float32") = model_params[123]
            lv618 = R.call_tir(cls.layer_norm2, (lv146_1, lv232, lv233_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv234: R.Tensor((1280, 5120), dtype="float32") = model_params[520]
            lv235: R.Tensor((5120,), dtype="float32") = model_params[117]
            lv147_1 = R.call_tir(cls.fused_matmul26_add22_gelu2, (lv618, lv234, lv235), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv236: R.Tensor((1280, 5120), dtype="float32") = model_params[519]
            lv237: R.Tensor((5120,), dtype="float32") = model_params[116]
            lv148_1 = R.call_tir(cls.fused_matmul26_add22_multiply11, (lv618, lv236, lv237, lv147_1), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv238_1: R.Tensor((5120, 1280), dtype="float32") = model_params[521]
            lv239: R.Tensor((1280,), dtype="float32") = model_params[118]
            lv149_1 = R.call_tir(cls.fused_matmul27_add20_add21, (lv148_1, lv238_1, lv239, lv146_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv150_2 = R.call_tir(cls.fused_reshape31_transpose33, (lv149_1,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv240: R.Tensor((1280, 1280, 1, 1), dtype="float32") = model_params[113]
            lv241: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[522]
            lv151_1 = R.call_tir(cls.fused_conv2d12_add17_add19, (lv150_2, lv240, lv241, lv132_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv242: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[125]
            lv243: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[523]
            lv152_2 = R.call_tir(cls.fused_conv2d13_add23, (lv151_1, lv242, lv243), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv244: R.Tensor((1280,), dtype="float32") = model_params[144]
            lv245: R.Tensor((1280,), dtype="float32") = model_params[143]
            lv153_1 = R.call_tir(cls.fused_group_norm8_silu6, (lv152_2, lv244, lv245), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv645 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv246: R.Tensor((1280, 1280), dtype="float32") = model_params[525]
            lv247: R.Tensor((1280,), dtype="float32") = model_params[147]
            lv154_2 = R.call_tir(cls.fused_matmul1_add_strided_slice5, (lv645, lv246, lv247), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv650 = R.call_tir(cls.reshape23, (lv154_2,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv248: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[141]
            lv249: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[524]
            lv155_1 = R.call_tir(cls.fused_conv2d14_add23_add24, (lv153_1, lv248, lv249, lv650), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv250_1: R.Tensor((1280,), dtype="float32") = model_params[146]
            lv251: R.Tensor((1280,), dtype="float32") = model_params[145]
            lv156_2 = R.call_tir(cls.fused_group_norm8_silu6, (lv155_1, lv250_1, lv251), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv252: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[142]
            lv253: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[526]
            lv157_1 = R.call_tir(cls.fused_conv2d14_add23_add25_divide4, (lv156_2, lv252, lv253, lv152_2), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv254: R.Tensor((1280,), dtype="float32") = model_params[151]
            lv255: R.Tensor((1280,), dtype="float32") = model_params[150]
            lv158_1 = R.call_tir(cls.fused_group_norm8_silu6, (lv157_1, lv254, lv255), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv664 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv256_1: R.Tensor((1280, 1280), dtype="float32") = model_params[528]
            lv257: R.Tensor((1280,), dtype="float32") = model_params[154]
            lv159_1 = R.call_tir(cls.fused_matmul1_add_strided_slice5, (lv664, lv256_1, lv257), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv669 = R.call_tir(cls.reshape23, (lv159_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv258_1: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[148]
            lv259: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[527]
            lv160_1 = R.call_tir(cls.fused_conv2d14_add23_add24, (lv158_1, lv258_1, lv259, lv669), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv260_1: R.Tensor((1280,), dtype="float32") = model_params[153]
            lv261: R.Tensor((1280,), dtype="float32") = model_params[152]
            lv161_1 = R.call_tir(cls.fused_group_norm8_silu6, (lv160_1, lv260_1, lv261), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv262_1: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[149]
            lv263: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[529]
            lv162_1 = R.call_tir(cls.fused_conv2d14_add23_add25_divide4, (lv161_1, lv262_1, lv263, lv157_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv264: R.Tensor((1280,), dtype="float32") = model_params[173]
            lv265: R.Tensor((1280,), dtype="float32") = model_params[172]
            lv163_1 = R.call_tir(cls.fused_group_norm8_silu6, (lv162_1, lv264, lv265), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv683 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv266: R.Tensor((1280, 1280), dtype="float32") = model_params[531]
            lv267: R.Tensor((1280,), dtype="float32") = model_params[176]
            lv164_1 = R.call_tir(cls.fused_matmul1_add_strided_slice5, (lv683, lv266, lv267), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv688 = R.call_tir(cls.reshape23, (lv164_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv268: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[170]
            lv269: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[530]
            lv165_1 = R.call_tir(cls.fused_conv2d14_add23_add24, (lv163_1, lv268, lv269, lv688), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv270: R.Tensor((1280,), dtype="float32") = model_params[175]
            lv271: R.Tensor((1280,), dtype="float32") = model_params[174]
            lv166_1 = R.call_tir(cls.fused_group_norm8_silu6, (lv165_1, lv270, lv271), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv272: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[171]
            lv273: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[532]
            lv167_1 = R.call_tir(cls.fused_conv2d14_add23_add25_divide4, (lv166_1, lv272, lv273, lv162_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv274: R.Tensor((1280,), dtype="float32") = model_params[156]
            lv275_1: R.Tensor((1280,), dtype="float32") = model_params[155]
            lv697 = R.call_tir(cls.group_norm9, (lv167_1, lv274, lv275_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv276_1: R.Tensor((1280, 1280, 1, 1), dtype="float32") = model_params[157]
            lv277: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[533]
            lv168_1 = R.call_tir(cls.fused_conv2d15_add23, (lv697, lv276_1, lv277), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv169_2 = R.call_tir(cls.fused_transpose34_reshape32, (lv168_1,), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv278: R.Tensor((1280,), dtype="float32") = model_params[165]
            lv279: R.Tensor((1280,), dtype="float32") = model_params[164]
            lv703 = R.call_tir(cls.layer_norm3, (lv169_2, lv278, lv279), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv280: R.Tensor((1280, 1280), dtype="float32") = model_params[534]
            lv705 = R.call_tir(cls.matmul28, (lv703, lv280), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv281: R.Tensor((1280, 1280), dtype="float32") = model_params[535]
            lv707 = R.call_tir(cls.matmul28, (lv703, lv281), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv282: R.Tensor((1280, 1280), dtype="float32") = model_params[536]
            lv709 = R.call_tir(cls.matmul28, (lv703, lv282), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv170_2 = R.call_tir(cls.fused_reshape33_transpose35_reshape34, (lv705,), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv171_1 = R.call_tir(cls.fused_reshape33_transpose35_reshape34_transpose36, (lv707,), out_sinfo=R.Tensor((16, 160, 64), dtype="float32"))
            lv172_1 = R.call_tir(cls.fused_reshape33_transpose35_reshape34, (lv709,), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv173_1 = R.call_tir(cls.fused_matmul29_multiply12, (lv170_2, lv171_1), out_sinfo=R.Tensor((16, 64, 64), dtype="float32"))
            lv722 = R.call_tir(cls.softmax6, (lv173_1,), out_sinfo=R.Tensor((16, 64, 64), dtype="float32"))
            lv723 = R.call_tir(cls.matmul30, (lv722, lv172_1), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv174_1 = R.call_tir(cls.fused_reshape35_transpose37_reshape36, (lv723,), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv283: R.Tensor((1280, 1280), dtype="float32") = model_params[537]
            lv284_1: R.Tensor((1280,), dtype="float32") = model_params[159]
            lv175_1 = R.call_tir(cls.fused_matmul28_add26_add27, (lv174_1, lv283, lv284_1, lv169_2), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv285: R.Tensor((1280,), dtype="float32") = model_params[167]
            lv286_1: R.Tensor((1280,), dtype="float32") = model_params[166]
            lv731 = R.call_tir(cls.layer_norm3, (lv175_1, lv285, lv286_1), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv287: R.Tensor((1280, 1280), dtype="float32") = model_params[538]
            lv733 = R.call_tir(cls.matmul28, (lv731, lv287), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv288_1: R.Tensor((768, 1280), dtype="float32") = model_params[539]
            lv735 = R.call_tir(cls.matmul23, (inp_2, lv288_1), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv289: R.Tensor((768, 1280), dtype="float32") = model_params[540]
            lv737 = R.call_tir(cls.matmul23, (inp_2, lv289), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv176_1 = R.call_tir(cls.fused_reshape33_transpose35_reshape34, (lv733,), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv177_1 = R.call_tir(cls.fused_reshape29_transpose29_reshape30_transpose30, (lv735,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv178_2 = R.call_tir(cls.fused_reshape29_transpose29_reshape30, (lv737,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv179_1 = R.call_tir(cls.fused_matmul31_multiply13, (lv176_1, lv177_1), out_sinfo=R.Tensor((16, 64, 77), dtype="float32"))
            lv750 = R.call_tir(cls.softmax7, (lv179_1,), out_sinfo=R.Tensor((16, 64, 77), dtype="float32"))
            lv751 = R.call_tir(cls.matmul32, (lv750, lv178_2), out_sinfo=R.Tensor((16, 64, 160), dtype="float32"))
            lv180_2 = R.call_tir(cls.fused_reshape35_transpose37_reshape36, (lv751,), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv290_1: R.Tensor((1280, 1280), dtype="float32") = model_params[541]
            lv291: R.Tensor((1280,), dtype="float32") = model_params[160]
            lv181_1 = R.call_tir(cls.fused_matmul28_add26_add27, (lv180_2, lv290_1, lv291, lv175_1), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv292: R.Tensor((1280,), dtype="float32") = model_params[169]
            lv293: R.Tensor((1280,), dtype="float32") = model_params[168]
            lv759 = R.call_tir(cls.layer_norm3, (lv181_1, lv292, lv293), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv294: R.Tensor((1280, 5120), dtype="float32") = model_params[543]
            lv295: R.Tensor((5120,), dtype="float32") = model_params[162]
            lv182_2 = R.call_tir(cls.fused_matmul33_add28_gelu3, (lv759, lv294, lv295), out_sinfo=R.Tensor((2, 64, 5120), dtype="float32"))
            lv296: R.Tensor((1280, 5120), dtype="float32") = model_params[542]
            lv297: R.Tensor((5120,), dtype="float32") = model_params[161]
            lv183_1 = R.call_tir(cls.fused_matmul33_add28_multiply14, (lv759, lv296, lv297, lv182_2), out_sinfo=R.Tensor((2, 64, 5120), dtype="float32"))
            lv298: R.Tensor((5120, 1280), dtype="float32") = model_params[544]
            lv299: R.Tensor((1280,), dtype="float32") = model_params[163]
            lv184_2 = R.call_tir(cls.fused_matmul34_add26_add27, (lv183_1, lv298, lv299, lv181_1), out_sinfo=R.Tensor((2, 64, 1280), dtype="float32"))
            lv185_1 = R.call_tir(cls.fused_reshape37_transpose38, (lv184_2,), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv300: R.Tensor((1280, 1280, 1, 1), dtype="float32") = model_params[158]
            lv301: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[545]
            lv186_1 = R.call_tir(cls.fused_conv2d15_add23_add25, (lv185_1, lv300, lv301, lv167_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv302: R.Tensor((1280,), dtype="float32") = model_params[180]
            lv303_1: R.Tensor((1280,), dtype="float32") = model_params[179]
            lv187_1 = R.call_tir(cls.fused_group_norm8_silu6, (lv186_1, lv302, lv303_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv783 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv304_1: R.Tensor((1280, 1280), dtype="float32") = model_params[547]
            lv305: R.Tensor((1280,), dtype="float32") = model_params[183]
            lv188_1 = R.call_tir(cls.fused_matmul1_add_strided_slice5, (lv783, lv304_1, lv305), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv788 = R.call_tir(cls.reshape23, (lv188_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv306: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[177]
            lv307: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[546]
            lv189_1 = R.call_tir(cls.fused_conv2d14_add23_add24, (lv187_1, lv306, lv307, lv788), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv308: R.Tensor((1280,), dtype="float32") = model_params[182]
            lv309: R.Tensor((1280,), dtype="float32") = model_params[181]
            lv190_1 = R.call_tir(cls.fused_group_norm8_silu6, (lv189_1, lv308, lv309), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv310: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[178]
            lv311: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[548]
            lv191_1 = R.call_tir(cls.fused_conv2d14_add23_add25_divide4, (lv190_1, lv310, lv311, lv186_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv797 = R.call_tir(cls.concatenate2, (lv191_1, lv162_1), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv312_1: R.Tensor((2560,), dtype="float32") = model_params[190]
            lv313: R.Tensor((2560,), dtype="float32") = model_params[189]
            lv192_1 = R.call_tir(cls.fused_group_norm10_silu7, (lv797, lv312_1, lv313), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv803 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv314: R.Tensor((1280, 1280), dtype="float32") = model_params[550]
            lv315: R.Tensor((1280,), dtype="float32") = model_params[193]
            lv193_1 = R.call_tir(cls.fused_matmul1_add_strided_slice5, (lv803, lv314, lv315), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv808 = R.call_tir(cls.reshape23, (lv193_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv316: R.Tensor((1280, 2560, 3, 3), dtype="float32") = model_params[186]
            lv317: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[549]
            lv194_1 = R.call_tir(cls.fused_conv2d16_add23_add24, (lv192_1, lv316, lv317, lv808), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv318: R.Tensor((1280,), dtype="float32") = model_params[192]
            lv319: R.Tensor((1280,), dtype="float32") = model_params[191]
            lv195_1 = R.call_tir(cls.fused_group_norm8_silu6, (lv194_1, lv318, lv319), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv320: R.Tensor((1280, 2560, 1, 1), dtype="float32") = model_params[188]
            lv321: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[552]
            lv196_1 = R.call_tir(cls.fused_conv2d17_add23, (lv797, lv320, lv321), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv322: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[187]
            lv323: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[551]
            lv197_2 = R.call_tir(cls.fused_conv2d14_add23_add25_divide4, (lv195_1, lv322, lv323, lv196_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv820 = R.call_tir(cls.concatenate2, (lv197_2, lv157_1), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv324: R.Tensor((2560,), dtype="float32") = model_params[198]
            lv325: R.Tensor((2560,), dtype="float32") = model_params[197]
            lv198_2 = R.call_tir(cls.fused_group_norm10_silu7, (lv820, lv324, lv325), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv826 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv326: R.Tensor((1280, 1280), dtype="float32") = model_params[554]
            lv327: R.Tensor((1280,), dtype="float32") = model_params[201]
            lv199_1 = R.call_tir(cls.fused_matmul1_add_strided_slice5, (lv826, lv326, lv327), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv831 = R.call_tir(cls.reshape23, (lv199_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv328: R.Tensor((1280, 2560, 3, 3), dtype="float32") = model_params[194]
            lv329: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[553]
            lv200_1 = R.call_tir(cls.fused_conv2d16_add23_add24, (lv198_2, lv328, lv329, lv831), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv330: R.Tensor((1280,), dtype="float32") = model_params[200]
            lv331: R.Tensor((1280,), dtype="float32") = model_params[199]
            lv201_1 = R.call_tir(cls.fused_group_norm8_silu6, (lv200_1, lv330, lv331), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv332: R.Tensor((1280, 2560, 1, 1), dtype="float32") = model_params[196]
            lv333: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[556]
            lv202_1 = R.call_tir(cls.fused_conv2d17_add23, (lv820, lv332, lv333), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv334: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[195]
            lv335: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[555]
            lv203_1 = R.call_tir(cls.fused_conv2d14_add23_add25_divide4, (lv201_1, lv334, lv335, lv202_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv843 = R.call_tir(cls.concatenate2, (lv203_1, lv152_2), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv336_1: R.Tensor((2560,), dtype="float32") = model_params[206]
            lv337: R.Tensor((2560,), dtype="float32") = model_params[205]
            lv204_1 = R.call_tir(cls.fused_group_norm10_silu7, (lv843, lv336_1, lv337), out_sinfo=R.Tensor((2, 2560, 8, 8), dtype="float32"))
            lv849 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv338: R.Tensor((1280, 1280), dtype="float32") = model_params[558]
            lv339: R.Tensor((1280,), dtype="float32") = model_params[209]
            lv205_1 = R.call_tir(cls.fused_matmul1_add_strided_slice5, (lv849, lv338, lv339), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv854 = R.call_tir(cls.reshape23, (lv205_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv340: R.Tensor((1280, 2560, 3, 3), dtype="float32") = model_params[202]
            lv341_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[557]
            lv206_2 = R.call_tir(cls.fused_conv2d16_add23_add24, (lv204_1, lv340, lv341_1, lv854), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv342: R.Tensor((1280,), dtype="float32") = model_params[208]
            lv343: R.Tensor((1280,), dtype="float32") = model_params[207]
            lv207_1 = R.call_tir(cls.fused_group_norm8_silu6, (lv206_2, lv342, lv343), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv344: R.Tensor((1280, 2560, 1, 1), dtype="float32") = model_params[204]
            lv345: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[560]
            lv208_1 = R.call_tir(cls.fused_conv2d17_add23, (lv843, lv344, lv345), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv346: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[203]
            lv347: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[559]
            lv209_1 = R.call_tir(cls.fused_conv2d14_add23_add25_divide4, (lv207_1, lv346, lv347, lv208_1), out_sinfo=R.Tensor((2, 1280, 8, 8), dtype="float32"))
            lv866 = R.call_tir(cls.resize2d, (lv209_1,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv348: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[210]
            lv349: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[561]
            lv210_1 = R.call_tir(cls.fused_conv2d10_add17, (lv866, lv348, lv349), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv870 = R.call_tir(cls.concatenate3, (lv210_1, lv151_1), out_sinfo=R.Tensor((2, 2560, 16, 16), dtype="float32"))
            lv350_1: R.Tensor((2560,), dtype="float32") = model_params[260]
            lv351: R.Tensor((2560,), dtype="float32") = model_params[259]
            lv211_1 = R.call_tir(cls.fused_group_norm11_silu8, (lv870, lv350_1, lv351), out_sinfo=R.Tensor((2, 2560, 16, 16), dtype="float32"))
            lv876 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv352: R.Tensor((1280, 1280), dtype="float32") = model_params[563]
            lv353: R.Tensor((1280,), dtype="float32") = model_params[263]
            lv212_1 = R.call_tir(cls.fused_matmul1_add_strided_slice5, (lv876, lv352, lv353), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv881 = R.call_tir(cls.reshape23, (lv212_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv354: R.Tensor((1280, 2560, 3, 3), dtype="float32") = model_params[256]
            lv355: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[562]
            lv213_1 = R.call_tir(cls.fused_conv2d18_add17_add18, (lv211_1, lv354, lv355, lv881), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv356_1: R.Tensor((1280,), dtype="float32") = model_params[262]
            lv357: R.Tensor((1280,), dtype="float32") = model_params[261]
            lv214_1 = R.call_tir(cls.fused_group_norm6_silu5, (lv213_1, lv356_1, lv357), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv358_1: R.Tensor((1280, 2560, 1, 1), dtype="float32") = model_params[258]
            lv359: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[565]
            lv215_1 = R.call_tir(cls.fused_conv2d19_add17, (lv870, lv358_1, lv359), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv360_1: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[257]
            lv361: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[564]
            lv216_1 = R.call_tir(cls.fused_conv2d10_add17_add19_divide3, (lv214_1, lv360_1, lv361, lv215_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv362_1: R.Tensor((1280,), dtype="float32") = model_params[212]
            lv363: R.Tensor((1280,), dtype="float32") = model_params[211]
            lv893 = R.call_tir(cls.group_norm7, (lv216_1, lv362_1, lv363), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv364: R.Tensor((1280, 1280, 1, 1), dtype="float32") = model_params[213]
            lv365: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[566]
            lv217_1 = R.call_tir(cls.fused_conv2d12_add17, (lv893, lv364, lv365), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv218_1 = R.call_tir(cls.fused_transpose24_reshape24, (lv217_1,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv366: R.Tensor((1280,), dtype="float32") = model_params[221]
            lv367: R.Tensor((1280,), dtype="float32") = model_params[220]
            lv899 = R.call_tir(cls.layer_norm2, (lv218_1, lv366, lv367), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv368: R.Tensor((1280, 1280), dtype="float32") = model_params[567]
            lv901 = R.call_tir(cls.matmul20, (lv899, lv368), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv369: R.Tensor((1280, 1280), dtype="float32") = model_params[568]
            lv903 = R.call_tir(cls.matmul20, (lv899, lv369), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv370: R.Tensor((1280, 1280), dtype="float32") = model_params[569]
            lv905 = R.call_tir(cls.matmul20, (lv899, lv370), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv219_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv901,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv220_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26_transpose26, (lv903,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv221_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv905,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv222_1 = R.call_tir(cls.fused_matmul21_multiply9, (lv219_1, lv220_1), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv918 = R.call_tir(cls.softmax4, (lv222_1,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv919 = R.call_tir(cls.matmul22, (lv918, lv221_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv223_1 = R.call_tir(cls.fused_reshape27_transpose27_reshape28, (lv919,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv371: R.Tensor((1280, 1280), dtype="float32") = model_params[570]
            lv372: R.Tensor((1280,), dtype="float32") = model_params[215]
            lv224_1 = R.call_tir(cls.fused_matmul20_add20_add21, (lv223_1, lv371, lv372, lv218_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv373: R.Tensor((1280,), dtype="float32") = model_params[223]
            lv374: R.Tensor((1280,), dtype="float32") = model_params[222]
            lv927 = R.call_tir(cls.layer_norm2, (lv224_1, lv373, lv374), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv375_1: R.Tensor((1280, 1280), dtype="float32") = model_params[571]
            lv929 = R.call_tir(cls.matmul20, (lv927, lv375_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv376_1: R.Tensor((768, 1280), dtype="float32") = model_params[572]
            lv931 = R.call_tir(cls.matmul23, (inp_2, lv376_1), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv377: R.Tensor((768, 1280), dtype="float32") = model_params[573]
            lv933 = R.call_tir(cls.matmul23, (inp_2, lv377), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv225_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv929,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv226_1 = R.call_tir(cls.fused_reshape29_transpose29_reshape30_transpose30, (lv931,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv227_1 = R.call_tir(cls.fused_reshape29_transpose29_reshape30, (lv933,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv228_1 = R.call_tir(cls.fused_matmul24_multiply10, (lv225_1, lv226_1), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv946 = R.call_tir(cls.softmax5, (lv228_1,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv947 = R.call_tir(cls.matmul25, (lv946, lv227_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv229_1 = R.call_tir(cls.fused_reshape27_transpose27_reshape28, (lv947,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv378: R.Tensor((1280, 1280), dtype="float32") = model_params[574]
            lv379: R.Tensor((1280,), dtype="float32") = model_params[216]
            lv230_1 = R.call_tir(cls.fused_matmul20_add20_add21, (lv229_1, lv378, lv379, lv224_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv380: R.Tensor((1280,), dtype="float32") = model_params[225]
            lv381: R.Tensor((1280,), dtype="float32") = model_params[224]
            lv955 = R.call_tir(cls.layer_norm2, (lv230_1, lv380, lv381), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv382: R.Tensor((1280, 5120), dtype="float32") = model_params[576]
            lv383: R.Tensor((5120,), dtype="float32") = model_params[218]
            lv231_1 = R.call_tir(cls.fused_matmul26_add22_gelu2, (lv955, lv382, lv383), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv384_1: R.Tensor((1280, 5120), dtype="float32") = model_params[575]
            lv385: R.Tensor((5120,), dtype="float32") = model_params[217]
            lv232_1 = R.call_tir(cls.fused_matmul26_add22_multiply11, (lv955, lv384_1, lv385, lv231_1), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv386_1: R.Tensor((5120, 1280), dtype="float32") = model_params[577]
            lv387: R.Tensor((1280,), dtype="float32") = model_params[219]
            lv233_2 = R.call_tir(cls.fused_matmul27_add20_add21, (lv232_1, lv386_1, lv387, lv230_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv234_1 = R.call_tir(cls.fused_reshape31_transpose33, (lv233_2,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv388_1: R.Tensor((1280, 1280, 1, 1), dtype="float32") = model_params[214]
            lv389: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[578]
            lv235_1 = R.call_tir(cls.fused_conv2d12_add17_add19, (lv234_1, lv388_1, lv389, lv216_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv974 = R.call_tir(cls.concatenate3, (lv235_1, lv127_1), out_sinfo=R.Tensor((2, 2560, 16, 16), dtype="float32"))
            lv390_1: R.Tensor((2560,), dtype="float32") = model_params[268]
            lv391: R.Tensor((2560,), dtype="float32") = model_params[267]
            lv236_1 = R.call_tir(cls.fused_group_norm11_silu8, (lv974, lv390_1, lv391), out_sinfo=R.Tensor((2, 2560, 16, 16), dtype="float32"))
            lv980 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv392: R.Tensor((1280, 1280), dtype="float32") = model_params[580]
            lv393: R.Tensor((1280,), dtype="float32") = model_params[271]
            lv237_1 = R.call_tir(cls.fused_matmul1_add_strided_slice5, (lv980, lv392, lv393), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv985 = R.call_tir(cls.reshape23, (lv237_1,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv394: R.Tensor((1280, 2560, 3, 3), dtype="float32") = model_params[264]
            lv395: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[579]
            lv238_2 = R.call_tir(cls.fused_conv2d18_add17_add18, (lv236_1, lv394, lv395, lv985), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv396: R.Tensor((1280,), dtype="float32") = model_params[270]
            lv397: R.Tensor((1280,), dtype="float32") = model_params[269]
            lv239_1 = R.call_tir(cls.fused_group_norm6_silu5, (lv238_2, lv396, lv397), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv398: R.Tensor((1280, 2560, 1, 1), dtype="float32") = model_params[266]
            lv399: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[582]
            lv240_1 = R.call_tir(cls.fused_conv2d19_add17, (lv974, lv398, lv399), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv400: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[265]
            lv401: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[581]
            lv241_1 = R.call_tir(cls.fused_conv2d10_add17_add19_divide3, (lv239_1, lv400, lv401, lv240_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv402: R.Tensor((1280,), dtype="float32") = model_params[227]
            lv403_1: R.Tensor((1280,), dtype="float32") = model_params[226]
            lv997 = R.call_tir(cls.group_norm7, (lv241_1, lv402, lv403_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv404_1: R.Tensor((1280, 1280, 1, 1), dtype="float32") = model_params[228]
            lv405: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[583]
            lv242_1 = R.call_tir(cls.fused_conv2d12_add17, (lv997, lv404_1, lv405), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv243_1 = R.call_tir(cls.fused_transpose24_reshape24, (lv242_1,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv406: R.Tensor((1280,), dtype="float32") = model_params[236]
            lv407: R.Tensor((1280,), dtype="float32") = model_params[235]
            lv1003 = R.call_tir(cls.layer_norm2, (lv243_1, lv406, lv407), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv408: R.Tensor((1280, 1280), dtype="float32") = model_params[584]
            lv1005 = R.call_tir(cls.matmul20, (lv1003, lv408), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv409: R.Tensor((1280, 1280), dtype="float32") = model_params[585]
            lv1007 = R.call_tir(cls.matmul20, (lv1003, lv409), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv410: R.Tensor((1280, 1280), dtype="float32") = model_params[586]
            lv1009 = R.call_tir(cls.matmul20, (lv1003, lv410), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv244_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv1005,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv245_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26_transpose26, (lv1007,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv246_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv1009,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv247_1 = R.call_tir(cls.fused_matmul21_multiply9, (lv244_1, lv245_1), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv1022 = R.call_tir(cls.softmax4, (lv247_1,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv1023 = R.call_tir(cls.matmul22, (lv1022, lv246_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv248_1 = R.call_tir(cls.fused_reshape27_transpose27_reshape28, (lv1023,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv411: R.Tensor((1280, 1280), dtype="float32") = model_params[587]
            lv412_1: R.Tensor((1280,), dtype="float32") = model_params[230]
            lv249_1 = R.call_tir(cls.fused_matmul20_add20_add21, (lv248_1, lv411, lv412_1, lv243_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv413: R.Tensor((1280,), dtype="float32") = model_params[238]
            lv414: R.Tensor((1280,), dtype="float32") = model_params[237]
            lv1031 = R.call_tir(cls.layer_norm2, (lv249_1, lv413, lv414), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv415: R.Tensor((1280, 1280), dtype="float32") = model_params[588]
            lv1033 = R.call_tir(cls.matmul20, (lv1031, lv415), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv416: R.Tensor((768, 1280), dtype="float32") = model_params[589]
            lv1035 = R.call_tir(cls.matmul23, (inp_2, lv416), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv417: R.Tensor((768, 1280), dtype="float32") = model_params[590]
            lv1037 = R.call_tir(cls.matmul23, (inp_2, lv417), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv250_2 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv1033,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv251_1 = R.call_tir(cls.fused_reshape29_transpose29_reshape30_transpose30, (lv1035,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv252_1 = R.call_tir(cls.fused_reshape29_transpose29_reshape30, (lv1037,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv253_1 = R.call_tir(cls.fused_matmul24_multiply10, (lv250_2, lv251_1), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv1050 = R.call_tir(cls.softmax5, (lv253_1,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv1051 = R.call_tir(cls.matmul25, (lv1050, lv252_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv254_1 = R.call_tir(cls.fused_reshape27_transpose27_reshape28, (lv1051,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv418: R.Tensor((1280, 1280), dtype="float32") = model_params[591]
            lv419: R.Tensor((1280,), dtype="float32") = model_params[231]
            lv255_1 = R.call_tir(cls.fused_matmul20_add20_add21, (lv254_1, lv418, lv419, lv249_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv420: R.Tensor((1280,), dtype="float32") = model_params[240]
            lv421: R.Tensor((1280,), dtype="float32") = model_params[239]
            lv1059 = R.call_tir(cls.layer_norm2, (lv255_1, lv420, lv421), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv422: R.Tensor((1280, 5120), dtype="float32") = model_params[593]
            lv423: R.Tensor((5120,), dtype="float32") = model_params[233]
            lv256_2 = R.call_tir(cls.fused_matmul26_add22_gelu2, (lv1059, lv422, lv423), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv424: R.Tensor((1280, 5120), dtype="float32") = model_params[592]
            lv425: R.Tensor((5120,), dtype="float32") = model_params[232]
            lv257_1 = R.call_tir(cls.fused_matmul26_add22_multiply11, (lv1059, lv424, lv425, lv256_2), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv426: R.Tensor((5120, 1280), dtype="float32") = model_params[594]
            lv427: R.Tensor((1280,), dtype="float32") = model_params[234]
            lv258_2 = R.call_tir(cls.fused_matmul27_add20_add21, (lv257_1, lv426, lv427, lv255_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv259_1 = R.call_tir(cls.fused_reshape31_transpose33, (lv258_2,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv428: R.Tensor((1280, 1280, 1, 1), dtype="float32") = model_params[229]
            lv429: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[595]
            lv260_2 = R.call_tir(cls.fused_conv2d12_add17_add19, (lv259_1, lv428, lv429, lv241_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv1078 = R.call_tir(cls.concatenate4, (lv260_2, lv102_1), out_sinfo=R.Tensor((2, 1920, 16, 16), dtype="float32"))
            lv430: R.Tensor((1920,), dtype="float32") = model_params[276]
            lv431: R.Tensor((1920,), dtype="float32") = model_params[275]
            lv261_1 = R.call_tir(cls.fused_group_norm12_silu9, (lv1078, lv430, lv431), out_sinfo=R.Tensor((2, 1920, 16, 16), dtype="float32"))
            lv1084 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv432: R.Tensor((1280, 1280), dtype="float32") = model_params[597]
            lv433: R.Tensor((1280,), dtype="float32") = model_params[279]
            lv262_2 = R.call_tir(cls.fused_matmul1_add_strided_slice5, (lv1084, lv432, lv433), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv1089 = R.call_tir(cls.reshape23, (lv262_2,), out_sinfo=R.Tensor((2, 1280, 1, 1), dtype="float32"))
            lv434: R.Tensor((1280, 1920, 3, 3), dtype="float32") = model_params[272]
            lv435: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[596]
            lv263_1 = R.call_tir(cls.fused_conv2d20_add17_add18, (lv261_1, lv434, lv435, lv1089), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv436: R.Tensor((1280,), dtype="float32") = model_params[278]
            lv437: R.Tensor((1280,), dtype="float32") = model_params[277]
            lv264_1 = R.call_tir(cls.fused_group_norm6_silu5, (lv263_1, lv436, lv437), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv438: R.Tensor((1280, 1920, 1, 1), dtype="float32") = model_params[274]
            lv439_1: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[599]
            lv265_1 = R.call_tir(cls.fused_conv2d21_add17, (lv1078, lv438, lv439_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv440: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[273]
            lv441: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[598]
            lv266_1 = R.call_tir(cls.fused_conv2d10_add17_add19_divide3, (lv264_1, lv440, lv441, lv265_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv442: R.Tensor((1280,), dtype="float32") = model_params[242]
            lv443: R.Tensor((1280,), dtype="float32") = model_params[241]
            lv1101 = R.call_tir(cls.group_norm7, (lv266_1, lv442, lv443), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv444_1: R.Tensor((1280, 1280, 1, 1), dtype="float32") = model_params[243]
            lv445: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[600]
            lv267_1 = R.call_tir(cls.fused_conv2d12_add17, (lv1101, lv444_1, lv445), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv268_1 = R.call_tir(cls.fused_transpose24_reshape24, (lv267_1,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv446: R.Tensor((1280,), dtype="float32") = model_params[251]
            lv447: R.Tensor((1280,), dtype="float32") = model_params[250]
            lv1107 = R.call_tir(cls.layer_norm2, (lv268_1, lv446, lv447), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv448: R.Tensor((1280, 1280), dtype="float32") = model_params[601]
            lv1109 = R.call_tir(cls.matmul20, (lv1107, lv448), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv449: R.Tensor((1280, 1280), dtype="float32") = model_params[602]
            lv1111 = R.call_tir(cls.matmul20, (lv1107, lv449), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv450: R.Tensor((1280, 1280), dtype="float32") = model_params[603]
            lv1113 = R.call_tir(cls.matmul20, (lv1107, lv450), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv269_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv1109,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv270_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26_transpose26, (lv1111,), out_sinfo=R.Tensor((16, 160, 256), dtype="float32"))
            lv271_1 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv1113,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv272_1 = R.call_tir(cls.fused_matmul21_multiply9, (lv269_1, lv270_1), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv1126 = R.call_tir(cls.softmax4, (lv272_1,), out_sinfo=R.Tensor((16, 256, 256), dtype="float32"))
            lv1127 = R.call_tir(cls.matmul22, (lv1126, lv271_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv273_1 = R.call_tir(cls.fused_reshape27_transpose27_reshape28, (lv1127,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv451: R.Tensor((1280, 1280), dtype="float32") = model_params[604]
            lv452: R.Tensor((1280,), dtype="float32") = model_params[245]
            lv274_1 = R.call_tir(cls.fused_matmul20_add20_add21, (lv273_1, lv451, lv452, lv268_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv453: R.Tensor((1280,), dtype="float32") = model_params[253]
            lv454: R.Tensor((1280,), dtype="float32") = model_params[252]
            lv1135 = R.call_tir(cls.layer_norm2, (lv274_1, lv453, lv454), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv455: R.Tensor((1280, 1280), dtype="float32") = model_params[605]
            lv1137 = R.call_tir(cls.matmul20, (lv1135, lv455), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv456_1: R.Tensor((768, 1280), dtype="float32") = model_params[606]
            lv1139 = R.call_tir(cls.matmul23, (inp_2, lv456_1), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv457: R.Tensor((768, 1280), dtype="float32") = model_params[607]
            lv1141 = R.call_tir(cls.matmul23, (inp_2, lv457), out_sinfo=R.Tensor((2, 77, 1280), dtype="float32"))
            lv275_2 = R.call_tir(cls.fused_reshape25_transpose25_reshape26, (lv1137,), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv276_2 = R.call_tir(cls.fused_reshape29_transpose29_reshape30_transpose30, (lv1139,), out_sinfo=R.Tensor((16, 160, 77), dtype="float32"))
            lv277_1 = R.call_tir(cls.fused_reshape29_transpose29_reshape30, (lv1141,), out_sinfo=R.Tensor((16, 77, 160), dtype="float32"))
            lv278_1 = R.call_tir(cls.fused_matmul24_multiply10, (lv275_2, lv276_2), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv1154 = R.call_tir(cls.softmax5, (lv278_1,), out_sinfo=R.Tensor((16, 256, 77), dtype="float32"))
            lv1155 = R.call_tir(cls.matmul25, (lv1154, lv277_1), out_sinfo=R.Tensor((16, 256, 160), dtype="float32"))
            lv279_1 = R.call_tir(cls.fused_reshape27_transpose27_reshape28, (lv1155,), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv458: R.Tensor((1280, 1280), dtype="float32") = model_params[608]
            lv459: R.Tensor((1280,), dtype="float32") = model_params[246]
            lv280_1 = R.call_tir(cls.fused_matmul20_add20_add21, (lv279_1, lv458, lv459, lv274_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv460: R.Tensor((1280,), dtype="float32") = model_params[255]
            lv461: R.Tensor((1280,), dtype="float32") = model_params[254]
            lv1163 = R.call_tir(cls.layer_norm2, (lv280_1, lv460, lv461), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv462_1: R.Tensor((1280, 5120), dtype="float32") = model_params[610]
            lv463: R.Tensor((5120,), dtype="float32") = model_params[248]
            lv281_1 = R.call_tir(cls.fused_matmul26_add22_gelu2, (lv1163, lv462_1, lv463), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv464_1: R.Tensor((1280, 5120), dtype="float32") = model_params[609]
            lv465: R.Tensor((5120,), dtype="float32") = model_params[247]
            lv282_1 = R.call_tir(cls.fused_matmul26_add22_multiply11, (lv1163, lv464_1, lv465, lv281_1), out_sinfo=R.Tensor((2, 256, 5120), dtype="float32"))
            lv466_1: R.Tensor((5120, 1280), dtype="float32") = model_params[611]
            lv467: R.Tensor((1280,), dtype="float32") = model_params[249]
            lv283_1 = R.call_tir(cls.fused_matmul27_add20_add21, (lv282_1, lv466_1, lv467, lv280_1), out_sinfo=R.Tensor((2, 256, 1280), dtype="float32"))
            lv284_2 = R.call_tir(cls.fused_reshape31_transpose33, (lv283_1,), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv468_1: R.Tensor((1280, 1280, 1, 1), dtype="float32") = model_params[244]
            lv469: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[612]
            lv285_1 = R.call_tir(cls.fused_conv2d12_add17_add19, (lv284_2, lv468_1, lv469, lv266_1), out_sinfo=R.Tensor((2, 1280, 16, 16), dtype="float32"))
            lv1182 = R.call_tir(cls.resize2d1, (lv285_1,), out_sinfo=R.Tensor((2, 1280, 32, 32), dtype="float32"))
            lv470: R.Tensor((1280, 1280, 3, 3), dtype="float32") = model_params[280]
            lv471: R.Tensor((1, 1280, 1, 1), dtype="float32") = model_params[613]
            lv286_2 = R.call_tir(cls.fused_conv2d22_add29, (lv1182, lv470, lv471), out_sinfo=R.Tensor((2, 1280, 32, 32), dtype="float32"))
            lv1186 = R.call_tir(cls.concatenate5, (lv286_2, lv101_1), out_sinfo=R.Tensor((2, 1920, 32, 32), dtype="float32"))
            lv472: R.Tensor((1920,), dtype="float32") = model_params[330]
            lv473: R.Tensor((1920,), dtype="float32") = model_params[329]
            lv287_1 = R.call_tir(cls.fused_group_norm13_silu10, (lv1186, lv472, lv473), out_sinfo=R.Tensor((2, 1920, 32, 32), dtype="float32"))
            lv1192 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv474: R.Tensor((1280, 640), dtype="float32") = model_params[615]
            lv475: R.Tensor((640,), dtype="float32") = model_params[333]
            lv288_2 = R.call_tir(cls.fused_matmul11_add10_strided_slice4, (lv1192, lv474, lv475), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv1197 = R.call_tir(cls.reshape13, (lv288_2,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv476: R.Tensor((640, 1920, 3, 3), dtype="float32") = model_params[326]
            lv477: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[614]
            lv289_1 = R.call_tir(cls.fused_conv2d23_add9_add11, (lv287_1, lv476, lv477, lv1197), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv478: R.Tensor((640,), dtype="float32") = model_params[332]
            lv479: R.Tensor((640,), dtype="float32") = model_params[331]
            lv290_2 = R.call_tir(cls.fused_group_norm3_silu3, (lv289_1, lv478, lv479), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv480: R.Tensor((640, 1920, 1, 1), dtype="float32") = model_params[328]
            lv481_1: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[617]
            lv291_1 = R.call_tir(cls.fused_conv2d24_add9, (lv1186, lv480, lv481_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv482_1: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[327]
            lv483: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[616]
            lv292_1 = R.call_tir(cls.fused_conv2d5_add9_add12_divide2, (lv290_2, lv482_1, lv483, lv291_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv484: R.Tensor((640,), dtype="float32") = model_params[282]
            lv485: R.Tensor((640,), dtype="float32") = model_params[281]
            lv1209 = R.call_tir(cls.group_norm4, (lv292_1, lv484, lv485), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv486: R.Tensor((640, 640, 1, 1), dtype="float32") = model_params[283]
            lv487: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[618]
            lv293_1 = R.call_tir(cls.fused_conv2d7_add9, (lv1209, lv486, lv487), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv294_1 = R.call_tir(cls.fused_transpose13_reshape14, (lv293_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv488: R.Tensor((640,), dtype="float32") = model_params[291]
            lv489: R.Tensor((640,), dtype="float32") = model_params[290]
            lv1215 = R.call_tir(cls.layer_norm1, (lv294_1, lv488, lv489), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv490_1: R.Tensor((640, 640), dtype="float32") = model_params[619]
            lv1217 = R.call_tir(cls.matmul12, (lv1215, lv490_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv491: R.Tensor((640, 640), dtype="float32") = model_params[620]
            lv1219 = R.call_tir(cls.matmul12, (lv1215, lv491), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv492_1: R.Tensor((640, 640), dtype="float32") = model_params[621]
            lv1221 = R.call_tir(cls.matmul12, (lv1215, lv492_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv295_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv1217,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv296_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16_transpose16, (lv1219,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv297_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv1221,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv298_1 = R.call_tir(cls.fused_matmul13_multiply6, (lv295_1, lv296_1), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1234 = R.call_tir(cls.softmax2, (lv298_1,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1235 = R.call_tir(cls.matmul14, (lv1234, lv297_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv299_1 = R.call_tir(cls.fused_reshape17_transpose17_reshape18, (lv1235,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv493: R.Tensor((640, 640), dtype="float32") = model_params[622]
            lv494_1: R.Tensor((640,), dtype="float32") = model_params[285]
            lv300_1 = R.call_tir(cls.fused_matmul12_add13_add14, (lv299_1, lv493, lv494_1, lv294_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv495: R.Tensor((640,), dtype="float32") = model_params[293]
            lv496_1: R.Tensor((640,), dtype="float32") = model_params[292]
            lv1243 = R.call_tir(cls.layer_norm1, (lv300_1, lv495, lv496_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv497: R.Tensor((640, 640), dtype="float32") = model_params[623]
            lv1245 = R.call_tir(cls.matmul12, (lv1243, lv497), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv498: R.Tensor((768, 640), dtype="float32") = model_params[624]
            lv1247 = R.call_tir(cls.matmul15, (inp_2, lv498), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv499: R.Tensor((768, 640), dtype="float32") = model_params[625]
            lv1249 = R.call_tir(cls.matmul15, (inp_2, lv499), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv301_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv1245,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv302_1 = R.call_tir(cls.fused_reshape19_transpose19_reshape20_transpose20, (lv1247,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv303_2 = R.call_tir(cls.fused_reshape19_transpose19_reshape20, (lv1249,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv304_2 = R.call_tir(cls.fused_matmul16_multiply7, (lv301_1, lv302_1), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1262 = R.call_tir(cls.softmax3, (lv304_2,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1263 = R.call_tir(cls.matmul17, (lv1262, lv303_2), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv305_1 = R.call_tir(cls.fused_reshape17_transpose17_reshape18, (lv1263,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv500: R.Tensor((640, 640), dtype="float32") = model_params[626]
            lv501: R.Tensor((640,), dtype="float32") = model_params[286]
            lv306_1 = R.call_tir(cls.fused_matmul12_add13_add14, (lv305_1, lv500, lv501, lv300_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv502: R.Tensor((640,), dtype="float32") = model_params[295]
            lv503: R.Tensor((640,), dtype="float32") = model_params[294]
            lv1271 = R.call_tir(cls.layer_norm1, (lv306_1, lv502, lv503), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv504: R.Tensor((640, 2560), dtype="float32") = model_params[628]
            lv505: R.Tensor((2560,), dtype="float32") = model_params[288]
            lv307_1 = R.call_tir(cls.fused_matmul18_add15_gelu1, (lv1271, lv504, lv505), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv506: R.Tensor((640, 2560), dtype="float32") = model_params[627]
            lv507: R.Tensor((2560,), dtype="float32") = model_params[287]
            lv308_1 = R.call_tir(cls.fused_matmul18_add15_multiply8, (lv1271, lv506, lv507, lv307_1), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv508: R.Tensor((2560, 640), dtype="float32") = model_params[629]
            lv509_1: R.Tensor((640,), dtype="float32") = model_params[289]
            lv309_1 = R.call_tir(cls.fused_matmul19_add13_add14, (lv308_1, lv508, lv509_1, lv306_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv310_1 = R.call_tir(cls.fused_reshape21_transpose23, (lv309_1,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv510_1: R.Tensor((640, 640, 1, 1), dtype="float32") = model_params[284]
            lv511: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[630]
            lv311_1 = R.call_tir(cls.fused_conv2d7_add9_add12, (lv310_1, lv510_1, lv511, lv292_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv1290 = R.call_tir(cls.concatenate6, (lv311_1, lv77_1), out_sinfo=R.Tensor((2, 1280, 32, 32), dtype="float32"))
            lv512: R.Tensor((1280,), dtype="float32") = model_params[338]
            lv513: R.Tensor((1280,), dtype="float32") = model_params[337]
            lv312_2 = R.call_tir(cls.fused_group_norm14_silu11, (lv1290, lv512, lv513), out_sinfo=R.Tensor((2, 1280, 32, 32), dtype="float32"))
            lv1296 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv514: R.Tensor((1280, 640), dtype="float32") = model_params[632]
            lv515: R.Tensor((640,), dtype="float32") = model_params[341]
            lv313_1 = R.call_tir(cls.fused_matmul11_add10_strided_slice4, (lv1296, lv514, lv515), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv1301 = R.call_tir(cls.reshape13, (lv313_1,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv516: R.Tensor((640, 1280, 3, 3), dtype="float32") = model_params[334]
            lv517: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[631]
            lv314_1 = R.call_tir(cls.fused_conv2d25_add9_add11, (lv312_2, lv516, lv517, lv1301), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv518_1: R.Tensor((640,), dtype="float32") = model_params[340]
            lv519: R.Tensor((640,), dtype="float32") = model_params[339]
            lv315_1 = R.call_tir(cls.fused_group_norm3_silu3, (lv314_1, lv518_1, lv519), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv520: R.Tensor((640, 1280, 1, 1), dtype="float32") = model_params[336]
            lv521: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[634]
            lv316_1 = R.call_tir(cls.fused_conv2d26_add9, (lv1290, lv520, lv521), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv522: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[335]
            lv523: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[633]
            lv317_1 = R.call_tir(cls.fused_conv2d5_add9_add12_divide2, (lv315_1, lv522, lv523, lv316_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv524: R.Tensor((640,), dtype="float32") = model_params[297]
            lv525: R.Tensor((640,), dtype="float32") = model_params[296]
            lv1313 = R.call_tir(cls.group_norm4, (lv317_1, lv524, lv525), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv526: R.Tensor((640, 640, 1, 1), dtype="float32") = model_params[298]
            lv527: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[635]
            lv318_1 = R.call_tir(cls.fused_conv2d7_add9, (lv1313, lv526, lv527), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv319_1 = R.call_tir(cls.fused_transpose13_reshape14, (lv318_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv528: R.Tensor((640,), dtype="float32") = model_params[306]
            lv529: R.Tensor((640,), dtype="float32") = model_params[305]
            lv1319 = R.call_tir(cls.layer_norm1, (lv319_1, lv528, lv529), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv530: R.Tensor((640, 640), dtype="float32") = model_params[636]
            lv1321 = R.call_tir(cls.matmul12, (lv1319, lv530), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv531: R.Tensor((640, 640), dtype="float32") = model_params[637]
            lv1323 = R.call_tir(cls.matmul12, (lv1319, lv531), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv532: R.Tensor((640, 640), dtype="float32") = model_params[638]
            lv1325 = R.call_tir(cls.matmul12, (lv1319, lv532), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv320_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv1321,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv321_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16_transpose16, (lv1323,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv322_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv1325,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv323_1 = R.call_tir(cls.fused_matmul13_multiply6, (lv320_1, lv321_1), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1338 = R.call_tir(cls.softmax2, (lv323_1,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1339 = R.call_tir(cls.matmul14, (lv1338, lv322_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv324_1 = R.call_tir(cls.fused_reshape17_transpose17_reshape18, (lv1339,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv533: R.Tensor((640, 640), dtype="float32") = model_params[639]
            lv534: R.Tensor((640,), dtype="float32") = model_params[300]
            lv325_1 = R.call_tir(cls.fused_matmul12_add13_add14, (lv324_1, lv533, lv534, lv319_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv535: R.Tensor((640,), dtype="float32") = model_params[308]
            lv536: R.Tensor((640,), dtype="float32") = model_params[307]
            lv1347 = R.call_tir(cls.layer_norm1, (lv325_1, lv535, lv536), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv537: R.Tensor((640, 640), dtype="float32") = model_params[640]
            lv1349 = R.call_tir(cls.matmul12, (lv1347, lv537), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv538: R.Tensor((768, 640), dtype="float32") = model_params[641]
            lv1351 = R.call_tir(cls.matmul15, (inp_2, lv538), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv539: R.Tensor((768, 640), dtype="float32") = model_params[642]
            lv1353 = R.call_tir(cls.matmul15, (inp_2, lv539), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv326_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv1349,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv327_1 = R.call_tir(cls.fused_reshape19_transpose19_reshape20_transpose20, (lv1351,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv328_1 = R.call_tir(cls.fused_reshape19_transpose19_reshape20, (lv1353,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv329_1 = R.call_tir(cls.fused_matmul16_multiply7, (lv326_1, lv327_1), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1366 = R.call_tir(cls.softmax3, (lv329_1,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1367 = R.call_tir(cls.matmul17, (lv1366, lv328_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv330_1 = R.call_tir(cls.fused_reshape17_transpose17_reshape18, (lv1367,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv540: R.Tensor((640, 640), dtype="float32") = model_params[643]
            lv541: R.Tensor((640,), dtype="float32") = model_params[301]
            lv331_1 = R.call_tir(cls.fused_matmul12_add13_add14, (lv330_1, lv540, lv541, lv325_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv542_1: R.Tensor((640,), dtype="float32") = model_params[310]
            lv543: R.Tensor((640,), dtype="float32") = model_params[309]
            lv1375 = R.call_tir(cls.layer_norm1, (lv331_1, lv542_1, lv543), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv544: R.Tensor((640, 2560), dtype="float32") = model_params[645]
            lv545: R.Tensor((2560,), dtype="float32") = model_params[303]
            lv332_1 = R.call_tir(cls.fused_matmul18_add15_gelu1, (lv1375, lv544, lv545), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv546: R.Tensor((640, 2560), dtype="float32") = model_params[644]
            lv547_1: R.Tensor((2560,), dtype="float32") = model_params[302]
            lv333_1 = R.call_tir(cls.fused_matmul18_add15_multiply8, (lv1375, lv546, lv547_1, lv332_1), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv548: R.Tensor((2560, 640), dtype="float32") = model_params[646]
            lv549: R.Tensor((640,), dtype="float32") = model_params[304]
            lv334_1 = R.call_tir(cls.fused_matmul19_add13_add14, (lv333_1, lv548, lv549, lv331_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv335_1 = R.call_tir(cls.fused_reshape21_transpose23, (lv334_1,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv550: R.Tensor((640, 640, 1, 1), dtype="float32") = model_params[299]
            lv551: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[647]
            lv336_2 = R.call_tir(cls.fused_conv2d7_add9_add12, (lv335_1, lv550, lv551, lv317_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv1394 = R.call_tir(cls.concatenate7, (lv336_2, lv52_2), out_sinfo=R.Tensor((2, 960, 32, 32), dtype="float32"))
            lv552: R.Tensor((960,), dtype="float32") = model_params[346]
            lv553: R.Tensor((960,), dtype="float32") = model_params[345]
            lv337_1 = R.call_tir(cls.fused_group_norm15_silu12, (lv1394, lv552, lv553), out_sinfo=R.Tensor((2, 960, 32, 32), dtype="float32"))
            lv1400 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv554: R.Tensor((1280, 640), dtype="float32") = model_params[649]
            lv555: R.Tensor((640,), dtype="float32") = model_params[349]
            lv338_1 = R.call_tir(cls.fused_matmul11_add10_strided_slice4, (lv1400, lv554, lv555), out_sinfo=R.Tensor((2, 640), dtype="float32"))
            lv1405 = R.call_tir(cls.reshape13, (lv338_1,), out_sinfo=R.Tensor((2, 640, 1, 1), dtype="float32"))
            lv556_1: R.Tensor((640, 960, 3, 3), dtype="float32") = model_params[342]
            lv557: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[648]
            lv339_1 = R.call_tir(cls.fused_conv2d27_add9_add11, (lv337_1, lv556_1, lv557, lv1405), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv558: R.Tensor((640,), dtype="float32") = model_params[348]
            lv559: R.Tensor((640,), dtype="float32") = model_params[347]
            lv340_1 = R.call_tir(cls.fused_group_norm3_silu3, (lv339_1, lv558, lv559), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv560: R.Tensor((640, 960, 1, 1), dtype="float32") = model_params[344]
            lv561: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[651]
            lv341_2 = R.call_tir(cls.fused_conv2d28_add9, (lv1394, lv560, lv561), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv562_1: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[343]
            lv563: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[650]
            lv342_1 = R.call_tir(cls.fused_conv2d5_add9_add12_divide2, (lv340_1, lv562_1, lv563, lv341_2), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv564_1: R.Tensor((640,), dtype="float32") = model_params[312]
            lv565: R.Tensor((640,), dtype="float32") = model_params[311]
            lv1417 = R.call_tir(cls.group_norm4, (lv342_1, lv564_1, lv565), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv566_1: R.Tensor((640, 640, 1, 1), dtype="float32") = model_params[313]
            lv567: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[652]
            lv343_1 = R.call_tir(cls.fused_conv2d7_add9, (lv1417, lv566_1, lv567), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv344_1 = R.call_tir(cls.fused_transpose13_reshape14, (lv343_1,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv568_1: R.Tensor((640,), dtype="float32") = model_params[321]
            lv569: R.Tensor((640,), dtype="float32") = model_params[320]
            lv1423 = R.call_tir(cls.layer_norm1, (lv344_1, lv568_1, lv569), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv570: R.Tensor((640, 640), dtype="float32") = model_params[653]
            lv1425 = R.call_tir(cls.matmul12, (lv1423, lv570), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv571: R.Tensor((640, 640), dtype="float32") = model_params[654]
            lv1427 = R.call_tir(cls.matmul12, (lv1423, lv571), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv572: R.Tensor((640, 640), dtype="float32") = model_params[655]
            lv1429 = R.call_tir(cls.matmul12, (lv1423, lv572), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv345_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv1425,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv346_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16_transpose16, (lv1427,), out_sinfo=R.Tensor((16, 80, 1024), dtype="float32"))
            lv347_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv1429,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv348_1 = R.call_tir(cls.fused_matmul13_multiply6, (lv345_1, lv346_1), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1442 = R.call_tir(cls.softmax2, (lv348_1,), out_sinfo=R.Tensor((16, 1024, 1024), dtype="float32"))
            lv1443 = R.call_tir(cls.matmul14, (lv1442, lv347_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv349_1 = R.call_tir(cls.fused_reshape17_transpose17_reshape18, (lv1443,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv573: R.Tensor((640, 640), dtype="float32") = model_params[656]
            lv574: R.Tensor((640,), dtype="float32") = model_params[315]
            lv350_2 = R.call_tir(cls.fused_matmul12_add13_add14, (lv349_1, lv573, lv574, lv344_1), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv575: R.Tensor((640,), dtype="float32") = model_params[323]
            lv576: R.Tensor((640,), dtype="float32") = model_params[322]
            lv1451 = R.call_tir(cls.layer_norm1, (lv350_2, lv575, lv576), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv577: R.Tensor((640, 640), dtype="float32") = model_params[657]
            lv1453 = R.call_tir(cls.matmul12, (lv1451, lv577), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv578: R.Tensor((768, 640), dtype="float32") = model_params[658]
            lv1455 = R.call_tir(cls.matmul15, (inp_2, lv578), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv579: R.Tensor((768, 640), dtype="float32") = model_params[659]
            lv1457 = R.call_tir(cls.matmul15, (inp_2, lv579), out_sinfo=R.Tensor((2, 77, 640), dtype="float32"))
            lv351_1 = R.call_tir(cls.fused_reshape15_transpose15_reshape16, (lv1453,), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv352_1 = R.call_tir(cls.fused_reshape19_transpose19_reshape20_transpose20, (lv1455,), out_sinfo=R.Tensor((16, 80, 77), dtype="float32"))
            lv353_1 = R.call_tir(cls.fused_reshape19_transpose19_reshape20, (lv1457,), out_sinfo=R.Tensor((16, 77, 80), dtype="float32"))
            lv354_1 = R.call_tir(cls.fused_matmul16_multiply7, (lv351_1, lv352_1), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1470 = R.call_tir(cls.softmax3, (lv354_1,), out_sinfo=R.Tensor((16, 1024, 77), dtype="float32"))
            lv1471 = R.call_tir(cls.matmul17, (lv1470, lv353_1), out_sinfo=R.Tensor((16, 1024, 80), dtype="float32"))
            lv355_1 = R.call_tir(cls.fused_reshape17_transpose17_reshape18, (lv1471,), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv580: R.Tensor((640, 640), dtype="float32") = model_params[660]
            lv581_1: R.Tensor((640,), dtype="float32") = model_params[316]
            lv356_2 = R.call_tir(cls.fused_matmul12_add13_add14, (lv355_1, lv580, lv581_1, lv350_2), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv582_1: R.Tensor((640,), dtype="float32") = model_params[325]
            lv583: R.Tensor((640,), dtype="float32") = model_params[324]
            lv1479 = R.call_tir(cls.layer_norm1, (lv356_2, lv582_1, lv583), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv584: R.Tensor((640, 2560), dtype="float32") = model_params[662]
            lv585: R.Tensor((2560,), dtype="float32") = model_params[318]
            lv357_1 = R.call_tir(cls.fused_matmul18_add15_gelu1, (lv1479, lv584, lv585), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv586: R.Tensor((640, 2560), dtype="float32") = model_params[661]
            lv587: R.Tensor((2560,), dtype="float32") = model_params[317]
            lv358_2 = R.call_tir(cls.fused_matmul18_add15_multiply8, (lv1479, lv586, lv587, lv357_1), out_sinfo=R.Tensor((2, 1024, 2560), dtype="float32"))
            lv588: R.Tensor((2560, 640), dtype="float32") = model_params[663]
            lv589: R.Tensor((640,), dtype="float32") = model_params[319]
            lv359_1 = R.call_tir(cls.fused_matmul19_add13_add14, (lv358_2, lv588, lv589, lv356_2), out_sinfo=R.Tensor((2, 1024, 640), dtype="float32"))
            lv360_2 = R.call_tir(cls.fused_reshape21_transpose23, (lv359_1,), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv590_1: R.Tensor((640, 640, 1, 1), dtype="float32") = model_params[314]
            lv591: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[664]
            lv361_1 = R.call_tir(cls.fused_conv2d7_add9_add12, (lv360_2, lv590_1, lv591, lv342_1), out_sinfo=R.Tensor((2, 640, 32, 32), dtype="float32"))
            lv1498 = R.call_tir(cls.resize2d2, (lv361_1,), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv592_1: R.Tensor((640, 640, 3, 3), dtype="float32") = model_params[350]
            lv593: R.Tensor((1, 640, 1, 1), dtype="float32") = model_params[665]
            lv362_2 = R.call_tir(cls.fused_conv2d29_add30, (lv1498, lv592_1, lv593), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv1502 = R.call_tir(cls.concatenate8, (lv362_2, lv51_1), out_sinfo=R.Tensor((2, 960, 64, 64), dtype="float32"))
            lv594_1: R.Tensor((960,), dtype="float32") = model_params[400]
            lv595: R.Tensor((960,), dtype="float32") = model_params[399]
            lv363_1 = R.call_tir(cls.fused_group_norm16_silu13, (lv1502, lv594_1, lv595), out_sinfo=R.Tensor((2, 960, 64, 64), dtype="float32"))
            lv1508 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv596_1: R.Tensor((1280, 320), dtype="float32") = model_params[667]
            lv597: R.Tensor((320,), dtype="float32") = model_params[403]
            lv364_1 = R.call_tir(cls.fused_matmul2_add2_strided_slice3, (lv1508, lv596_1, lv597), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv1513 = R.call_tir(cls.reshape3, (lv364_1,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv598: R.Tensor((320, 960, 3, 3), dtype="float32") = model_params[396]
            lv599: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[666]
            lv365_1 = R.call_tir(cls.fused_conv2d30_add1_add3, (lv363_1, lv598, lv599, lv1513), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv600: R.Tensor((320,), dtype="float32") = model_params[402]
            lv601: R.Tensor((320,), dtype="float32") = model_params[401]
            lv366_1 = R.call_tir(cls.fused_group_norm_silu1, (lv365_1, lv600, lv601), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv602: R.Tensor((320, 960, 1, 1), dtype="float32") = model_params[398]
            lv603: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[669]
            lv367_1 = R.call_tir(cls.fused_conv2d31_add1, (lv1502, lv602, lv603), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv604: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[397]
            lv605: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[668]
            lv368_1 = R.call_tir(cls.fused_conv2d1_add1_add4_divide1, (lv366_1, lv604, lv605, lv367_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv606: R.Tensor((320,), dtype="float32") = model_params[352]
            lv607: R.Tensor((320,), dtype="float32") = model_params[351]
            lv1525 = R.call_tir(cls.group_norm1, (lv368_1, lv606, lv607), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv608: R.Tensor((320, 320, 1, 1), dtype="float32") = model_params[353]
            lv609_1: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[670]
            lv369_1 = R.call_tir(cls.fused_conv2d2_add1, (lv1525, lv608, lv609_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv370_1 = R.call_tir(cls.fused_transpose3_reshape4, (lv369_1,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv610_1: R.Tensor((320,), dtype="float32") = model_params[361]
            lv611: R.Tensor((320,), dtype="float32") = model_params[360]
            lv1531 = R.call_tir(cls.layer_norm, (lv370_1, lv610_1, lv611), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv612: R.Tensor((320, 320), dtype="float32") = model_params[671]
            lv1533 = R.call_tir(cls.matmul3, (lv1531, lv612), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv613: R.Tensor((320, 320), dtype="float32") = model_params[672]
            lv1535 = R.call_tir(cls.matmul3, (lv1531, lv613), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv614: R.Tensor((320, 320), dtype="float32") = model_params[673]
            lv1537 = R.call_tir(cls.matmul3, (lv1531, lv614), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv371_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv1533,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv372_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6_transpose6, (lv1535,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv373_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv1537,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv374_1 = R.call_tir(cls.fused_matmul4_multiply3, (lv371_1, lv372_1), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1550 = R.call_tir(cls.softmax, (lv374_1,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1551 = R.call_tir(cls.matmul5, (lv1550, lv373_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv375_2 = R.call_tir(cls.fused_reshape7_transpose7_reshape8, (lv1551,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv615: R.Tensor((320, 320), dtype="float32") = model_params[674]
            lv616: R.Tensor((320,), dtype="float32") = model_params[355]
            lv376_2 = R.call_tir(cls.fused_matmul3_add5_add6, (lv375_2, lv615, lv616, lv370_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv617: R.Tensor((320,), dtype="float32") = model_params[363]
            lv618_1: R.Tensor((320,), dtype="float32") = model_params[362]
            lv1559 = R.call_tir(cls.layer_norm, (lv376_2, lv617, lv618_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv619: R.Tensor((320, 320), dtype="float32") = model_params[675]
            lv1561 = R.call_tir(cls.matmul3, (lv1559, lv619), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv620: R.Tensor((768, 320), dtype="float32") = model_params[676]
            lv1563 = R.call_tir(cls.matmul6, (inp_2, lv620), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv621: R.Tensor((768, 320), dtype="float32") = model_params[677]
            lv1565 = R.call_tir(cls.matmul6, (inp_2, lv621), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv377_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv1561,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv378_1 = R.call_tir(cls.fused_reshape9_transpose9_reshape10_transpose10, (lv1563,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv379_1 = R.call_tir(cls.fused_reshape9_transpose9_reshape10, (lv1565,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv380_1 = R.call_tir(cls.fused_matmul7_multiply4, (lv377_1, lv378_1), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1578 = R.call_tir(cls.softmax1, (lv380_1,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1579 = R.call_tir(cls.matmul8, (lv1578, lv379_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv381_1 = R.call_tir(cls.fused_reshape7_transpose7_reshape8, (lv1579,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv622: R.Tensor((320, 320), dtype="float32") = model_params[678]
            lv623: R.Tensor((320,), dtype="float32") = model_params[356]
            lv382_1 = R.call_tir(cls.fused_matmul3_add5_add6, (lv381_1, lv622, lv623, lv376_2), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv624: R.Tensor((320,), dtype="float32") = model_params[365]
            lv625: R.Tensor((320,), dtype="float32") = model_params[364]
            lv1587 = R.call_tir(cls.layer_norm, (lv382_1, lv624, lv625), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv626: R.Tensor((320, 1280), dtype="float32") = model_params[680]
            lv627: R.Tensor((1280,), dtype="float32") = model_params[358]
            lv383_1 = R.call_tir(cls.fused_matmul9_add7_gelu, (lv1587, lv626, lv627), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv628: R.Tensor((320, 1280), dtype="float32") = model_params[679]
            lv629: R.Tensor((1280,), dtype="float32") = model_params[357]
            lv384_2 = R.call_tir(cls.fused_matmul9_add7_multiply5, (lv1587, lv628, lv629, lv383_1), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv630: R.Tensor((1280, 320), dtype="float32") = model_params[681]
            lv631: R.Tensor((320,), dtype="float32") = model_params[359]
            lv385_1 = R.call_tir(cls.fused_matmul10_add5_add6, (lv384_2, lv630, lv631, lv382_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv386_2 = R.call_tir(cls.fused_reshape11_transpose11, (lv385_1,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv632: R.Tensor((320, 320, 1, 1), dtype="float32") = model_params[354]
            lv633: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[682]
            lv387_1 = R.call_tir(cls.fused_conv2d2_add1_add4, (lv386_2, lv632, lv633, lv368_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv1606 = R.call_tir(cls.concatenate9, (lv387_1, lv27_1), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv634: R.Tensor((640,), dtype="float32") = model_params[408]
            lv635: R.Tensor((640,), dtype="float32") = model_params[407]
            lv388_2 = R.call_tir(cls.fused_group_norm17_silu14, (lv1606, lv634, lv635), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv1612 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv636: R.Tensor((1280, 320), dtype="float32") = model_params[684]
            lv637: R.Tensor((320,), dtype="float32") = model_params[411]
            lv389_1 = R.call_tir(cls.fused_matmul2_add2_strided_slice3, (lv1612, lv636, lv637), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv1617 = R.call_tir(cls.reshape3, (lv389_1,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv638: R.Tensor((320, 640, 3, 3), dtype="float32") = model_params[404]
            lv639: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[683]
            lv390_2 = R.call_tir(cls.fused_conv2d32_add1_add3, (lv388_2, lv638, lv639, lv1617), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv640: R.Tensor((320,), dtype="float32") = model_params[410]
            lv641: R.Tensor((320,), dtype="float32") = model_params[409]
            lv391_1 = R.call_tir(cls.fused_group_norm_silu1, (lv390_2, lv640, lv641), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv642: R.Tensor((320, 640, 1, 1), dtype="float32") = model_params[406]
            lv643: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[686]
            lv392_1 = R.call_tir(cls.fused_conv2d33_add1, (lv1606, lv642, lv643), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv644: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[405]
            lv645_1: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[685]
            lv393_1 = R.call_tir(cls.fused_conv2d1_add1_add4_divide1, (lv391_1, lv644, lv645_1, lv392_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv646: R.Tensor((320,), dtype="float32") = model_params[367]
            lv647: R.Tensor((320,), dtype="float32") = model_params[366]
            lv1629 = R.call_tir(cls.group_norm1, (lv393_1, lv646, lv647), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv648: R.Tensor((320, 320, 1, 1), dtype="float32") = model_params[368]
            lv649: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[687]
            lv394_1 = R.call_tir(cls.fused_conv2d2_add1, (lv1629, lv648, lv649), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv395_1 = R.call_tir(cls.fused_transpose3_reshape4, (lv394_1,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv650_1: R.Tensor((320,), dtype="float32") = model_params[376]
            lv651: R.Tensor((320,), dtype="float32") = model_params[375]
            lv1635 = R.call_tir(cls.layer_norm, (lv395_1, lv650_1, lv651), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv652: R.Tensor((320, 320), dtype="float32") = model_params[688]
            lv1637 = R.call_tir(cls.matmul3, (lv1635, lv652), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv653: R.Tensor((320, 320), dtype="float32") = model_params[689]
            lv1639 = R.call_tir(cls.matmul3, (lv1635, lv653), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv654: R.Tensor((320, 320), dtype="float32") = model_params[690]
            lv1641 = R.call_tir(cls.matmul3, (lv1635, lv654), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv396_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv1637,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv397_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6_transpose6, (lv1639,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv398_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv1641,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv399_1 = R.call_tir(cls.fused_matmul4_multiply3, (lv396_1, lv397_1), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1654 = R.call_tir(cls.softmax, (lv399_1,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1655 = R.call_tir(cls.matmul5, (lv1654, lv398_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv400_1 = R.call_tir(cls.fused_reshape7_transpose7_reshape8, (lv1655,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv655: R.Tensor((320, 320), dtype="float32") = model_params[691]
            lv656: R.Tensor((320,), dtype="float32") = model_params[370]
            lv401_1 = R.call_tir(cls.fused_matmul3_add5_add6, (lv400_1, lv655, lv656, lv395_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv657: R.Tensor((320,), dtype="float32") = model_params[378]
            lv658: R.Tensor((320,), dtype="float32") = model_params[377]
            lv1663 = R.call_tir(cls.layer_norm, (lv401_1, lv657, lv658), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv659: R.Tensor((320, 320), dtype="float32") = model_params[692]
            lv1665 = R.call_tir(cls.matmul3, (lv1663, lv659), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv660: R.Tensor((768, 320), dtype="float32") = model_params[693]
            lv1667 = R.call_tir(cls.matmul6, (inp_2, lv660), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv661: R.Tensor((768, 320), dtype="float32") = model_params[694]
            lv1669 = R.call_tir(cls.matmul6, (inp_2, lv661), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv402_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv1665,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv403_2 = R.call_tir(cls.fused_reshape9_transpose9_reshape10_transpose10, (lv1667,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv404_2 = R.call_tir(cls.fused_reshape9_transpose9_reshape10, (lv1669,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv405_1 = R.call_tir(cls.fused_matmul7_multiply4, (lv402_1, lv403_2), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1682 = R.call_tir(cls.softmax1, (lv405_1,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1683 = R.call_tir(cls.matmul8, (lv1682, lv404_2), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv406_1 = R.call_tir(cls.fused_reshape7_transpose7_reshape8, (lv1683,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv662: R.Tensor((320, 320), dtype="float32") = model_params[695]
            lv663: R.Tensor((320,), dtype="float32") = model_params[371]
            lv407_1 = R.call_tir(cls.fused_matmul3_add5_add6, (lv406_1, lv662, lv663, lv401_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv664_1: R.Tensor((320,), dtype="float32") = model_params[380]
            lv665: R.Tensor((320,), dtype="float32") = model_params[379]
            lv1691 = R.call_tir(cls.layer_norm, (lv407_1, lv664_1, lv665), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv666: R.Tensor((320, 1280), dtype="float32") = model_params[697]
            lv667: R.Tensor((1280,), dtype="float32") = model_params[373]
            lv408_1 = R.call_tir(cls.fused_matmul9_add7_gelu, (lv1691, lv666, lv667), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv668: R.Tensor((320, 1280), dtype="float32") = model_params[696]
            lv669_1: R.Tensor((1280,), dtype="float32") = model_params[372]
            lv409_1 = R.call_tir(cls.fused_matmul9_add7_multiply5, (lv1691, lv668, lv669_1, lv408_1), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv670: R.Tensor((1280, 320), dtype="float32") = model_params[698]
            lv671: R.Tensor((320,), dtype="float32") = model_params[374]
            lv410_1 = R.call_tir(cls.fused_matmul10_add5_add6, (lv409_1, lv670, lv671, lv407_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv411_1 = R.call_tir(cls.fused_reshape11_transpose11, (lv410_1,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv672: R.Tensor((320, 320, 1, 1), dtype="float32") = model_params[369]
            lv673: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[699]
            lv412_2 = R.call_tir(cls.fused_conv2d2_add1_add4, (lv411_1, lv672, lv673, lv393_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv1710 = R.call_tir(cls.concatenate9, (lv412_2, lv3_1), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv674: R.Tensor((640,), dtype="float32") = model_params[416]
            lv675: R.Tensor((640,), dtype="float32") = model_params[415]
            lv413_1 = R.call_tir(cls.fused_group_norm17_silu14, (lv1710, lv674, lv675), out_sinfo=R.Tensor((2, 640, 64, 64), dtype="float32"))
            lv1716 = R.call_tir(cls.silu, (lv2_1,), out_sinfo=R.Tensor((2, 1280), dtype="float32"))
            lv676: R.Tensor((1280, 320), dtype="float32") = model_params[701]
            lv677: R.Tensor((320,), dtype="float32") = model_params[419]
            lv414_1 = R.call_tir(cls.fused_matmul2_add2_strided_slice3, (lv1716, lv676, lv677), out_sinfo=R.Tensor((2, 320), dtype="float32"))
            lv1721 = R.call_tir(cls.reshape3, (lv414_1,), out_sinfo=R.Tensor((2, 320, 1, 1), dtype="float32"))
            lv678: R.Tensor((320, 640, 3, 3), dtype="float32") = model_params[412]
            lv679: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[700]
            lv415_1 = R.call_tir(cls.fused_conv2d32_add1_add3, (lv413_1, lv678, lv679, lv1721), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv680: R.Tensor((320,), dtype="float32") = model_params[418]
            lv681: R.Tensor((320,), dtype="float32") = model_params[417]
            lv416_1 = R.call_tir(cls.fused_group_norm_silu1, (lv415_1, lv680, lv681), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv682: R.Tensor((320, 640, 1, 1), dtype="float32") = model_params[414]
            lv683_1: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[703]
            lv417_1 = R.call_tir(cls.fused_conv2d33_add1, (lv1710, lv682, lv683_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv684: R.Tensor((320, 320, 3, 3), dtype="float32") = model_params[413]
            lv685: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[702]
            lv418_1 = R.call_tir(cls.fused_conv2d1_add1_add4_divide1, (lv416_1, lv684, lv685, lv417_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv686: R.Tensor((320,), dtype="float32") = model_params[382]
            lv687: R.Tensor((320,), dtype="float32") = model_params[381]
            lv1733 = R.call_tir(cls.group_norm1, (lv418_1, lv686, lv687), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv688_1: R.Tensor((320, 320, 1, 1), dtype="float32") = model_params[383]
            lv689: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[704]
            lv419_1 = R.call_tir(cls.fused_conv2d2_add1, (lv1733, lv688_1, lv689), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv420_1 = R.call_tir(cls.fused_transpose3_reshape4, (lv419_1,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv690: R.Tensor((320,), dtype="float32") = model_params[391]
            lv691: R.Tensor((320,), dtype="float32") = model_params[390]
            lv1739 = R.call_tir(cls.layer_norm, (lv420_1, lv690, lv691), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv692: R.Tensor((320, 320), dtype="float32") = model_params[705]
            lv1741 = R.call_tir(cls.matmul3, (lv1739, lv692), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv693: R.Tensor((320, 320), dtype="float32") = model_params[706]
            lv1743 = R.call_tir(cls.matmul3, (lv1739, lv693), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv694: R.Tensor((320, 320), dtype="float32") = model_params[707]
            lv1745 = R.call_tir(cls.matmul3, (lv1739, lv694), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv421_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv1741,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv422_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6_transpose6, (lv1743,), out_sinfo=R.Tensor((16, 40, 4096), dtype="float32"))
            lv423_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv1745,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv424_1 = R.call_tir(cls.fused_matmul4_multiply3, (lv421_1, lv422_1), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1758 = R.call_tir(cls.softmax, (lv424_1,), out_sinfo=R.Tensor((16, 4096, 4096), dtype="float32"))
            lv1759 = R.call_tir(cls.matmul5, (lv1758, lv423_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv425_1 = R.call_tir(cls.fused_reshape7_transpose7_reshape8, (lv1759,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv695: R.Tensor((320, 320), dtype="float32") = model_params[708]
            lv696: R.Tensor((320,), dtype="float32") = model_params[385]
            lv426_1 = R.call_tir(cls.fused_matmul3_add5_add6, (lv425_1, lv695, lv696, lv420_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv697_1: R.Tensor((320,), dtype="float32") = model_params[393]
            lv698: R.Tensor((320,), dtype="float32") = model_params[392]
            lv1767 = R.call_tir(cls.layer_norm, (lv426_1, lv697_1, lv698), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv699: R.Tensor((320, 320), dtype="float32") = model_params[709]
            lv1769 = R.call_tir(cls.matmul3, (lv1767, lv699), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv700: R.Tensor((768, 320), dtype="float32") = model_params[710]
            lv1771 = R.call_tir(cls.matmul6, (inp_2, lv700), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv701: R.Tensor((768, 320), dtype="float32") = model_params[711]
            lv1773 = R.call_tir(cls.matmul6, (inp_2, lv701), out_sinfo=R.Tensor((2, 77, 320), dtype="float32"))
            lv427_1 = R.call_tir(cls.fused_reshape5_transpose5_reshape6, (lv1769,), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv428_1 = R.call_tir(cls.fused_reshape9_transpose9_reshape10_transpose10, (lv1771,), out_sinfo=R.Tensor((16, 40, 77), dtype="float32"))
            lv429_1 = R.call_tir(cls.fused_reshape9_transpose9_reshape10, (lv1773,), out_sinfo=R.Tensor((16, 77, 40), dtype="float32"))
            lv430_1 = R.call_tir(cls.fused_matmul7_multiply4, (lv427_1, lv428_1), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1786 = R.call_tir(cls.softmax1, (lv430_1,), out_sinfo=R.Tensor((16, 4096, 77), dtype="float32"))
            lv1787 = R.call_tir(cls.matmul8, (lv1786, lv429_1), out_sinfo=R.Tensor((16, 4096, 40), dtype="float32"))
            lv431_1 = R.call_tir(cls.fused_reshape7_transpose7_reshape8, (lv1787,), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv702: R.Tensor((320, 320), dtype="float32") = model_params[712]
            lv703_1: R.Tensor((320,), dtype="float32") = model_params[386]
            lv432_1 = R.call_tir(cls.fused_matmul3_add5_add6, (lv431_1, lv702, lv703_1, lv426_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv704: R.Tensor((320,), dtype="float32") = model_params[395]
            lv705_1: R.Tensor((320,), dtype="float32") = model_params[394]
            lv1795 = R.call_tir(cls.layer_norm, (lv432_1, lv704, lv705_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv706: R.Tensor((320, 1280), dtype="float32") = model_params[714]
            lv707_1: R.Tensor((1280,), dtype="float32") = model_params[388]
            lv433_1 = R.call_tir(cls.fused_matmul9_add7_gelu, (lv1795, lv706, lv707_1), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv708: R.Tensor((320, 1280), dtype="float32") = model_params[713]
            lv709_1: R.Tensor((1280,), dtype="float32") = model_params[387]
            lv434_1 = R.call_tir(cls.fused_matmul9_add7_multiply5, (lv1795, lv708, lv709_1, lv433_1), out_sinfo=R.Tensor((2, 4096, 1280), dtype="float32"))
            lv710: R.Tensor((1280, 320), dtype="float32") = model_params[715]
            lv711: R.Tensor((320,), dtype="float32") = model_params[389]
            lv435_1 = R.call_tir(cls.fused_matmul10_add5_add6, (lv434_1, lv710, lv711, lv432_1), out_sinfo=R.Tensor((2, 4096, 320), dtype="float32"))
            lv436_1 = R.call_tir(cls.fused_reshape11_transpose11, (lv435_1,), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv712: R.Tensor((320, 320, 1, 1), dtype="float32") = model_params[384]
            lv713: R.Tensor((1, 320, 1, 1), dtype="float32") = model_params[716]
            lv437_1 = R.call_tir(cls.fused_conv2d2_add1_add4, (lv436_1, lv712, lv713, lv418_1), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv714: R.Tensor((320,), dtype="float32") = model_params[2]
            lv715: R.Tensor((320,), dtype="float32") = model_params[1]
            lv438_1 = R.call_tir(cls.fused_group_norm_silu1, (lv437_1, lv714, lv715), out_sinfo=R.Tensor((2, 320, 64, 64), dtype="float32"))
            lv716: R.Tensor((4, 320, 3, 3), dtype="float32") = model_params[3]
            lv717: R.Tensor((1, 4, 1, 1), dtype="float32") = model_params[717]
            lv439_2 = R.call_tir(cls.fused_conv2d34_add31, (lv438_1, lv716, lv717), out_sinfo=R.Tensor((2, 4, 64, 64), dtype="float32"))
            lv440_1 = R.call_tir(cls.fused_split_subtract_multiply15_add32, (lv439_2,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            gv: R.Tensor((1, 4, 64, 64), dtype="float32") = lv440_1
            R.output(gv)
        return gv

    @R.function
    def vae(inp_0: R.Tensor((1, 4, 64, 64), dtype="float32"), model_params: R.Tuple(R.Tensor((512, 4, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((3, 128, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512, 512, 3, 3), dtype="float32"), R.Tensor((256, 512, 3, 3), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256, 512, 1, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256, 256, 3, 3), dtype="float32"), R.Tensor((128, 256, 3, 3), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128, 256, 1, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128, 128, 3, 3), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((4, 4, 1, 1), dtype="float32"), R.Tensor((1, 4, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((512, 512), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 512, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 256, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 128, 1, 1), dtype="float32"), R.Tensor((1, 3, 1, 1), dtype="float32"))) -> R.Tensor((1, 512, 512, 3), dtype="float32"):
        R.func_attr({"global_symbol": "main", "num_input": 1})
        cls = Module
        with R.dataflow():
            lv = R.call_tir(cls.multiply28, (inp_0,), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            lv914: R.Tensor((4, 4, 1, 1), dtype="float32") = model_params[99]
            lv915: R.Tensor((1, 4, 1, 1), dtype="float32") = model_params[100]
            lv575 = R.call_tir(cls.fused_conv2d35_add38, (lv, lv914, lv915), out_sinfo=R.Tensor((1, 4, 64, 64), dtype="float32"))
            lv916: R.Tensor((512, 4, 3, 3), dtype="float32") = model_params[0]
            lv917: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[101]
            lv576 = R.call_tir(cls.fused_conv2d36_add39, (lv575, lv916, lv917), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv918: R.Tensor((512,), dtype="float32") = model_params[13]
            lv919: R.Tensor((512,), dtype="float32") = model_params[12]
            lv577 = R.call_tir(cls.fused_group_norm18_silu15, (lv576, lv918, lv919), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv920: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[10]
            lv921: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[102]
            lv578 = R.call_tir(cls.fused_conv2d37_add39, (lv577, lv920, lv921), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv922: R.Tensor((512,), dtype="float32") = model_params[15]
            lv923: R.Tensor((512,), dtype="float32") = model_params[14]
            lv579 = R.call_tir(cls.fused_group_norm18_silu15, (lv578, lv922, lv923), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv924: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[11]
            lv925: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[103]
            lv580 = R.call_tir(cls.fused_conv2d37_add39_add40_divide7, (lv579, lv924, lv925, lv576), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv581 = R.call_tir(cls.fused_reshape51_transpose45_transpose46, (lv580,), out_sinfo=R.Tensor((1, 512, 4096), dtype="float32"))
            lv926: R.Tensor((512,), dtype="float32") = model_params[5]
            lv927: R.Tensor((512,), dtype="float32") = model_params[4]
            lv22 = R.call_tir(cls.group_norm19, (lv581, lv926, lv927), out_sinfo=R.Tensor((1, 512, 4096), dtype="float32"))
            lv23 = R.call_tir(cls.transpose45, (lv22,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv928: R.Tensor((512, 512), dtype="float32") = model_params[104]
            lv929: R.Tensor((512,), dtype="float32") = model_params[8]
            lv582 = R.call_tir(cls.fused_matmul40_add41, (lv23, lv928, lv929), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv930: R.Tensor((512, 512), dtype="float32") = model_params[105]
            lv931: R.Tensor((512,), dtype="float32") = model_params[6]
            lv583 = R.call_tir(cls.fused_matmul40_add41, (lv23, lv930, lv931), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv932: R.Tensor((512, 512), dtype="float32") = model_params[106]
            lv933: R.Tensor((512,), dtype="float32") = model_params[9]
            lv584 = R.call_tir(cls.fused_matmul40_add41, (lv23, lv932, lv933), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv585 = R.call_tir(cls.fused_reshape52_transpose48, (lv582,), out_sinfo=R.Tensor((1, 1, 4096, 512), dtype="float32"))
            lv586 = R.call_tir(cls.fused_reshape52_transpose48_transpose49, (lv583,), out_sinfo=R.Tensor((1, 1, 512, 4096), dtype="float32"))
            lv587 = R.call_tir(cls.fused_reshape52_transpose48, (lv584,), out_sinfo=R.Tensor((1, 1, 4096, 512), dtype="float32"))
            lv588 = R.call_tir(cls.fused_matmul41_multiply29, (lv585, lv586, R.const(0.044194173067808151, "float32")), out_sinfo=R.Tensor((1, 1, 4096, 4096), dtype="float32"))
            lv44 = R.call_tir(cls.softmax9, (lv588,), out_sinfo=R.Tensor((1, 1, 4096, 4096), dtype="float32"))
            lv45 = R.call_tir(cls.matmul42, (lv44, lv587), out_sinfo=R.Tensor((1, 1, 4096, 512), dtype="float32"))
            lv589 = R.call_tir(cls.fused_transpose50_reshape53, (lv45,), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv934: R.Tensor((512, 512), dtype="float32") = model_params[107]
            lv935: R.Tensor((512,), dtype="float32") = model_params[7]
            lv590 = R.call_tir(cls.fused_matmul40_add41, (lv589, lv934, lv935), out_sinfo=R.Tensor((1, 4096, 512), dtype="float32"))
            lv591 = R.call_tir(cls.fused_transpose46_reshape54_add40_divide7, (lv590, lv580), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv936: R.Tensor((512,), dtype="float32") = model_params[19]
            lv937: R.Tensor((512,), dtype="float32") = model_params[18]
            lv592 = R.call_tir(cls.fused_group_norm18_silu15, (lv591, lv936, lv937), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv938: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[16]
            lv939: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[108]
            lv593 = R.call_tir(cls.fused_conv2d37_add39, (lv592, lv938, lv939), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv940: R.Tensor((512,), dtype="float32") = model_params[21]
            lv941: R.Tensor((512,), dtype="float32") = model_params[20]
            lv594 = R.call_tir(cls.fused_group_norm18_silu15, (lv593, lv940, lv941), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv942: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[17]
            lv943: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[109]
            lv595 = R.call_tir(cls.fused_conv2d37_add39_add40_divide7_divide7, (lv594, lv942, lv943, lv591), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv944: R.Tensor((512,), dtype="float32") = model_params[25]
            lv945: R.Tensor((512,), dtype="float32") = model_params[24]
            lv596 = R.call_tir(cls.fused_group_norm18_silu15, (lv595, lv944, lv945), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv946: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[22]
            lv947: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[110]
            lv597 = R.call_tir(cls.fused_conv2d37_add39, (lv596, lv946, lv947), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv948: R.Tensor((512,), dtype="float32") = model_params[27]
            lv949: R.Tensor((512,), dtype="float32") = model_params[26]
            lv598 = R.call_tir(cls.fused_group_norm18_silu15, (lv597, lv948, lv949), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv950: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[23]
            lv951: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[111]
            lv599 = R.call_tir(cls.fused_conv2d37_add39_add40_divide7, (lv598, lv950, lv951, lv595), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv952: R.Tensor((512,), dtype="float32") = model_params[31]
            lv953: R.Tensor((512,), dtype="float32") = model_params[30]
            lv600 = R.call_tir(cls.fused_group_norm18_silu15, (lv599, lv952, lv953), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv954: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[28]
            lv955: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[112]
            lv601 = R.call_tir(cls.fused_conv2d37_add39, (lv600, lv954, lv955), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv956: R.Tensor((512,), dtype="float32") = model_params[33]
            lv957: R.Tensor((512,), dtype="float32") = model_params[32]
            lv602 = R.call_tir(cls.fused_group_norm18_silu15, (lv601, lv956, lv957), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv958: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[29]
            lv959: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[113]
            lv603 = R.call_tir(cls.fused_conv2d37_add39_add40_divide7, (lv602, lv958, lv959, lv599), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv960: R.Tensor((512,), dtype="float32") = model_params[37]
            lv961: R.Tensor((512,), dtype="float32") = model_params[36]
            lv604 = R.call_tir(cls.fused_group_norm18_silu15, (lv603, lv960, lv961), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv962: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[34]
            lv963: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[114]
            lv605 = R.call_tir(cls.fused_conv2d37_add39, (lv604, lv962, lv963), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv964: R.Tensor((512,), dtype="float32") = model_params[39]
            lv965: R.Tensor((512,), dtype="float32") = model_params[38]
            lv606 = R.call_tir(cls.fused_group_norm18_silu15, (lv605, lv964, lv965), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv966: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[35]
            lv967: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[115]
            lv607 = R.call_tir(cls.fused_conv2d37_add39_add40_divide7, (lv606, lv966, lv967, lv603), out_sinfo=R.Tensor((1, 512, 64, 64), dtype="float32"))
            lv104 = R.call_tir(cls.resize2d3, (lv607,), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv968: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[40]
            lv969: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[116]
            lv608 = R.call_tir(cls.fused_conv2d38_add42, (lv104, lv968, lv969), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv970: R.Tensor((512,), dtype="float32") = model_params[44]
            lv971: R.Tensor((512,), dtype="float32") = model_params[43]
            lv609 = R.call_tir(cls.fused_group_norm20_silu16, (lv608, lv970, lv971), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv972: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[41]
            lv973: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[117]
            lv610 = R.call_tir(cls.fused_conv2d38_add42, (lv609, lv972, lv973), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv974: R.Tensor((512,), dtype="float32") = model_params[46]
            lv975: R.Tensor((512,), dtype="float32") = model_params[45]
            lv611 = R.call_tir(cls.fused_group_norm20_silu16, (lv610, lv974, lv975), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv976: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[42]
            lv977: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[118]
            lv612 = R.call_tir(cls.fused_conv2d38_add42_add43_divide9, (lv611, lv976, lv977, lv608), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv978: R.Tensor((512,), dtype="float32") = model_params[50]
            lv979: R.Tensor((512,), dtype="float32") = model_params[49]
            lv613 = R.call_tir(cls.fused_group_norm20_silu16, (lv612, lv978, lv979), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv980: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[47]
            lv981: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[119]
            lv614 = R.call_tir(cls.fused_conv2d38_add42, (lv613, lv980, lv981), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv982: R.Tensor((512,), dtype="float32") = model_params[52]
            lv983: R.Tensor((512,), dtype="float32") = model_params[51]
            lv615 = R.call_tir(cls.fused_group_norm20_silu16, (lv614, lv982, lv983), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv984: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[48]
            lv985: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[120]
            lv616 = R.call_tir(cls.fused_conv2d38_add42_add43_divide9, (lv615, lv984, lv985, lv612), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv986: R.Tensor((512,), dtype="float32") = model_params[56]
            lv987: R.Tensor((512,), dtype="float32") = model_params[55]
            lv617 = R.call_tir(cls.fused_group_norm20_silu16, (lv616, lv986, lv987), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv988: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[53]
            lv989: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[121]
            lv618 = R.call_tir(cls.fused_conv2d38_add42, (lv617, lv988, lv989), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv990: R.Tensor((512,), dtype="float32") = model_params[58]
            lv991: R.Tensor((512,), dtype="float32") = model_params[57]
            lv619 = R.call_tir(cls.fused_group_norm20_silu16, (lv618, lv990, lv991), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv992: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[54]
            lv993: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[122]
            lv620 = R.call_tir(cls.fused_conv2d38_add42_add43_divide9, (lv619, lv992, lv993, lv616), out_sinfo=R.Tensor((1, 512, 128, 128), dtype="float32"))
            lv144 = R.call_tir(cls.resize2d4, (lv620,), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv994: R.Tensor((512, 512, 3, 3), dtype="float32") = model_params[59]
            lv995: R.Tensor((1, 512, 1, 1), dtype="float32") = model_params[123]
            lv621 = R.call_tir(cls.fused_conv2d39_add44, (lv144, lv994, lv995), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv996: R.Tensor((512,), dtype="float32") = model_params[64]
            lv997: R.Tensor((512,), dtype="float32") = model_params[63]
            lv622 = R.call_tir(cls.fused_group_norm21_silu17, (lv621, lv996, lv997), out_sinfo=R.Tensor((1, 512, 256, 256), dtype="float32"))
            lv998: R.Tensor((256, 512, 3, 3), dtype="float32") = model_params[60]
            lv999: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[124]
            lv623 = R.call_tir(cls.fused_conv2d40_add45, (lv622, lv998, lv999), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1000: R.Tensor((256,), dtype="float32") = model_params[66]
            lv1001: R.Tensor((256,), dtype="float32") = model_params[65]
            lv624 = R.call_tir(cls.fused_group_norm22_silu18, (lv623, lv1000, lv1001), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1002: R.Tensor((256, 512, 1, 1), dtype="float32") = model_params[62]
            lv1003: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[126]
            lv625 = R.call_tir(cls.fused_conv2d42_add45, (lv621, lv1002, lv1003), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1004: R.Tensor((256, 256, 3, 3), dtype="float32") = model_params[61]
            lv1005: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[125]
            lv626 = R.call_tir(cls.fused_conv2d41_add45_add46_divide10, (lv624, lv1004, lv1005, lv625), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1006: R.Tensor((256,), dtype="float32") = model_params[70]
            lv1007: R.Tensor((256,), dtype="float32") = model_params[69]
            lv627 = R.call_tir(cls.fused_group_norm22_silu18, (lv626, lv1006, lv1007), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1008: R.Tensor((256, 256, 3, 3), dtype="float32") = model_params[67]
            lv1009: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[127]
            lv628 = R.call_tir(cls.fused_conv2d41_add45, (lv627, lv1008, lv1009), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1010: R.Tensor((256,), dtype="float32") = model_params[72]
            lv1011: R.Tensor((256,), dtype="float32") = model_params[71]
            lv629 = R.call_tir(cls.fused_group_norm22_silu18, (lv628, lv1010, lv1011), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1012: R.Tensor((256, 256, 3, 3), dtype="float32") = model_params[68]
            lv1013: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[128]
            lv630 = R.call_tir(cls.fused_conv2d41_add45_add46_divide10, (lv629, lv1012, lv1013, lv626), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1014: R.Tensor((256,), dtype="float32") = model_params[76]
            lv1015: R.Tensor((256,), dtype="float32") = model_params[75]
            lv631 = R.call_tir(cls.fused_group_norm22_silu18, (lv630, lv1014, lv1015), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1016: R.Tensor((256, 256, 3, 3), dtype="float32") = model_params[73]
            lv1017: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[129]
            lv632 = R.call_tir(cls.fused_conv2d41_add45, (lv631, lv1016, lv1017), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1018: R.Tensor((256,), dtype="float32") = model_params[78]
            lv1019: R.Tensor((256,), dtype="float32") = model_params[77]
            lv633 = R.call_tir(cls.fused_group_norm22_silu18, (lv632, lv1018, lv1019), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv1020: R.Tensor((256, 256, 3, 3), dtype="float32") = model_params[74]
            lv1021: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[130]
            lv634 = R.call_tir(cls.fused_conv2d41_add45_add46_divide10, (lv633, lv1020, lv1021, lv630), out_sinfo=R.Tensor((1, 256, 256, 256), dtype="float32"))
            lv187 = R.call_tir(cls.resize2d5, (lv634,), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv1022: R.Tensor((256, 256, 3, 3), dtype="float32") = model_params[79]
            lv1023: R.Tensor((1, 256, 1, 1), dtype="float32") = model_params[131]
            lv635 = R.call_tir(cls.fused_conv2d43_add47, (lv187, lv1022, lv1023), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv1024: R.Tensor((256,), dtype="float32") = model_params[84]
            lv1025: R.Tensor((256,), dtype="float32") = model_params[83]
            lv636 = R.call_tir(cls.fused_group_norm23_silu19, (lv635, lv1024, lv1025), out_sinfo=R.Tensor((1, 256, 512, 512), dtype="float32"))
            lv1026: R.Tensor((128, 256, 3, 3), dtype="float32") = model_params[80]
            lv1027: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[132]
            lv637 = R.call_tir(cls.fused_conv2d44_add48, (lv636, lv1026, lv1027), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1028: R.Tensor((128,), dtype="float32") = model_params[86]
            lv1029: R.Tensor((128,), dtype="float32") = model_params[85]
            lv638 = R.call_tir(cls.fused_group_norm24_silu20, (lv637, lv1028, lv1029), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1030: R.Tensor((128, 256, 1, 1), dtype="float32") = model_params[82]
            lv1031: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[134]
            lv639 = R.call_tir(cls.fused_conv2d46_add48, (lv635, lv1030, lv1031), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1032: R.Tensor((128, 128, 3, 3), dtype="float32") = model_params[81]
            lv1033: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[133]
            lv640 = R.call_tir(cls.fused_conv2d45_add48_add49_divide11, (lv638, lv1032, lv1033, lv639), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1034: R.Tensor((128,), dtype="float32") = model_params[90]
            lv1035: R.Tensor((128,), dtype="float32") = model_params[89]
            lv641 = R.call_tir(cls.fused_group_norm24_silu20, (lv640, lv1034, lv1035), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1036: R.Tensor((128, 128, 3, 3), dtype="float32") = model_params[87]
            lv1037: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[135]
            lv642 = R.call_tir(cls.fused_conv2d45_add48, (lv641, lv1036, lv1037), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1038: R.Tensor((128,), dtype="float32") = model_params[92]
            lv1039: R.Tensor((128,), dtype="float32") = model_params[91]
            lv643 = R.call_tir(cls.fused_group_norm24_silu20, (lv642, lv1038, lv1039), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1040: R.Tensor((128, 128, 3, 3), dtype="float32") = model_params[88]
            lv1041: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[136]
            lv644 = R.call_tir(cls.fused_conv2d45_add48_add49_divide11, (lv643, lv1040, lv1041, lv640), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1042: R.Tensor((128,), dtype="float32") = model_params[96]
            lv1043: R.Tensor((128,), dtype="float32") = model_params[95]
            lv645 = R.call_tir(cls.fused_group_norm24_silu20, (lv644, lv1042, lv1043), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1044: R.Tensor((128, 128, 3, 3), dtype="float32") = model_params[93]
            lv1045: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[137]
            lv646 = R.call_tir(cls.fused_conv2d45_add48, (lv645, lv1044, lv1045), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1046: R.Tensor((128,), dtype="float32") = model_params[98]
            lv1047: R.Tensor((128,), dtype="float32") = model_params[97]
            lv647 = R.call_tir(cls.fused_group_norm24_silu20, (lv646, lv1046, lv1047), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1048: R.Tensor((128, 128, 3, 3), dtype="float32") = model_params[94]
            lv1049: R.Tensor((1, 128, 1, 1), dtype="float32") = model_params[138]
            lv648 = R.call_tir(cls.fused_conv2d45_add48_add49_divide11, (lv647, lv1048, lv1049, lv644), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1050: R.Tensor((128,), dtype="float32") = model_params[2]
            lv1051: R.Tensor((128,), dtype="float32") = model_params[1]
            lv649 = R.call_tir(cls.fused_group_norm24_silu20, (lv648, lv1050, lv1051), out_sinfo=R.Tensor((1, 128, 512, 512), dtype="float32"))
            lv1052: R.Tensor((3, 128, 3, 3), dtype="float32") = model_params[3]
            lv1053: R.Tensor((1, 3, 1, 1), dtype="float32") = model_params[139]
            lv650 = R.call_tir(cls.fused_conv2d47_add50_divide12_add51_tir_clip, (lv649, lv1052, lv1053), out_sinfo=R.Tensor((1, 3, 512, 512), dtype="float32"))
            lv651 = R.call_tir(cls.fused_transpose51_multiply30_tir_round, (lv650,), out_sinfo=R.Tensor((1, 512, 512, 3), dtype="float32"))
            gv: R.Tensor((1, 512, 512, 3), dtype="float32") = lv651
            R.output(gv)
        return gv




def test_rpc():
    if not tvm.runtime.enabled("rpc"):
        return
    n = 1024
    dtype = "float32"
    temp = utils.tempdir()
    wasm_path = temp.relpath("relax.wasm")
    target = tvm.target.Target("webgpu", host="llvm -mtriple=wasm32-unknown-unknown-wasm")

    mod = Module

    print("start building")

    ex = relax.build(mod, target)

    print("build success")

    ex.export_library(wasm_path, fcompile=tvmjs.create_tvmjs_wasm)
    wasm_binary = open(wasm_path, "rb").read()

    remote = rpc.connect(
        proxy_host,
        proxy_port,
        key="wasm",
        session_constructor_args=["rpc.WasmSession", wasm_binary],
    )

    def check(remote):
        dev = remote.webgpu(0)
        # invoke the function
        vm = relax.VirtualMachine(remote.system_lib(), device=dev)
        adata = np.random.uniform(size=n).astype(dtype)
        bdata = np.random.uniform(size=n).astype(dtype)
        a = tvm.nd.array(adata, dev)
        b = tvm.nd.array(bdata, dev)

        #deal with params and inputs    

        vm.set_input("main", a, b)
        vm.invoke_stateful("main")
        c = vm.get_outputs("main")
        np.testing.assert_equal(c.numpy(), a.numpy() + b.numpy())
        print("Test pass..")

    check(remote)


test_rpc()
